{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition App using Neural Networks\n",
    "https://data-flair.training/blogs/python-deep-learning-project-handwritten-digit-recognition/\n",
    "\n",
    "Data: MNIST dataset\n",
    "\n",
    "Model: <a href=\"https://data-flair.training/blogs/convolutional-neural-networks-tutorial/\">Convolutional Neural Networks</a>\n",
    "\n",
    "Theory: <a href=\"https://www.linkedin.com/learning/neural-networks-and-convolutional-neural-networks-essential-training/neural-networks?autoAdvance=true&autoSkip=true&autoplay=true&resume=false&u=36492188\">\"Neural_Networks_and_Convolutional_Neural_Networks_Essential_Training\"</a> LinkedIn course by Jonathan Fernandes.\n",
    "\n",
    "Libraries: Keras, Tkinter for building GUI.\n",
    "\n",
    "In the end, a GUI is built where one can draw a digit and recognize it straight away. If the image is misclassified, one can provide correct label and save the handwritten image and label. Corrections can then be added to the main image dataset for model re-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and MNIST dataset\n",
    "MNIST dataset contains 60,000 training images of handwritten digits from 0 to 9 and 10,000 images for testing. Images have black background and white-gray digits. Digits are represented as a 28Ã—28 matrix where each cell contains grayscale pixel value from 0 (black) to 255 (white). There are 10 classes, one for each digit.\n",
    "\n",
    "The Keras library already contains some datasets and MNIST is one of them. So we can easily import the dataset and start working with it. The `mnist.load_data()` method returns us the training data, its labels and also the testing data and its labels.\n",
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import os\n",
    "os.chdir('C:/Sereda/Lectures/Springboard/Projects/DigitRecognition_CNN')\n",
    "\n",
    "#pip install tensorflow\n",
    "#pip install keras\n",
    "#pip install pillow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # save image array to csv\n",
    "# to view plots in notebook\n",
    "%matplotlib inline\n",
    "import seaborn as sn\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D   # convolutional NNs\n",
    "from keras import backend as K\n",
    "#from keras import utils as np_utils\n",
    "#from tensorflow.keras import utils as np_utils\n",
    "#from keras.preprocessing.image import load_img, array_to_img\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#import tensorflow as tf\n",
    "\n",
    "from PIL import Image, ImageGrab\n",
    "# Image augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "# GUI\n",
    "from keras.models import load_model\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "import win32gui\n",
    "\n",
    "assert(np.__version__ == '1.22.3')\n",
    "assert(pd.__version__ == '1.3.4')\n",
    "assert(sn.__version__ == '0.11.2')\n",
    "assert(keras.__version__ == '2.8.0')\n",
    "from platform import python_version\n",
    "assert(python_version() == '3.9.7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset\n",
      "X_train: (60000, 28, 28)\n",
      "y_train: (60000,)\n",
      "X_test: (10000, 28, 28)\n",
      "y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "print(\"MNIST dataset\")\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"y_train:\",y_train.shape)\n",
    "print(\"X_test:\",X_test.shape)\n",
    "print(\"y_test:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot an image to see if data is correct"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n = 0 # image number\n",
    "img = X_train[n,:,:].astype('float32')/255\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f'MNIST image {n}')\n",
    "plt.show()\n",
    "print(f\"Label = {y_train[n]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "The image data cannot be fed directly into the model so we need to perform some operations and process the data to make it ready for our neural network. The dimension of the training data is (60000,28,28). The CNN model will require one more dimension so we reshape the matrix to shape (60000,28,28,1), while for non-convolutional NN it is (60000,$28^{2}$,1).\n",
    "#### Training data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images as 28 x 28 matrixes for training\n"
     ]
    }
   ],
   "source": [
    "# Keep image data as a matrix or convert to vector?\n",
    "mx = True # True/False: If false - as 28^2 vector for NN, if true - as 28 x 28 matrix for CNN\n",
    "sy = X_test.shape[1]; sx = X_test.shape[2]; # image sizes\n",
    "if mx:\n",
    "    print(f\"Images as {sy} x {sx} matrixes for training\")\n",
    "    input_shape = (sy, sx, 1)\n",
    "else:\n",
    "    print(f\"Images as {sy*sx}-component vectors for training\")\n",
    "    input_shape = (sy*sx, )\n",
    "    X_train = X_train.reshape(-1,sy*sx)\n",
    "    X_test = X_test.reshape(-1,sy*sx)\n",
    "    print(\"New training size, X_train:\",X_train.shape)\n",
    "    print(\"New test size, X_test:\",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add manually labeled images\n",
    "If some corrections were saved during previous digit recognitions, read them to add to the training dataset. Misclassified images were appended to a CSV file, and their manual labels were appended to a separate CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved misclassified images with available labels: images.csv\n",
      "Adding manually labeled images to MNIST training set, X_labeled: (1600, 784)\n",
      "New training size, X_train: (61600, 28, 28)\n",
      "Reading saved correct labels: labels.csv\n",
      "Adding manual labels to MNIST training labels, y_labeled: (1600, 1)\n",
      "New training labels size: (61600, 1)\n",
      "Last label: [7.]\n",
      "Last image:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ml = 1600;#1453; # manual labels\n",
    "if ml>0:\n",
    "    saved_images = 'images.csv' # (n, sy*sx) n vectors with sy*sx components of 0-255, no header\n",
    "    saved_labels = 'labels.csv' # scalars, no header\n",
    "    \n",
    "    if os.path.exists(saved_images):\n",
    "        print(\"Reading saved misclassified images with available labels:\",saved_images)\n",
    "        X_labeled = pd.read_csv(saved_images, header=None)[:ml]\n",
    "        print(\"Adding manually labeled images to MNIST training set, X_labeled:\",X_labeled.shape)\n",
    "        if mx:\n",
    "            X_train = np.concatenate( (X_train, np.array(X_labeled).reshape(-1,sy,sx)) )\n",
    "        else:\n",
    "            X_train = np.concatenate( (X_train, np.array(X_labeled)) )\n",
    "        print(\"New training size, X_train:\",X_train.shape)\n",
    "    else: print(\"No saved labeled images\")\n",
    "    \n",
    "    if os.path.exists(saved_labels):\n",
    "        print(\"Reading saved correct labels:\",saved_labels)\n",
    "        y_labeled = pd.read_csv(saved_labels, header=None)[:ml]\n",
    "        print(\"Adding manual labels to MNIST training labels, y_labeled:\",y_labeled.shape)\n",
    "        #y_train = np.concatenate( (y_train, np.array(y_labeled).reshape(-1)) )\n",
    "        # Do we need to add last dimension 1 to y_train?\n",
    "        y_train = np.concatenate((y_train.reshape(-1,1), np.array(y_labeled).ravel().reshape(-1,1)))\n",
    "        print(\"New training labels size:\",y_train.shape)\n",
    "    else: print(\"No saved manual labels\")\n",
    "    \n",
    "    # Verify\n",
    "    print(\"Last label:\",y_train[-1])\n",
    "    print(\"Last image:\")\n",
    "    plt.imshow(X_train.reshape(-1,sy,sx)[-1], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Kaggle training images with available labels: data/train.csv\n",
      "Adding Kaggle images to MNIST training set, X_kaggle: (42000, 784)\n",
      "New training size, X_train: (103600, 28, 28)\n",
      "Reading saved Kaggle labels\n",
      "Adding Kaggle labels to MNIST training labels, y_kaggle: (42000,)\n",
      "New training labels size: (103600, 1)\n",
      "Last label: [9.]\n",
      "Last image:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kl = 42000; # Kaggle labels\n",
    "if kl>0:\n",
    "    kaggle_images = 'data/train.csv' # (n, sy*sx) n vectors with sy*sx components of 0-255, 1 header line\n",
    "    \n",
    "    if os.path.exists(kaggle_images):\n",
    "        print(\"Reading Kaggle training images with available labels:\",kaggle_images)\n",
    "        yX_kaggle = pd.read_csv(kaggle_images, header='infer')[:kl] # Label & image from Kaggle\n",
    "        X_kaggle = yX_kaggle.iloc[:,1:]        \n",
    "        print(\"Adding Kaggle images to MNIST training set, X_kaggle:\",X_kaggle.shape)\n",
    "        if mx:\n",
    "            X_train = np.concatenate( (X_train, np.array(X_kaggle).reshape(-1,sy,sx)) )\n",
    "        else:\n",
    "            X_train = np.concatenate( (X_train, np.array(X_kaggle)) )\n",
    "        print(\"New training size, X_train:\",X_train.shape)\n",
    "        print(\"Reading saved Kaggle labels\")\n",
    "        y_kaggle = yX_kaggle.iloc[:,0]\n",
    "        print(\"Adding Kaggle labels to MNIST training labels, y_kaggle:\",y_kaggle.shape)\n",
    "        #y_train = np.concatenate( (y_train, np.array(y_kaggle).reshape(-1)) )\n",
    "        # Do we need to add last dimension 1 to y_train?\n",
    "        y_train = np.concatenate((y_train.reshape(-1,1), np.array(y_kaggle).ravel().reshape(-1,1)))\n",
    "        print(\"New training labels size:\",y_train.shape)\n",
    "    else: print(\"No saved Kaggle images\")\n",
    "\n",
    "    # Verify\n",
    "    print(\"Last label:\",y_train[-1])\n",
    "    print(\"Last image:\")\n",
    "    plt.imshow(X_train.reshape(-1,sy,sx)[-1], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale predictors from 0 to 1 and one-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (103600, 28, 28)\n",
      "X_test: (10000, 28, 28)\n",
      "103600 train samples\n",
      "10000 test samples\n",
      "y_train: (103600, 10)\n",
      "y_test: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')/255 # Convert to float to avoid problems during rescaling to 0-1.\n",
    "# Could do 1-... to invert image and make white background with black digits.\n",
    "X_test = X_test.astype('float32')/255 # 1-...\n",
    "#print(\"min. X_train[0] =\",np.min(X_train[0])) # 0-1\n",
    "#print(\"max. X_train[0] =\",np.max(X_train[0]))\n",
    "print('X_train:', X_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices using label encoding\n",
    "num_classes=10   # 10 digits 0-9\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "print('y_train:', y_train.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Verify manual labels\n",
    "nl=1599#nl+1#1599\n",
    "plt.imshow(X_train.reshape(-1,sy,sx)[60000+nl], cmap='gray');\n",
    "print(np.argmax(y_train[60000+nl]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network models in Keras\n",
    "### 3.1 NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not mx:\n",
    "    # Create model\n",
    "    nl1 = 50; nl2 = 50; # neurons in NN layers [50, 50]\n",
    "    dropout = 0;#[0] 0.2;\n",
    "    nn = Sequential()\n",
    "    # Add the first hidden layer\n",
    "    #nn.add(Dense(nl1, activation='relu', input_shape=(28*28,))) # 50 nodes, ReLU activation function, input_shape indicated only once\n",
    "    nn.add(Dense(nl1, activation='relu', input_shape=(sy*sx,))) # 50 nodes, ReLU activation function, input_shape indicated only once\n",
    "    # Dropout\n",
    "    #nn.add(Dropout(dropout))\n",
    "    # Add the second hidden layer\n",
    "    nn.add(Dense(nl2, activation='relu')) #50,25\n",
    "    # Add the output layer\n",
    "    nn.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer='adam'\n",
    "    nn.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy','AUC'])\n",
    "    nn.summary()\n",
    "    \n",
    "    print(\"Explanation of the number of weights:\")\n",
    "    print(\"1st layer's parameters:\",nl1*(sx*sy+1)) # Each of the 'nl1' nodes of layer 1 takes sx*sy inputs (pixels here) +1 due to bias\n",
    "    print(\"2nd layer's parameters:\",nl2*(nl1+1)) # Each of the 'nl2' nodes of layer 2 takes 'nl1' inputs + 1 bias\n",
    "    print(\"output layer's parameters:\",num_classes*(nl2+1)) # Each of the 10 output classes takes 'nl2' inputs + 1 bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Convolutional Neural Networks\n",
    "A CNN model generally consists of convolutional and pooling layers. It works better for data that are represented as grid structures,- this is the reason why CNN works well for image classification problems. One image is distinguishable from another by its spatial structure. Areas close to each other are highly significant for an image.\n",
    "\n",
    "However, fully connected neural networks typically don't work well on large images, since they don't scale well with image size. Example: 32 * 32 * 3 image = 3072 weights; 200 * 200 * 3 = 120000 weights. This large number of parameters can quickly lead to overfitting. One could work with smaller version of images, but one would lose information.\n",
    "\n",
    "The dropout layer is used to deactivate some of the neurons and while training, it reduces overfitting of the model.\n",
    "\n",
    "We will then compile the model with the Adadelta optimizer.\n",
    "However, beware of poor scaling of compute time with image size.\n",
    "\n",
    "#### Convolutions\n",
    "* <b>Kernel</b> (filter, or matrix): 3 x 3 or 5 x 5. Center element of the kernel is placed over the source pixel. The source pixel is then replaced with the sum of elementwise products in the kernel and corresponding nearby source pixels.\n",
    "* Convolve it over the image in all possible ways.\n",
    "This gives 2D <b>activation map</b>. Convolving decreases the spatial size.\n",
    "\n",
    "<b>Zero Padding</b>\n",
    "\n",
    "Using n x n filter, we lose n-1 pixels in each of the 2 dimensions each time. To preserve the spatial size of the input (do not lose pixels), pad with 0's.\n",
    "\n",
    "F: size of filter<br>\n",
    "S: stride<br>\n",
    "N: size of image<br>\n",
    "P: amount of padding<br>\n",
    "The image output size is given by $\\frac{N-F+2P}{S}+1$\n",
    "\n",
    "#### Pooling\n",
    "<b>Max Pooling</b>\n",
    "\n",
    "Keep only a maximal value from each block, e.g. 2 x 2.\n",
    "\n",
    "<b>Average Pooling</b>\n",
    "\n",
    "Keep only an average value from each block.\n",
    "Why to use the pooling? To progressively reduce the size and chances of overfitting. Max pooling is more popular, as convolutions \"light up\" when they detect a particular feature in a region of an image. When downsampling, it makes sense to preserve the parts that were most activated.\n",
    "\n",
    "#### Dropout\n",
    "<b>Dropout</b> to prevent overfitting.\n",
    "* Randomly kill each neuron in layer of a training set with probability p, typically 0.5 (half of the neurons in a layer are dropped during the training). Therefore, the network cannot rely on activation of any set of hidden units, since they may be turned off at any time during training, and the model is forced to learn more general and more robust patterns from the data.\n",
    "We don't use dropout during validation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    loss = np.abs(y_pred - y_true)\n",
    "    loss = np.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 48)        480       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 96)        41568     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 12, 96)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 10, 10, 48)        41520     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 48)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5, 5, 48)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1200)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               307456    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 393,594\n",
      "Trainable params: 393,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Explanation of the number of weights:\n",
      "1st layer's parameters: 320\n",
      "2nd layer's parameters: 18496\n",
      "6th layer's parameters: 2359552\n",
      "output layer's parameters: 2570\n"
     ]
    }
   ],
   "source": [
    "if mx:\n",
    "    nl1=48; nl2=96; nl3=48; dropout=0.25; #nl1=32\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv2D(nl1, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "    cnn.add(Conv2D(nl2, (3, 3), activation='relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn.add(Dropout(0.25))\n",
    "\n",
    "    cnn.add(Conv2D(nl3, (3, 3), activation='relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn.add(Dropout(0.25))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(256, activation='relu'))\n",
    "    cnn.add(Dropout(0.5))\n",
    "    cnn.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "    loss = 'categorical_crossentropy';    #categorical_crossentropy, mean_squared_error, mean_absolute_error\n",
    "    optimizer='Adam' #'Adadelta','AdaGrad','Adam','AdaMax','Ftrl','Nadam','Optimizer',RMSProp','SGD'\n",
    "    cnn.compile(loss=loss, optimizer=optimizer, metrics=['accuracy','AUC']) #optimizer=tf.keras.optimizers.Adadelta()\n",
    "    cnn.summary()\n",
    "    \n",
    "    print(\"Explanation of the number of weights:\")\n",
    "    print(\"1st layer's parameters:\",32*((28-3+2*1)//3+1)) # convolutions*nodes: Filter size = 3, Padding = 1, Stride = 3\n",
    "    print(\"2nd layer's parameters:\", (32*((26-3+2*1)//3+1) + 1)*64)\n",
    "    print(\"6th layer's parameters:\", (9216+1)*256)\n",
    "    print(\"output layer's parameters:\", (256+1)*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Choose / load / train / save the model\n",
    "If the model is newly created or its hyperparameters updated or new training data added, we train and save the model. Otherwise, if returning to already studied model, we load it from an HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "Epoch 1/19\n",
      "810/810 [==============================] - 179s 220ms/step - loss: 0.1911 - accuracy: 0.9388 - auc: 0.9967 - val_loss: 0.0281 - val_accuracy: 0.9900 - val_auc: 0.9999\n",
      "Epoch 2/19\n",
      "810/810 [==============================] - 177s 218ms/step - loss: 0.0603 - accuracy: 0.9813 - auc: 0.9991 - val_loss: 0.0218 - val_accuracy: 0.9923 - val_auc: 0.9999\n",
      "Epoch 3/19\n",
      "810/810 [==============================] - 176s 217ms/step - loss: 0.0436 - accuracy: 0.9864 - auc: 0.9994 - val_loss: 0.0202 - val_accuracy: 0.9932 - val_auc: 0.9999\n",
      "Epoch 4/19\n",
      "810/810 [==============================] - 172s 213ms/step - loss: 0.0361 - accuracy: 0.9888 - auc: 0.9995 - val_loss: 0.0151 - val_accuracy: 0.9947 - val_auc: 0.9998\n",
      "Epoch 5/19\n",
      "810/810 [==============================] - 180s 222ms/step - loss: 0.0316 - accuracy: 0.9902 - auc: 0.9996 - val_loss: 0.0123 - val_accuracy: 0.9965 - val_auc: 0.9999\n",
      "Epoch 6/19\n",
      "810/810 [==============================] - 180s 222ms/step - loss: 0.0264 - accuracy: 0.9915 - auc: 0.9996 - val_loss: 0.0111 - val_accuracy: 0.9965 - val_auc: 0.9999\n",
      "Epoch 7/19\n",
      "810/810 [==============================] - 184s 227ms/step - loss: 0.0236 - accuracy: 0.9926 - auc: 0.9997 - val_loss: 0.0104 - val_accuracy: 0.9968 - val_auc: 0.9999\n",
      "Epoch 8/19\n",
      "810/810 [==============================] - 171s 211ms/step - loss: 0.0222 - accuracy: 0.9927 - auc: 0.9997 - val_loss: 0.0116 - val_accuracy: 0.9963 - val_auc: 0.9998\n",
      "Epoch 9/19\n",
      "810/810 [==============================] - 173s 214ms/step - loss: 0.0199 - accuracy: 0.9935 - auc: 0.9997 - val_loss: 0.0091 - val_accuracy: 0.9971 - val_auc: 0.9999\n",
      "Epoch 10/19\n",
      "810/810 [==============================] - 178s 220ms/step - loss: 0.0186 - accuracy: 0.9937 - auc: 0.9997 - val_loss: 0.0097 - val_accuracy: 0.9966 - val_auc: 0.9999\n",
      "Epoch 11/19\n",
      "810/810 [==============================] - 174s 215ms/step - loss: 0.0163 - accuracy: 0.9946 - auc: 0.9998 - val_loss: 0.0076 - val_accuracy: 0.9978 - val_auc: 0.9998\n",
      "Epoch 12/19\n",
      "810/810 [==============================] - 165s 204ms/step - loss: 0.0161 - accuracy: 0.9949 - auc: 0.9998 - val_loss: 0.0074 - val_accuracy: 0.9978 - val_auc: 0.9999\n",
      "Epoch 13/19\n",
      "810/810 [==============================] - 165s 204ms/step - loss: 0.0142 - accuracy: 0.9953 - auc: 0.9998 - val_loss: 0.0090 - val_accuracy: 0.9977 - val_auc: 0.9998\n",
      "Epoch 14/19\n",
      "810/810 [==============================] - 166s 205ms/step - loss: 0.0152 - accuracy: 0.9948 - auc: 0.9999 - val_loss: 0.0090 - val_accuracy: 0.9972 - val_auc: 0.9999\n",
      "Epoch 15/19\n",
      "810/810 [==============================] - 166s 205ms/step - loss: 0.0140 - accuracy: 0.9955 - auc: 0.9998 - val_loss: 0.0079 - val_accuracy: 0.9974 - val_auc: 0.9999\n",
      "Epoch 16/19\n",
      "810/810 [==============================] - 165s 204ms/step - loss: 0.0129 - accuracy: 0.9958 - auc: 0.9998 - val_loss: 0.0079 - val_accuracy: 0.9975 - val_auc: 0.9999\n",
      "Epoch 17/19\n",
      "810/810 [==============================] - 178s 220ms/step - loss: 0.0123 - accuracy: 0.9959 - auc: 0.9998 - val_loss: 0.0069 - val_accuracy: 0.9979 - val_auc: 0.9998\n",
      "Epoch 18/19\n",
      "810/810 [==============================] - 266s 328ms/step - loss: 0.0128 - accuracy: 0.9957 - auc: 0.9999 - val_loss: 0.0087 - val_accuracy: 0.9976 - val_auc: 0.9998\n",
      "Epoch 19/19\n",
      "810/810 [==============================] - 263s 325ms/step - loss: 0.0120 - accuracy: 0.9961 - auc: 0.9998 - val_loss: 0.0074 - val_accuracy: 0.9980 - val_auc: 0.9998\n",
      "{'verbose': 1, 'epochs': 19, 'steps': 810}\n",
      "{'loss': [0.19112366437911987, 0.06034287065267563, 0.04359244927763939, 0.03613486513495445, 0.031635385006666183, 0.026421990245580673, 0.02359657548367977, 0.022237839177250862, 0.019911326467990875, 0.01864156872034073, 0.016329210251569748, 0.01608073152601719, 0.01419472973793745, 0.015214651823043823, 0.013993101194500923, 0.012927241623401642, 0.012312617152929306, 0.012806035578250885, 0.011973670683801174], 'accuracy': [0.9388030767440796, 0.9812548160552979, 0.986361026763916, 0.9887934327125549, 0.9901737570762634, 0.9915154576301575, 0.9925868511199951, 0.9926834106445312, 0.9935328364372253, 0.9937354922294617, 0.9945560097694397, 0.9948938488960266, 0.995299220085144, 0.9947972893714905, 0.9954922795295715, 0.9958397746086121, 0.9959073066711426, 0.9957432150840759, 0.9960810542106628], 'auc': [0.9967061877250671, 0.9991004467010498, 0.9994290471076965, 0.9995277523994446, 0.9996004700660706, 0.9996263980865479, 0.9996811151504517, 0.9997497200965881, 0.9997439980506897, 0.9997438192367554, 0.9998025894165039, 0.9997873902320862, 0.9998258948326111, 0.9998520016670227, 0.9997958540916443, 0.9998334050178528, 0.9998429417610168, 0.9998547434806824, 0.9998044371604919], 'val_loss': [0.028075464069843292, 0.02176188863813877, 0.02021081931889057, 0.015108161605894566, 0.012332327663898468, 0.011114455759525299, 0.010443396866321564, 0.011603116057813168, 0.009057443588972092, 0.009725809097290039, 0.00760197127237916, 0.007392426952719688, 0.00904825422912836, 0.008989027701318264, 0.007865499705076218, 0.007895993068814278, 0.006940019782632589, 0.008682265877723694, 0.007371857296675444], 'val_accuracy': [0.9900000095367432, 0.9922999739646912, 0.9932000041007996, 0.994700014591217, 0.9965000152587891, 0.9965000152587891, 0.9968000054359436, 0.9962999820709229, 0.9970999956130981, 0.9965999722480774, 0.9977999925613403, 0.9977999925613403, 0.9976999759674072, 0.9972000122070312, 0.9973999857902527, 0.9975000023841858, 0.9979000091552734, 0.9976000189781189, 0.9980000257492065], 'val_auc': [0.999895453453064, 0.9998680353164673, 0.9999247193336487, 0.9997659921646118, 0.9998791813850403, 0.9998811483383179, 0.9998826384544373, 0.9998317360877991, 0.9998839497566223, 0.9999383091926575, 0.9998359680175781, 0.9999417662620544, 0.999780535697937, 0.9999398589134216, 0.9998859763145447, 0.9998863339424133, 0.9998314380645752, 0.9997751116752625, 0.99978107213974]}\n",
      "Save model: models/cnn_cce_48_96_48_Adam.e19.kaggle.h5\n"
     ]
    }
   ],
   "source": [
    "# Choose model\n",
    "#print(\"Manual labels: ml =\",ml)\n",
    "epochs = 19; #[200],400, header='infer')[:kl]\n",
    "if mx:\n",
    "    model = cnn\n",
    "    #model_fn = 'models/cnn.e'+str(epochs)+'.h5' # HDF5 format\n",
    "    if kl==0:\n",
    "        model_fn = 'models/cnn_cce_'+str(nl1)+'_'+str(nl2)+'_'+str(nl3)+'_'+optimizer+'.e'+str(epochs)+'.h5' # HDF5 format\n",
    "    else:\n",
    "        model_fn = 'models/cnn_cce_'+str(nl1)+'_'+str(nl2)+'_'+str(nl3)+'_'+optimizer+'.e'+str(epochs)+'.kaggle.h5'\n",
    "else:\n",
    "    model = nn\n",
    "    #model_fn = 'models/nn_'+str(nl1)+'_'+str(nl2)+'.ml'+str(ml)+'.e'+str(epochs)+'.h5'\n",
    "    model_fn = 'models/nn_'+str(nl1)+'_d0_'+str(nl2)+'.ml'+str(ml)+'.e'+str(epochs)+'.h5'\n",
    "\n",
    "# If the model was previously trained, read the weights\n",
    "if os.path.isfile(model_fn):\n",
    "    print(\"Reading model weights:\",model_fn)\n",
    "    model.load_weights(model_fn)\n",
    "# Otherwise, train and save the model\n",
    "else:\n",
    "    print(\"Train model\")\n",
    "    #model.fit(X_train, y_train, epochs=1)\n",
    "    history_model = model.fit(X_train, y_train, batch_size=128, epochs=epochs, validation_data=(X_test, y_test), verbose=1);\n",
    "    print(history_model.params)\n",
    "    print(history_model.history)\n",
    "    if mx:\n",
    "        pd.DataFrame(history_model.history).to_csv('performance/cnn_relu32_relu64_d'+str(dropout)+'.ml'+str(ml)+'.e'+str(epochs)+'.csv', index=None) # Save training performance\n",
    "    else:\n",
    "        pd.DataFrame(history_model.history).to_csv('performance/nn_relu50_d'+str(dropout)+'_relu50.ml'+str(ml)+'.e'+str(epochs)+'.csv', index=None) # Save training performance\n",
    "    print(\"Save model:\", model_fn)\n",
    "    model.save(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19,0.011973670683801174,0.9960810542106628,0.9998044371604919,0.007371857296675444,0.9980000257492065,0.99978107213974\n"
     ]
    }
   ],
   "source": [
    "print(f\"{epochs},{history_model.history['loss'][-1]},{history_model.history['accuracy'][-1]},{history_model.history['auc'][-1]},{history_model.history['val_loss'][-1]},{history_model.history['val_accuracy'][-1]},{history_model.history['val_auc'][-1]}\")\n",
    "f = open('performance/cnn_relu48_relu96_relu48_d0.25.ml1600_k.e1-13.csv', 'a')\n",
    "f.write(f\"\\n{epochs},{history_model.history['loss'][-1]},{history_model.history['accuracy'][-1]},{history_model.history['auc'][-1]},{history_model.history['val_loss'][-1]},{history_model.history['val_accuracy'][-1]},{history_model.history['val_auc'][-1]}\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1668/33164213.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#model.load_weights(\"models/nn_50_50.ml0.e400.h5\")\n",
    "model.load_weights(\"models/nn_50_75.ml0.e200.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.fit()` function of Keras will start the training of the model. It takes the <b>training data, validation data, epochs,</b> and <b>batch size</b>. It takes some time to train the model. After training, we save the weights and model definition in the HDF5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Evaluate the model\n",
    "We have 10,000 images in our dataset which will be used to evaluate how good our model works. The testing data was not involved in the training of the model; therefore, it is new data for our model. The MNIST dataset is well balanced so we can get around 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN model training history: read it if not in memory\n",
    "try:\n",
    "    history_model.history['loss'][:1] #history_model.head(1)\n",
    "    # Plot training performance\n",
    "    plt.plot(history_model.history['auc'])\n",
    "    plt.plot(history_model.history['val_auc'])\n",
    "    plt.plot(history_model.history['accuracy'])\n",
    "    plt.plot(history_model.history['val_accuracy'])\n",
    "    plt.legend(['AUC train', 'AUC test', 'Accuracy train', 'Accuracy test']);\n",
    "    plt.xlabel('epoch');\n",
    "    plt.ylabel('performance');\n",
    "    plt.xlim(0,epochs)\n",
    "    if mx==True:\n",
    "        plt.title('CNN_'+optimizer+'_'+str(nl1)+'_'+str(nl2)+'_d'+str(dropout)+' Manual labels: '+str(ml) + ', epochs: '+str(epochs));\n",
    "    else:\n",
    "        plt.title('NN_'+optimizer+'_'+str(nl1)+'_d'+str(dropout)+'_'+str(nl2)+' Manual labels: '+str(ml) + ', epochs: '+str(epochs));\n",
    "except AttributeError:\n",
    "    print(history_model.head())\n",
    "    # Plot training performance\n",
    "    plt.plot(history_model['auc'])\n",
    "    plt.plot(history_model['val_auc'])\n",
    "    plt.plot(history_model['accuracy'])\n",
    "    plt.plot(history_model['val_accuracy'])\n",
    "    plt.legend(['AUC train', 'AUC test', 'Accuracy train', 'Accuracy test']);\n",
    "    plt.xlabel('epoch');\n",
    "    plt.ylabel('performance');\n",
    "    plt.xlim(0,epochs)\n",
    "except NameError:\n",
    "    if mx:\n",
    "        history_model = pd.read_csv('performance/cnn_relu32_relu64_d'+str(dropout)+'.ml'+str(ml)+'.e'+str(epochs)+'.csv', header='infer')\n",
    "    else:\n",
    "        history_model = pd.read_csv('performance/nn_relu50_d0_relu50.ml'+str(ml)+'.e'+str(epochs)+'.csv', header='infer')\n",
    "        history_model[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_model.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test scores after NN model training: 50 + 50 neurons, no dropout\n",
    "\n",
    "|Manual labels|Epochs|Loss|Accuracy|\n",
    "|:--:|:--:|:--:|:-----:|\n",
    "|0  |200|0.272163|0.975200|\n",
    "|200|200|0.260561|0.974100|\n",
    "|400|200|0.242828|0.978000|\n",
    "|600*|200|0.204577|0.978400|\n",
    "|600|200|0.268606|0.973200|\n",
    "|800|200|0.241012|0.976800|\n",
    "|0  |400|0.380669|0.972900|\n",
    "\n",
    "|model|nl1|dropout1|nl2|dropout2|manual_labels|loss|optimizer|epochs|test_loss|test_accuracy|trials|accuracy|\n",
    "|:---:|:-:|:------:|:-:|:------:|:-----------:|:--:|:-------:|:----:|:-------:|:-----------:|:----:|:------:|\n",
    "|NN|50|0|50|0|0|categorical_crossentropy|Adam|200|0.272163|0.9752|500|0.694|\n",
    "|NN|50|0|25|0|0|categorical_crossentropy|Adam|200|0.278739|0.9733|200|0.59|\n",
    "|NN|50|0|75|0|0|categorical_crossentropy|Adam|200|0.248749|0.9771|?150|?0.566667|\n",
    "|NN|50|0.2|50|0|0|categorical_crossentropy|Adam|200|0.154201|0.974|200|0.625|\n",
    "|NN|50|0.5|50|0|0|categorical_crossentropy|Adam|200|0.113652|0.9684|200|0.635|\n",
    "|NN|50|0.8|50|0|0|categorical_crossentropy|Adam|200|2.46691|0.3037|200|0.58|\n",
    "|NN|50|0|50|0|200|categorical_crossentropy|Adam|200|0.260561|0.9741|500|0.851304|\n",
    "|NN|50|0|50|0|400|categorical_crossentropy|Adam|200|0.242828|0.978|500|0.924|\n",
    "|NN|50|0|50|0|600|categorical_crossentropy|Adam|200|0.268606|0.9732|500|0.924|\n",
    "|NN|50|0|50|0|800|categorical_crossentropy|Adam|200|0.241012|0.9768|500|0.966|\n",
    "|NN|50|0|50|0|1000|categorical_crossentropy|Adam|200|0.264144|0.9738|500|0.934|\n",
    "|NN|50|0|50|0|1200|categorical_crossentropy|Adam|200|0.258430|0.975900|500|0.958|\n",
    "|NN|50|0|50|0|1400|categorical_crossentropy|Adam|200|0.370877|0.975700|500|0.958|\n",
    "|NN|50|0|50|0|1600|categorical_crossentropy|Adam|200|0.302795|0.972100|500|0.918|\n",
    "|NN|50|0|50|0|800|categorical_crossentropy|Adam|400|0.341100|0.978|?500|?0.958|\n",
    "|NN|50|0|50|0|800|categorical_crossentropy|Adam|100|0.179699|0.975600|?500|?0.958|\n",
    "|NN|50|0|50|0|800|categorical_crossentropy|Adam|300|0.338739|0.976600|?500|?0.958|\n",
    "|NN|50|0|50|0|800|categorical_crossentropy|Adam|150|0.211067|0.976100|?500|?0.958|\n",
    "|NN|50|0|50|0|800|categorical_crossentropy|Adam|250|0.211067|0.975100|?500|?0.958|\n",
    "\n",
    "|NN|50|0|50|0|0|categorical_crossentropy|Adam|400|0.380669|0.9729|500|0.588|\n",
    "|CNN|32|0.25|64|0.5|0|categorical_crossentropy|Adadelta|200|0.122603|0.9631|200|0.55|\n",
    "|CNN|32|0.25|64|0.5|0|mean_squared_error|Adadelta|200|0.017082|0.8916|200|0.62|\n",
    "|CNN|32|0.25|64|0.5|0|mean_absolute_error|Adadelta|200|0.022845|0.9011|200|0.585|\n",
    "|CNN|32|0.25|64|0.5|0|categorical_crossentropy|Adadelta|400|0.011047|0.9285|200|0.55|\n",
    "|CNN|32|0.25|64|0.5|0|categorical_crossentropy|Adam|200|0.068824|0.992700|200|0.83|\n",
    "|CNN|32|0.25|64|0.5|0|categorical_crossentropy|RMSProp|200|0.072486|0.988700|200|0.75|\n",
    "|CNN|32|0.25|64|0.5|0|categorical_crossentropy|SGD|200|0.029612|0.991300|200|0.775|\n",
    "|CNN|32|0.25|64|0.5|0|categorical_crossentropy|AdaGrad|200|0.046999|0.985400|200|0.625|\n",
    "|CNN|32|0.25|64|0.5|0|categorical_crossentropy|AdaMax|200|0.044286|0.992100|200|0.765|\n",
    "|CNN|32|0.25|64|0.5|0|categorical_crossentropy|Nadam|200|0.068032|0.993300|200|0.795|\n",
    "|CNN|32|0.25|64|0.5|0|categorical_crossentropy|Ftrl|200|2.301052|0.113500|200|0.1|\n",
    "|CNN|32|0.25|64|0.5|800|categorical_crossentropy|Adam|400|0.117345|0.993000|?200|?0.1|\n",
    "|CNN|32|0.25|64|0.5|42800|categorical_crossentropy|Adam|200|0.030696|0.997800|?200|?0.1|\n",
    "|CNN|32|0.25|64|0.5|42800|categorical_crossentropy|Adam|300|0.030780|0.998100|?200|?0.1|\n",
    "|CNN|32|0.25|64|0.5|42800|categorical_crossentropy|Adam|400|0.043855|0.997600|?200|?0.1|\n",
    "|CNN|32|0.25|64|0.5|42800|categorical_crossentropy|Adam|100|0.021059|0.997600|?200|?0.1|\n",
    "|CNN|48|0.25|64|0.5|42800|categorical_crossentropy|Adam|50|0.020265|0.998200|?200|?0.1|\n",
    "|CNN|48|0.25|96|0.5|42800|categorical_crossentropy|Adam|50|0.010477|0.997700|?200|?0.1|\n",
    "\n",
    "#### 3 layers\n",
    "|CNN|48|0.25|96|32|0.5|43600|categorical_crossentropy|Adam|50|0.008915|0.998500|?200|?0.1|\n",
    "|CNN|48|0.25|96|48|0.5|43600|categorical_crossentropy|Adam|50|0.006292|0.998400|?200|?0.1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Enhancements to CNNs\n",
    "* <b>Image augmentation</b>: Taking images in training set and manipulating them to create many altered versions of the same image. There are more images for our model to train on.\n",
    "Image manipulations make our model more robust. The distortions are random combinations of shifts, scaling, skewing, and compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=False) #True\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=False) #True\n",
    "\n",
    "jf_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    horizontal_flip=False) #True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually labaled images convert to matrix\n",
    "try:\n",
    "    X_labeled.head(1)\n",
    "    X_labeled_mx = np.array(X_labeled).reshape(-1,28,28)\n",
    "    X_labeled_mx.shape\n",
    "    n=0 # image number\n",
    "    print(y_labeled.iloc[n][0]) # correct label\n",
    "    plt.imshow(X_labeled_mx[n], cmap='gray'); # visual verification\n",
    "    try: y_labeled.shape\n",
    "    except NameError:\n",
    "        if os.path.exists(saved_labels):\n",
    "            print(\"Reading saved correct labels:\",saved_labels)\n",
    "            y_labeled = pd.read_csv(saved_labels, header=None)\n",
    "            # Save images\n",
    "            for n in range(X_labeled_mx.shape[0]):\n",
    "                # Convert numeric array to image\n",
    "                im = Image.fromarray((X_labeled_mx[n]).astype(np.uint8))\n",
    "                # Save image: subfolders are integer labels, create in advance!\n",
    "                im.save('images_train/'+str(int(y_labeled.iloc[n][0]))+'/'+str(n)+'.png')\n",
    "        else:\n",
    "            print(f\"Error: {saved_labels} does not exist!\")\n",
    "            print(\"Initialize y_labeled with image labels in vector form.\")\n",
    "    else:\n",
    "        print(f\"y_labeled was initialize earlier\")\n",
    "except NameError:\n",
    "    print(\"No manually added data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check on a sample to see the image generators work as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory('images_train/', target_size=(150,150), save_to_dir='images_augm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate variations of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for batch in train_datagen.flow_from_directory('images_train/', target_size=(150,150), save_to_dir='images_augm'):\n",
    "    i += 1\n",
    "    if(i>10): break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create GUI to predict digits\n",
    "Here an interactive window (GUI) is created where one can draw a digit using mouse and get a prediction of what this digit is. The GUI is implemented using Tkinter library that comes in the Python standard library. The `App` class is responsible for building the GUI for our app. It has a canvas where one can draw by capturing the mouse event. Functions are triggered by pushing control buttons: button 'Clear' clears canvas and button 'Recognise' activates the function `predict_digit()` to recognize the digit. This function takes the image as input and then uses the trained model to predict the digit. The predicted label and its probability percentage are displayed.\n",
    "### 4.1 Basic GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.python.org/3/library/tkinter.html\n",
    "def predict_digit(img):\n",
    "    #resize image to 28x28 pixels\n",
    "    img = img.resize((28,28))\n",
    "    #convert rgb to grayscale\n",
    "    img = img.convert('L')\n",
    "    img = np.array(img)\n",
    "    #reshaping to support our model input and normalizing\n",
    "    if mx: img = img.reshape(1,28,28,1)\n",
    "    else: img = img.reshape(1,28*28,1)\n",
    "    img = 1 - img/255.0\n",
    "    #predicting the class\n",
    "    res = model.predict([img])[0]\n",
    "    return np.argmax(res), max(res)\n",
    "\n",
    "class App(tk.Tk):\n",
    "    def __init__(self):\n",
    "        tk.Tk.__init__(self)\n",
    "        self.x = self.y = 0\n",
    "        self.width = self.height = 60 # sizes of handwritten digit box\n",
    "        # Creating elements\n",
    "        #self.canvas = tk.Canvas(self, width=28, height=28, bg = \"white\", cursor=\"cross\")\n",
    "        self.canvas = tk.Canvas(self, width=self.width, height=self.height, bg = \"white\", cursor=\"cross\")\n",
    "        self.label = tk.Label(self, text=\"Thinking..\", font=(\"Helvetica\", 48))\n",
    "        self.button_classify = tk.Button(self, text = \"Recognise\", command =         self.classify_handwriting) \n",
    "        self.button_clear = tk.Button(self, text = \"Clear\", command = self.clear_all)\n",
    "        # Grid structure\n",
    "        self.canvas.grid(row=0, column=0, pady=2, sticky=W, )\n",
    "        self.label.grid(row=0, column=1,pady=2, padx=2)\n",
    "        self.button_classify.grid(row=1, column=1, pady=2, padx=2)\n",
    "        self.button_clear.grid(row=1, column=0, pady=2)\n",
    "        #self.canvas.bind(\"<Motion>\", self.start_pos)\n",
    "        self.canvas.bind(\"<B1-Motion>\", self.draw_lines)\n",
    "    def clear_all(self):\n",
    "        self.canvas.delete(\"all\")\n",
    "    def classify_handwriting(self):\n",
    "        HWND = self.canvas.winfo_id() # get the handle of the canvas\n",
    "        rect = win32gui.GetWindowRect(HWND) # get the coordinate of the canvas\n",
    "        im = ImageGrab.grab(rect)\n",
    "        digit, acc = predict_digit(im)\n",
    "        self.label.configure(text= str(digit)+', '+ str(int(acc*100))+'%')\n",
    "    def draw_lines(self, event):\n",
    "        self.x = event.x\n",
    "        self.y = event.y\n",
    "        r=self.width//28\n",
    "        self.canvas.create_oval(self.x-r, self.y-r, self.x + r, self.y + r, fill='black')\n",
    "\n",
    "#if mx: model = load_model('cnn.e200.h5')\n",
    "#else: model = load_model('nn.e200.h5')\n",
    "app = App()\n",
    "mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 GUI with added functionality\n",
    "<b>Instructions:<b><br>\n",
    "* If a handwritten digit is misclassified, press \"Fix\" button to add the image to train set, which will be concatenated with the MNIST data before next training cycle.\n",
    "* Type in correct label and press \"Get label\" button to add the label to train set.\n",
    "* Press \"Save corrections\" button to save manually labeled images and labels, and clean memory.\n",
    "\n",
    "Accuracy % is displayed on the bottom right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.python.org/3/library/tkinter.html\n",
    "#https://datatofish.com/entry-box-tkinter/\n",
    "\n",
    "#model = load_model('nn.e200.h5')\n",
    "def predict_digit(img):\n",
    "    #resize image to 28x28 pixels\n",
    "    img = img.resize((28,28))\n",
    "    #convert rgb to grayscale\n",
    "    img = img.convert('L')\n",
    "    img = np.array(img)\n",
    "    #reshaping to support our model input and normalizing\n",
    "    if mx: img = img.reshape(1,28,28,1)\n",
    "    else: img = img.reshape(1,28*28,1)\n",
    "    img = img/255.0 # make pixels 0 to 1\n",
    "    img = 1 - img # negate: black background, white digit (as in training data)\n",
    "    #plt.imshow(img, cmap='gray');\n",
    "    #predicting the class\n",
    "    res = model.predict([img])[0]\n",
    "    return np.argmax(res), max(res)\n",
    "class App(tk.Tk):\n",
    "    def __init__(self):\n",
    "        tk.Tk.__init__(self)\n",
    "        # Parameters\n",
    "        self.x = self.y = 0\n",
    "        self.width = self.height = 60 # sizes of handwritten digit box\n",
    "        self.i = 0 # counter of fixed labels\n",
    "        self.cl = np.array([], dtype='float32') # corrected labels\n",
    "        self.images_misclass = pd.DataFrame([], dtype='int') # incorrectly classified images\n",
    "        self.n_ca = 0 # counter of correct answers\n",
    "        self.n_aa = 0 # counter of all answers\n",
    "        self.acc_running = 0 # running accuracy\n",
    "        # Creating elements\n",
    "        self.canvas = tk.Canvas(self, width=self.width, height=self.height, bg = \"white\", cursor=\"cross\")\n",
    "        self.label = tk.Label(self, text=\"Draw digit\", font=(\"Helvetica\", 48))\n",
    "        self.button_classify = tk.Button(self, text = \"Recognise\", command =         self.classify_handwriting) \n",
    "        self.button_clear = tk.Button(self, text = \"Clear\", command = self.clear_all)\n",
    "        self.button_fix = tk.Button(self, text = \"Fix\", command = self.fix)\n",
    "        self.entry1 = tk.Entry(self)\n",
    "        self.canvas.create_window((self.width+2,self.height+2), window=self.entry1)\n",
    "        self.button_getlabel = tk.Button(self, text='Get label', command=self.get_label)\n",
    "        self.button_save = tk.Button(self, text='Save corrections', command=self.save)\n",
    "        self.accuracy = tk.Label(self, text=\"Accuracy\", font=(\"Helvetica\", 28)) # running accuracy\n",
    "        # Grid structure\n",
    "        self.canvas.grid(row=0, column=0, pady=2, sticky=W, )\n",
    "        self.label.grid(row=0, column=1,pady=2, padx=2)\n",
    "        self.button_classify.grid(row=1, column=1, pady=2, padx=2)\n",
    "        self.button_clear.grid(row=1, column=0, pady=2)\n",
    "        self.canvas.bind(\"<B1-Motion>\", self.draw_lines)\n",
    "        self.button_fix.grid(row=2, column=0, pady=2, padx=2)\n",
    "        self.entry1.grid(row=2, column=1, pady=2, padx=2)\n",
    "        self.button_getlabel.grid(row=2, column=2, pady=2, padx=2)\n",
    "        self.button_save.grid(row=3, column=1, pady=2, padx=2)\n",
    "        self.accuracy.grid(row=3, column=2, pady=2, padx=2)\n",
    "        \n",
    "    def clear_all(self):\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.acc_running = self.n_ca/self.n_aa\n",
    "        self.accuracy.configure(text= str(int(self.acc_running*100))+'%')\n",
    "    def classify_handwriting(self):\n",
    "        HWND = self.canvas.winfo_id() # get the handle of the canvas\n",
    "        rect = win32gui.GetWindowRect(HWND) # get the coordinate of the canvas\n",
    "        im = ImageGrab.grab(rect)\n",
    "        digit, acc = predict_digit(im)\n",
    "        self.label.configure(text= str(digit)+', '+ str(int(acc*100))+'%')\n",
    "        self.n_ca += 1\n",
    "        self.n_aa += 1\n",
    "    def draw_lines(self, event):\n",
    "        self.x = event.x\n",
    "        self.y = event.y\n",
    "        r=self.width//28\n",
    "        self.canvas.create_oval(self.x-r, self.y-r, self.x + r, self.y + r, fill='black')\n",
    "    def get_label(self):\n",
    "        '''Get and append correct label.'''\n",
    "        y_label = int(self.entry1.get()) # text box gives a string\n",
    "        self.cl = np.append(self.cl, y_label) # append corrected label\n",
    "        print(self.i, self.cl)\n",
    "        self.i = self.i + 1 # update counter of corrected labels\n",
    "        self.canvas.delete(\"all\") # clear canvas\n",
    "        self.label.configure(text= 'Next image')\n",
    "        self.n_ca -= 1\n",
    "        self.acc_running = self.n_ca/self.n_aa\n",
    "        self.accuracy.configure(text= str(int(self.acc_running*100))+'%')\n",
    "    def fix(self):\n",
    "        '''Get handwritten image for subsequent saving. Type instructions. Get and append correct label.'''\n",
    "        global im # misclassified image\n",
    "        # Grab misclassified image from canvas\n",
    "        HWND = self.canvas.winfo_id() # get the handle of the canvas\n",
    "        rect = win32gui.GetWindowRect(HWND) # get the coordinate of the canvas\n",
    "        im = ImageGrab.grab(rect)\n",
    "        # Crop image padding\n",
    "        left = 2 # pixels to crop on the left\n",
    "        top = 2 # pixels to crop on top\n",
    "        right = self.width + left\n",
    "        bottom = self.height + top\n",
    "        im = im.crop((left, top, right, bottom))\n",
    "        # Grayscale, reshape, negate\n",
    "        im = im.convert('L') # Convert rgb to grayscale\n",
    "        im = im.resize((28,28)) # Reshape to support our training model input (MNIST) and normalizing\n",
    "        im = np.array(im) # convert to numeric array\n",
    "        im = 255 - im # negate: black background, white digit (as in training data)\n",
    "        # Append misclassified image\n",
    "        self.images_misclass = self.images_misclass.append(pd.DataFrame(im.reshape(1,28*28)), ignore_index=True)\n",
    "        plt.imshow(im, cmap='gray')\n",
    "        # Type instructions\n",
    "        self.label.configure(text=\"Enter label\\nHit Get label\", font=(\"Helvetica\", 48))\n",
    "    def save(self):\n",
    "        saved_images = 'images.csv'\n",
    "        saved_labels = 'labels.csv'\n",
    "        #print(\"Saving misclassified images:\", saved_images)\n",
    "        pd.DataFrame(self.images_misclass).to_csv(saved_images, index=False, header=None, mode='a') # append images to file\n",
    "        #print(\"Saving correct labels:\", saved_labels)\n",
    "        pd.DataFrame(self.cl).to_csv(saved_labels, index=False, header=None, mode='a') # append labels to file\n",
    "        self.images_misclass = pd.DataFrame([], dtype='int') # after saving, clear the memory\n",
    "        self.cl = np.array([], dtype='float32')\n",
    "        self.canvas.delete(\"all\") # clear canvas\n",
    "        self.label.configure(text= 'Next image')\n",
    "\n",
    "#if mx: model = load_model('cnn.e200.h5')\n",
    "#else: model = load_model('nn.e200.h5')\n",
    "app = App()\n",
    "mainloop();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recognition of handwritten digits was tested by iterating over the digits from 0 to 9 until 200 misclassified digits were obtained. We started tracking the actual digits and their classifications after geting 78 misclassified digits during the development of GUI. At the end, we had 308 correctly classified and 122 misclassified images, so that the accuracy was $308/430 = 0.716279$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mann-Whitney U rank test\n",
    "The assumptions of this test:\n",
    "* The observations from the two groups should be randomly selected from the target populations.\n",
    "* Observations are independent of each other. Indeed, samples are taken from tests of the models re-trained using different number of manually labeled images.\n",
    "* Observations should be continuous or ordinal. We have ordinal 0, 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "stats.mannwhitneyu(correctness_ml0, correctness_ml200, use_continuity=False, alternative='less')\n",
    "# The Mann-Whitney U statistic and p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mannwhitneyu(correctness_ml200, correctness_ml400, use_continuity=False, alternative='less')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a tiny p-value, we can confidently conclude that re-training with manually added labels leads to a statistically significant increase in the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "Let's see how the digit recognition accuracy depend on the number of training epochs for different number of manually labeled images added to the training set.\n",
    "\n",
    "Next we study the effect of the number of neurons in the 2nd layer.\n",
    "\n",
    "Then we use a dropout before 2nd layer.\n",
    "\n",
    "Neural networks are trained using stochastic gradient descent optimization algorithm and weights are updated using the backpropagation of error algorithm. This requires that you choose a loss function when designing and configuring your model, so we shall try different choices. The â€œgradientâ€ in gradient descent refers to an error gradient. The model with a given set of weights is used to make predictions and the error for those predictions is calculated. The gradient descent algorithm seeks to change the weights so that the next evaluation reduces the error, meaning the optimization algorithm is navigating down the gradient (or slope) of error. Since we seek to minimize the error, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply â€œloss.â€ The cost or loss function has an important job in that it must faithfully distill all aspects of the model down into a single number in such a way that improvements in that number are a sign of a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx=True; ml=0; epochs=200; loss='mean_absolute_error'; # current model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gui.py # update model filename in it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx=False; nl1=50; dropout=0; nl2=50; ml=400; epochs=200;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer='Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance of NN 50-50 digit classification: \"+str(ml)+\" manual labels, \"+str(epochs)+\" epochs.\")\n",
    "# After adding 200 manually labeled misclassified images to the training set and re-training the NN model:\n",
    "if (ml==0) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*50),\n",
    "        'prediction':np.array([0,1,2,3,4,0,3,7,2,9,0,1,2,3,4,5,6,7,3,9,0,1,2,3,4,5,6,3,8,9,6,1,2,3,6,5,5,2,8,3,0,8,2,3,4,5,5,7,3,5,\\\n",
    "        0,2,2,3,4,0,6,2,5,9,0,1,2,2,4,1,6,3,6,3,0,1,2,3,4,5,6,8,8,3,0,1,2,3,4,5,6,1,8,9,0,1,2,3,4,5,6,7,8,1,5,1,2,3,9,7,6,2,8,7,\\\n",
    "        0,1,2,3,4,5,6,2,8,7,6,1,2,3,4,5,6,1,8,5,7,3,2,3,4,5,6,7,5,6,5,1,2,3,4,5,5,3,8,9,0,1,2,3,4,5,6,3,8,3,9,1,2,3,4,5,6,8,3,3,\\\n",
    "        0,1,2,3,6,5,5,7,8,3,0,1,2,3,9,5,6,8,2,3,0,1,2,3,4,3,6,8,3,9,0,1,2,3,4,5,6,1,8,8,0,1,2,3,4,5,6,7,8,3,0,8,2,3,4,5,6,7,2,2,\\\n",
    "        3,6,2,3,4,5,5,7,8,9,0,1,2,3,4,5,6,7,3,9,0,9,2,3,4,5,6,7,3,3,0,1,2,3,4,5,6,2,8,5,0,8,2,3,4,3,6,7,2,3,6,2,2,3,4,7,6,1,8,9,\\\n",
    "        0,1,2,3,4,5,6,2,4,0,0,1,2,3,4,6,5,7,8,7,0,1,2,3,4,5,5,7,8,3,0,1,2,3,4,5,7,2,2,3,0,1,3,3,4,5,6,2,1,9,0,1,2,3,4,3,6,7,3,3,\\\n",
    "        0,3,3,3,4,5,6,3,3,3,0,1,2,3,4,5,5,7,2,8,0,1,2,3,4,5,6,2,6,9,0,1,2,3,4,5,6,1,8,9,0,1,2,3,4,5,0,7,3,9,0,1,2,3,6,5,6,2,8,3,\\\n",
    "        0,1,2,5,7,5,5,7,0,9,0,1,2,3,4,5,6,7,8,3,3,3,2,3,4,5,6,3,3,5,0,8,2,8,4,5,6,3,3,9,6,1,3,3,4,5,6,3,2,3,0,1,2,3,3,5,2,3,8,3,\\\n",
    "        0,1,2,3,4,5,6,3,8,3,0,8,2,3,5,3,6,3,2,3,0,8,2,3,4,3,5,7,3,3])}\n",
    "    correctness_nn_50_50_ml0_e200 = list(map(int, perf['actual'] == perf['prediction'])) # for statistical test\n",
    "elif (ml==200) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*115),\n",
    "        'prediction':np.array([0,1,2,3,4,6,6,7,4,9,9,1,2,3,4,5,6,7,8,9,8,1,2,3,4,5,6,7,8,9,9,1,2,8,4,5,6,7,8,9,\\\n",
    "        9,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,5,7,8,9,0,8,2,3,4,5,8,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,\\\n",
    "        0,1,3,3,4,5,8,7,8,9,0,1,2,3,4,5,6,9,8,9,0,1,2,3,4,5,6,7,8,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "        0,1,2,3,9,6,0,7,8,3,7,1,2,9,4,5,5,7,8,9,0,1,2,3,4,5,6,7,8,9,8,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,2,8,3,9,1,2,9,4,5,6,7,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,2,8,0,1,2,3,4,5,6,7,2,9,0,1,2,3,4,8,6,7,8,3,0,8,2,9,4,5,5,8,8,7,0,1,2,3,4,5,6,7,8,9,7,8,2,3,4,5,6,7,6,0,\\\n",
    "        0,1,2,8,4,5,8,2,8,9,0,1,2,3,4,5,6,8,2,9,0,1,2,3,4,5,6,7,8,9,0,7,2,3,4,8,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,2,2,3,4,9,6,7,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,9,8,8,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,8,0,1,2,3,4,9,8,7,8,9,0,1,2,3,4,8,6,7,8,9,\\\n",
    "        0,8,2,9,4,9,6,7,8,9,0,2,2,3,9,5,6,7,8,2,0,1,8,2,4,5,5,7,3,9,9,1,2,3,9,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,0,1,2,3,9,5,6,2,8,8,\\\n",
    "        0,0,2,3,4,5,6,7,8,9,0,1,2,3,4,9,4,7,8,3,1,1,2,3,4,5,6,1,8,0,0,1,2,3,4,5,6,7,8,8,0,1,2,3,9,5,6,7,3,9,0,1,2,1,4,5,6,7,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,7,5,6,7,4,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,6,8,2,3,4,8,6,7,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,8,8,0,1,2,3,4,9,6,9,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,0,6,7,8,9,0,1,2,3,4,6,8,7,8,9,0,1,2,3,4,5,8,7,8,9,\\\n",
    "        6,1,2,3,4,5,6,7,8,9,0,1,8,5,4,5,6,2,8,9,9,1,2,3,4,5,6,7,8,9,0,1,2,3,4,8,0,7,6,9,0,1,2,3,9,5,6,7,1,9,6,1,2,2,4,5,6,7,8,9,\\\n",
    "        0,1,9,3,4,5,5,7,8,9,0,1,2,1,4,5,6,7,8,9,0,1,2,3,4,5,5,7,8,9,0,1,2,3,4,9,8,2,6,9,1,1,2,9,4,5,6,7,8,9,0,1,2,3,4,9,6,3,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,9,9,0,1,2,3,4,9,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,9,6,7,8,9,\\\n",
    "        0,9,2,3,4,5,6,7,8,9,9,1,0,9,4,5,5,4,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,8,6,8,8,9,0,1,2,3,4,9,6,7,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,7,1,8,3,4,9,6,2,4,0,0,1,2,1,4,5,6,7,8,9,0,1,2,3,9,5,6,7,9,3,0,8,2,3,4,6,6,7,8,9,\\\n",
    "        9,1,2,3,4,5,6,7,8,9,9,1,2,3,4,5,6,7,8,9,0,7,2,3,4,5,6,2,8,9,0,1,2,3,4,5,8,7,8,9,0,1,2,3,9,5,5,7,8,9,0,1,2,3,4,9,6,7,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,7,9,1,2,3,4,5,5,7,8,9,0,1,2,2,4,5,6,9,8,9,0,1,2,3,4,5,6,7,5,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,9,5,8,7,8,9,0,1,2,3,7,5,6,7,8,9,\\\n",
    "        9,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,9,5,8,2,4,5])}\n",
    "    correctness_nn_50_50_ml200_e200 = list(map(int, perf['actual'] == perf['prediction']))\n",
    "elif (ml==400) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*50),\n",
    "        'prediction':np.array([0,1,2,3,4,5,6,7,8,9,6,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,8,7,0,1,2,9,4,5,8,7,6,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,9,1,2,3,4,5,6,7,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,9,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,7,2,3,4,5,6,7,8,7,0,1,2,3,4,5,6,7,8,9,\\\n",
    "        0,1,3,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,8,8,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,8,7,6,7,0,1,2,3,4,5,5,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,\\\n",
    "        9,1,2,3,9,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,6,6,2,2,9,8,1,2,3,4,5,6,7,8,0,0,1,2,3,4,5,6,7,7,9,0,1,2,3,4,5,8,7,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,3,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,6,5,6,7,6,9,0,1,2,3,4,5,6,2,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "        0,1,2,3,4,5,8,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,8,7,8,9,0,1,2,3,4,5,6,7,8,9,9,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,8,7,8,9,\\\n",
    "        0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,2,9])}\n",
    "    correctness_nn_50_50_ml400_e200 = list(map(int, perf['actual'] == perf['prediction']))\n",
    "#elif (ml==600) & (epochs==200) & (err == True):\n",
    "    #perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*50),\n",
    "    #    'prediction':np.array([0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "    #    0,1,2,3,4,5,8,7,8,9,0,1,2,2,4,5,6,7,8,8,0,1,2,3,5,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,\\\n",
    "    #    9,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,2,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "    #    0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,2,1,2,3,4,5,6,7,8,9,0,1,2,3,9,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,\\\n",
    "    #    2,1,2,3,4,5,6,7,3,7,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,9,6,7,8,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "    #    0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,5,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "    #    7,2,2,3,4,5,6,7,8,7,9,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,9,1,2,3,4,5,6,2,8,9,\\\n",
    "    #    0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,\\\n",
    "    #    7,1,9,3,8,5,6,7,1,6,9,1,2,3,4,5,6,7,8,8,0,1,2,3,4,5,6,7,8,9])}\n",
    "    #correctness_nn_50_50_ml600_e200_old = list(map(int, perf['actual'] == perf['prediction']))\n",
    "elif (ml==600) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*50),\n",
    "     'prediction':np.array([0,1,8,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,9,1,2,3,4,6,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,2,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,5,1,8,3,4,5,8,7,8,9,0,1,2,3,4,5,8,7,8,9,0,1,2,3,4,5,6,7,8,9,9,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,1,4,5,6,7,8,9,0,1,2,7,4,9,6,2,8,3,0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9,0,7,2,3,4,3,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,8,2,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,9,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,\\\n",
    "     0,1,3,5,4,5,6,7,8,9,0,1,1,9,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,8,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,3,8,9,0,1,2,3,4,5,6,7,8,9,0,1,1,3,4,9,6,7,8,9,9,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,5,7,8,9,\\\n",
    "     0,1,2,3,4,5,0,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,3,8,9,0,1,2,3,4,5,6,7,8,9,0,1,8,3,4,5,6,7,8,9,0,1,2,3,4,5,8,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,3,8,9,0,1,2,3,4,5,0,7,8,9])}\n",
    "    correctness_nn_50_50_ml600_e200 = list(map(int, perf['actual'] == perf['prediction']))\n",
    "elif (ml==800) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*180),\n",
    "     'prediction':np.array([0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,3,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,6,6,7,8,9,0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,3,9,0,1,2,3,4,5,6,7,8,9,0,7,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,1,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,3,6,7,8,8,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,8,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,1,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,8,0,1,2,2,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,3,9,0,1,1,3,4,5,8,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,0,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,5,4,5,6,8,8,9,4,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,4,0,1,2,3,4,5,6,7,8,9, 9,1,2,3,4,6,6,3,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,9,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,8,0,1,2,3,4,5,6,8,8,9,0,5,2,3,4,5,6,7,8,9,0,1,2,3,4,6,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,6,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,4,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,0,7,8,9,0,1,2,3,4,5,0,7,8,9,0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,5,0,1,2,3,7,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,5,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,2,8,9,0,1,2,3,4,5,6,7,8,4,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,7,9,7,5,6,7,8,9,0,1,2,3,4,5,0,7,8,9,0,1,2,3,4,5,6,7,8,9,0,0,2,3,7,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,8,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,9,5,6,7,8,9,0,1,2,3,4,5,6,7,5,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,9,4,5,6,9,8,0,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,8,\\\n",
    "     0,1,2,3,4,5,6,7,8,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,5,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,9,8,9,\\\n",
    "     0,1,2,3,4,5,6,2,8,9,0,1,2,3,4,5,8,7,6,9,0,1,2,9,4,5,6,2,0,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,5,5,6,7,8,9,0,1,9,7,6,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,3,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,6,1,2,3,9,5,6,7,8,9,0,1,2,3,4,5,0,7,8,2,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,2,8,9,0,1,2,3,4,5,6,7,3,9,0,1,2,3,4,5,6,7,8,9,9,1,2,1,4,5,6,7,2,9,0,1,2,3,6,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,5,2,3,4,9,6,7,8,9,0,5,2,3,7,5,6,7,8,7,0,1,2,3,4,5,6,7,8,7,0,1,3,3,4,6,9,7,8,7,0,1,2,5,4,5,6,9,6,9,\\\n",
    "     0,1,2,3,4,3,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,6,5,7,8,0,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,9,1,2,3,4,5,6,7,8,9,0,1,2,3,9,5,6,7,8,9,0,1,2,3,4,5,6,7,8,7,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,8,7,8,9,0,6,2,3,4,5,6,7,8,9,0,1,2,3,4,5,8,7,8,7,0,1,2,3,4,5,6,7,8,1,0,1,2,3,1,5,6,7,8,9,6,7,2,3,4,5,6,7,8,9,\\\n",
    "     0,9,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,7,5,6,7,8,9,0,5,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,6,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,1,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,1,5,9,7,8,9,0,1,2,3,8,6,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,6,0,7,3,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,9,3,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,0,0,2,8,9,0,1,2,3,4,6,6,8,8,9])}\n",
    "    correctness_nn_50_50_ml800_e200 = list(map(int, perf['actual'] == perf['prediction']))\n",
    "elif (ml==800) & (epochs==400):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*2),\n",
    "     'prediction':np.array([0,6,2,3,4,0,6,7,8,9,0,1,2,3,4,5,6,7,8,9])} # HERE Apr 28\n",
    "    correctness_nn_50_50_ml800_e400 = list(map(int, perf['actual'] == perf['prediction']))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "elif (ml==1000) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*50),\n",
    "     'prediction':np.array([0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9,6,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,7,2,3,4,5,6,7,0,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,9,6,7,8,9,0,1,7,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,2,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,9,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,7,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,9,5,6,7,0,0,0,1,2,3,4,5,6,7,2,9,0,1,5,5,4,5,6,7,8,9,0,8,2,3,4,5,6,7,1,9,0,1,2,9,4,5,6,7,6,9,\\\n",
    "     0,1,7,3,4,5,6,7,8,9,0,1,2,3,4,5,0,7,8,9,0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,2,1,3,8,4,5,6,7,8,1,\\\n",
    "     0,1,2,3,4,5,0,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,9,3,4,5,5,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,9,0,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9])}\n",
    "    correctness_nn_50_50_ml1000_e200 = list(map(int, perf['actual'] == perf['prediction']))\n",
    "elif (ml==1200) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*50),\n",
    "     'prediction':np.array([0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,0,7,8,9,0,1,2,3,4,5,6,7,0,9,\\\n",
    "     0,1,2,3,4,1,6,7,8,9,0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9,7,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,2,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9,0,4,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,8,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,9,8,4,9,6,7,8,9,0,3,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,0,3,4,5,6,7,8,9,0,4,2,9,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,8,6,7,8,9,0,1,2,3,4,5,6,7,8,9,6,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9])}\n",
    "    correctness_nn_50_50_ml1200_e200 = list(map(int, perf['actual'] == perf['prediction']))\n",
    "elif (ml==1400) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*50),\n",
    "     'prediction':np.array([0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,2,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,2,4,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9,0,9,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,9,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,7,9,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,7,5,6,7,8,9,0,1,2,3,4,9,6,7,8,9,0,1,2,3,4,5,6,9,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,8,7,8,4,0,1,2,9,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,1,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,8,7,8,9,\\\n",
    "     9,1,2,3,4,5,6,7,8,9,0,1,2,3,7,5,0,8,8,9,0,1,2,3,4,5,8,7,8,9])}\n",
    "    correctness_nn_50_50_ml1400_e200 = list(map(int, perf['actual'] == perf['prediction']))\n",
    "elif (ml==1600) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*100),\n",
    "     'prediction':np.array([0,1,2,3,4,5,6,7,8,4,0,1,2,9,4,5,6,7,8,9,0,1,2,3,9,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,9,5,6,2,8,9,0,1,2,3,4,5,6,7,8,9,6,1,2,3,4,5,6,7,8,9,0,1,2,9,4,5,6,7,8,9,0,1,2,3,4,5,6,7,3,9,\\\n",
    "     0,1,2,3,9,5,6,7,6,8,0,1,2,3,4,5,8,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,2,9,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,2,3,4,5,6,7,8,1,\\\n",
    "     0,1,3,9,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,9,4,9,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,9,6,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,8,7,8,9,0,1,9,5,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,7,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,6,6,7,8,9,0,1,2,3,4,5,9,7,8,9,0,1,2,3,4,5,6,7,8,4,6,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,9,6,7,8,9,9,1,2,9,4,5,0,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,0,3,4,5,6,7,8,9,0,1,2,5,4,7,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,7,5,0,9,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,9,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,9,3,4,5,6,7,8,9,0,1,7,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,2,8,9,0,1,2,3,4,5,6,7,8,4,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,9,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,6,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,6,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,4,9,0,9,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,6,9,0,1,2,5,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,9,2,3,4,5,6,7,8,9,0,1,2,9,4,5,6,8,8,9,0,1,2,3,4,5,6,7,8,9,0,1,8,9,4,5,6,7,8,9,0,1,9,3,4,5,6,7,8,9,\\\n",
    "     0,6,2,3,4,5,6,7,8,9,0,1,2,9,9,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,9,5,6,7,8,7,\\\n",
    "     0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,5,7,8,9,0,1,2,3,4,5,6,7,8,9])}\n",
    "    correctness_nn_50_50_ml1600_e200 = list(map(int, perf['actual'] == perf['prediction'])) # HERE ml=1600\n",
    "    \n",
    "    \n",
    "\n",
    "elif (ml==0) & (epochs==400):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*50),\n",
    "        'prediction':np.array([0,1,2,3,4,3,5,2,8,3,0,5,2,3,4,3,9,7,3,9,0,8,2,3,4,3,5,2,3,3,6,5,2,3,4,5,8,3,8,3,6,1,2,3,4,5,6,3,3,9,\\\n",
    "            0,1,2,3,4,3,8,3,9,3,0,1,2,3,4,5,6,3,8,3,7,1,2,3,4,3,3,2,8,8,3,3,2,3,4,5,5,7,2,9,0,3,2,3,4,5,6,7,2,9,6,1,5,3,4,5,5,7,2,3,\\\n",
    "            0,1,2,3,4,5,5,3,8,9,0,1,2,3,1,3,5,2,8,3,0,1,2,3,9,3,0,3,9,3,0,1,2,3,7,5,6,3,3,9,0,1,2,3,4,5,6,1,8,3,2,1,2,3,4,5,8,2,2,3,\\\n",
    "            0,1,2,1,4,5,3,7,3,9,0,1,2,3,4,5,6,3,3,3,3,1,2,3,4,3,6,2,8,3,9,1,2,3,4,5,5,2,8,2,0,1,2,3,4,3,5,1,5,3,8,5,2,3,1,3,3,7,6,3,\\\n",
    "            2,1,2,3,7,5,6,7,3,3,0,1,2,3,4,5,6,3,8,3,0,1,2,3,4,3,3,7,2,3,9,1,2,3,4,5,5,7,6,3,0,1,2,3,4,3,5,2,2,3,2,1,2,3,4,5,6,3,8,3,\\\n",
    "            0,1,2,3,1,3,5,2,8,3,8,1,2,3,4,5,8,3,2,3,0,1,2,3,4,3,5,2,8,3,0,8,2,3,4,5,6,2,8,2,0,1,2,3,4,8,8,7,3,6,0,1,2,3,4,3,6,2,8,2,\\\n",
    "            0,5,2,3,4,3,5,2,3,3,0,1,2,3,4,3,6,2,2,3,0,1,2,3,4,3,6,3,8,3,0,8,2,3,4,3,8,3,3,3,0,1,2,3,1,5,5,2,8,8,0,1,2,3,4,5,3,8,6,3,\\\n",
    "            0,8,3,3,3,3,6,3,8,3,0,1,2,3,1,5,5,2,3,9,0,1,2,3,4,3,5,3,3,0,0,2,2,3,1,3,6,2,3,3,0,1,2,3,9,5,6,7,8,3,0,1,2,3,4,5,8,2,2,9,\\\n",
    "            0,8,2,3,4,3,6,2,3,3,0,5,2,3,4,3,6,2,3,3,7,1,2,3,4,5,5,2,8,9])}\n",
    "    correctness_nn_50_50_ml0_e400 = list(map(int, perf['actual'] == perf['prediction']))\n",
    "contingency_matrix = pd.crosstab(perf['prediction'], perf['actual'], rownames=['Predicted'], colnames=['Actual'])\n",
    "\n",
    "# Confusion matrix calculated manually\n",
    "def cm(digits,classf):\n",
    "    CM = np.array([[0]*10]*10)\n",
    "    for i in range(len(digits)):\n",
    "        CM[digits[i]][classf[i]] = CM[digits[i]][classf[i]] + 1\n",
    "    return CM\n",
    "#print(contingency_matrix)\n",
    "#print(\"Sample size:\",len(perf['actual']))\n",
    "#print(\"Accuracy=\",np.sum([contingency_matrix[i][i] for i in range(10)]) / np.sum(np.sum(contingency_matrix)))\n",
    "# Plot contingency matrix\n",
    "\n",
    "plt.clf()\n",
    "res = sn.heatmap(contingency_matrix.T, annot=True, fmt='d', cmap=\"YlGnBu\", cbar=False)\n",
    "plt.title(f\"Sample size = {len(perf['actual'])}, Accuracy = {np.sum([contingency_matrix[i][i] for i in range(10)]) / np.sum(np.sum(contingency_matrix))}\")\n",
    "plt.savefig(\"fig/cm.nn_50_50.ml\"+str(ml)+\".e\"+str(epochs)+\".png\", bbox_inches='tight', dpi=100)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness = perf['actual'] == perf['prediction']\n",
    "accuracy_cumulative = []\n",
    "for i in range(len(correctness)):\n",
    "    accuracy_cumulative.append(correctness[:i+1].sum()/(i+1))\n",
    "accuracy_cumulative_nn_50_50_ml1600_e200 = accuracy_cumulative\n",
    "pd.DataFrame(accuracy_cumulative).to_csv('performance/accuracy_cumulative_nn_50_50_ml'+str(ml)+'_e'+str(epochs)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(accuracy_cumulative_ml0, bins=100);\n",
    "plt.hist(accuracy_cumulative_ml200, bins=100);\n",
    "plt.hist(accuracy_cumulative_ml400, bins=100);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gui.py # update model filename in it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Current model parameters\n",
    "#loss='categorical_crossentropy';\n",
    "#loss='mean_squared_error';\n",
    "(mx==True) & (nl1==32) & (nl2==64) & (dropout==0.25) & (ml==0) & (epochs==200) & (loss=='categorical_crossentropy')\\\n",
    "& (optimizer == 'Ftrl')\n",
    "#mx=True; nl1=32; nl2=64; dropout=0.25; ml=0; epochs=400; loss='categorical_crossentropy';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Performance of NN_{nl1}_{nl2} digit classification: \"+str(ml)+\" manual labels, \"+str(epochs)+\" epochs.\")\n",
    "if (nl1==50) & (nl2==25) & (ml==0) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([9,1,3,3,4,5,5,2,2,3,6,8,2,3,9,3,6,7,3,9,4,1,2,3,9,5,3,2,8,3,0,5,2,8,4,5,6,7,3,3,0,1,2,3,4,3,6,2,2,3,\\\n",
    "     0,1,2,3,9,6,3,7,8,3,0,1,2,3,4,5,6,2,8,3,0,3,3,3,4,3,6,7,3,3,0,8,2,3,4,9,3,2,8,3,0,1,5,3,4,5,5,2,2,9,0,8,2,3,4,5,5,0,3,9,\\\n",
    "     2,8,2,3,4,5,8,2,8,3,6,1,2,3,4,5,9,3,3,8,3,8,2,3,4,5,6,2,2,9,0,1,2,3,4,5,6,1,8,3,0,8,2,3,4,5,8,2,5,6,9,1,3,3,4,5,8,2,8,9,\\\n",
    "     0,8,2,3,8,5,3,7,8,9,0,8,2,3,4,5,3,7,2,9,0,1,2,3,4,5,8,8,8,3])}\n",
    "    correctness_nn_50_25_ml0_e200 = list(map(int, perf['actual'] == perf['prediction'])) # for statistical test\n",
    "elif (nl1==50) & (nl2==75) & (ml==0) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([0,8,3,3,9,5,9,2,6,3,0,1,2,3,4,5,6,1,2,3,0,6,2,3,9,5,5,7,8,3,0,8,2,3,4,3,5,7,8,3,2,8,2,3,4,5,5,7,8,8,\\\n",
    "     0,1,2,5,4,3,2,7,3,3,7,8,2,2,4,5,8,7,3,3,0,8,2,3,4,3,6,2,2,7,0,5,2,3,2,5,8,7,8,9,2,1,2,9,9,5,6,2,6,9,0,1,2,3,4,5,8,3,3,3,\\\n",
    "     0,1,2,5,2,5,5,3,8,3,4,1,2,3,9,5,3,7,8,3,0,1,2,9,4,5,6,5,8,3,0,3,2,3,4,5,8,8,8,9,0,1,2,3,4,5,2,1,2,9,2,1,2,3,4,5,8,2,3,3,\\\n",
    "     3,1,2,3,4,3,6,2,1,9,0,9,2,3,4,5,5,2,3,3,0,1,2,3,4,5,8,2,2,3])}\n",
    "    correctness_nn_50_75_ml0_e200 = list(map(int, perf['actual'] == perf['prediction'])) # for statistical test\n",
    "elif (nl1==50) & (dropout==0.2) & (nl2==50) & (ml==0) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([0,0,2,3,4,6,5,2,6,9,0,8,2,3,4,5,6,7,3,9,0,1,2,3,4,5,5,2,8,9,0,1,2,3,4,5,5,3,8,3,0,8,2,3,4,5,6,2,3,3,\\\n",
    "     0,9,2,3,4,5,8,2,8,9,0,1,2,3,7,3,8,2,9,9,0,5,2,3,4,5,6,2,9,5,9,8,2,3,9,6,5,3,3,8,0,8,3,3,4,5,5,2,3,9,0,1,2,3,4,5,6,2,8,8,\\\n",
    "     6,8,2,3,4,3,5,3,8,3,0,1,2,3,8,5,6,7,3,3,4,1,2,3,3,5,6,2,8,9,0,5,2,3,4,6,6,2,3,9,9,1,2,3,4,5,6,4,8,3,0,1,2,3,4,6,5,2,8,9,\\\n",
    "     0,1,2,3,4,5,5,7,8,9,0,1,2,3,9,5,3,7,3,3,8,1,2,3,1,5,2,2,3,9])}\n",
    "    correctness_nn_50_d2_50_ml0_e200 = list(map(int, perf['actual'] == perf['prediction'])) # for statistical test\n",
    "elif (nl1==50) & (dropout==0.5) & (nl2==50) & (ml==0) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([0,1,2,3,4,5,6,2,3,3,0,1,2,3,9,5,6,7,8,9,0,1,2,3,4,5,6,8,3,9,0,3,2,3,9,5,5,7,8,9,2,1,2,3,4,3,6,7,6,5,\\\n",
    "     2,1,5,3,4,5,3,2,8,3,0,1,2,3,4,5,6,2,2,9,2,1,2,3,4,5,8,2,3,3,0,1,2,3,4,5,6,7,3,9,0,3,3,3,4,5,5,7,2,3,0,3,2,3,9,3,8,2,8,9,\\\n",
    "     6,1,2,3,3,5,6,1,3,2,4,8,0,3,4,5,8,5,8,9,2,1,2,3,9,5,6,3,8,9,0,8,2,3,9,5,2,3,3,9,0,8,2,3,4,5,8,2,3,3,8,1,2,3,4,5,6,7,3,9,\\\n",
    "     0,1,2,3,4,5,6,8,3,3,0,0,2,3,9,5,6,2,3,9,2,6,2,3,4,5,5,7,2,3])}\n",
    "elif (nl1==50) & (dropout==0.8) & (nl2==50) & (ml==0) & (epochs==200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([6,1,2,3,4,5,3,2,8,3,0,8,3,3,9,5,5,8,8,3,0,7,2,3,4,5,0,2,8,9,2,1,2,3,9,5,8,7,8,9,3,1,2,3,4,5,6,7,8,3,\\\n",
    "     0,8,3,3,4,5,6,2,3,6,7,9,3,2,4,5,3,3,8,3,0,1,2,3,4,5,5,8,8,3,0,1,2,3,4,5,6,2,8,3,9,1,2,3,4,5,5,3,8,3,0,1,2,3,4,5,6,7,2,9,\\\n",
    "     2,5,2,5,4,5,6,7,3,3,0,1,2,3,9,5,6,9,3,3,0,1,2,3,4,6,3,7,8,3,0,3,2,3,4,5,9,3,6,9,4,8,2,3,9,5,6,2,3,3,0,5,2,3,4,5,5,8,5,2,\\\n",
    "     4,8,2,3,8,5,3,7,2,3,6,8,2,3,9,5,8,7,3,9,0,7,2,3,1,6,5,3,8,3])}\n",
    "elif (mx==True): # Convolutional Neural Net\n",
    " if loss == 'categorical_crossentropy':\n",
    "  if optimizer == 'Adadelta':\n",
    "   if epochs == 200:\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([0,3,2,3,8,3,6,2,8,9,0,1,2,3,4,5,6,2,8,3,0,1,2,3,9,5,6,2,3,7,0,1,2,3,9,5,5,5,9,3,8,1,2,3,9,5,3,7,8,7,\\\n",
    "     2,5,2,2,4,5,5,3,3,3,0,6,2,3,4,5,0,2,2,3,0,1,2,3,4,5,5,1,2,9,6,8,2,3,4,5,6,3,3,3,0,5,2,3,9,5,5,2,8,3,6,8,2,3,4,5,5,2,6,3,\\\n",
    "     0,8,2,3,9,5,8,8,3,9,0,1,2,3,8,5,6,2,3,9,0,1,2,3,4,6,3,1,8,9,0,1,2,3,3,5,6,7,3,9,6,2,3,3,9,5,6,2,8,3,0,8,2,3,4,3,2,7,5,2,\\\n",
    "     0,8,2,3,4,3,6,5,3,3,0,3,2,3,3,5,2,2,8,3,4,8,2,3,6,3,6,3,8,3])}\n",
    "   elif epochs == 400:\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "    'prediction':np.array([0,1,2,3,4,3,6,3,2,9,0,8,2,3,9,5,2,8,8,3,2,8,2,3,9,5,2,2,8,9,0,6,2,3,4,5,6,2,8,8,0,1,2,3,4,5,8,7,3,9,\\\n",
    "     0,3,2,3,4,8,5,2,3,9,9,1,2,3,4,5,5,2,9,3,0,1,2,3,4,3,2,2,6,3,0,1,2,3,7,3,3,6,8,5,0,1,2,3,4,3,6,2,3,3,0,0,3,3,4,5,7,7,3,8,\\\n",
    "     0,6,2,3,4,6,3,7,8,9,0,1,2,3,4,5,0,2,3,3,0,4,2,3,9,3,5,7,6,3,6,8,3,3,4,3,6,5,5,3,6,1,2,3,2,5,3,3,8,9,0,8,3,3,5,5,9,3,3,3,\\\n",
    "     3,2,2,3,4,5,2,7,2,2,0,1,2,3,4,5,6,7,6,3,2,1,2,3,4,3,5,8,8,3])}\n",
    "  elif optimizer == 'Adam':\n",
    "   if epochs == 200:\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([0,1,2,3,4,5,5,7,8,3,0,1,2,3,4,5,6,7,8,5,0,1,2,3,4,5,5,2,3,3,0,1,2,3,4,5,5,2,8,5,0,1,2,3,4,5,6,1,8,9,\\\n",
    "     2,6,2,3,4,5,5,3,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,2,8,9,0,1,2,3,4,5,5,7,8,3,0,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,5,3,0,9,0,1,2,3,4,5,5,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,3,3,3,2,6,2,3,4,5,5,2,8,1,\\\n",
    "     0,1,2,3,4,5,5,7,8,7,0,1,2,3,4,5,8,7,8,3,0,1,2,3,4,5,6,7,8,9])}\n",
    "  elif optimizer == 'RMSProp':\n",
    "   if epochs == 200:\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([3,1,2,3,4,5,6,3,8,9,2,1,2,3,4,5,5,3,8,9,0,1,2,3,4,5,8,7,8,9,2,1,2,3,4,5,5,3,8,9,0,1,2,3,4,5,5,2,8,3,\\\n",
    "     2,1,2,3,4,5,5,3,8,3,0,1,2,3,4,5,5,3,8,9,0,6,2,3,4,5,5,2,3,3,0,1,2,3,4,5,5,3,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,3,5,9,\\\n",
    "     0,1,2,3,4,5,5,7,3,3,0,1,2,3,4,3,5,3,8,3,0,1,2,3,4,5,6,7,8,3,0,1,2,3,4,5,5,7,8,9,0,1,2,3,4,5,6,3,3,9,2,1,2,3,4,5,5,3,3,3,\\\n",
    "     0,1,2,3,4,5,6,3,8,3,0,7,2,3,4,5,6,7,8,9,0,1,2,3,4,5,5,3,3,2])}\n",
    "  elif optimizer == 'SGD':\n",
    "   if epochs == 200:\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([0,8,2,3,4,5,8,7,8,3,0,8,2,3,4,5,6,3,8,9,0,1,2,3,4,5,5,7,8,3,0,8,2,3,4,5,5,7,8,3,0,1,2,3,4,5,5,1,6,9,\\\n",
    "     0,1,2,3,4,5,5,2,8,9,6,1,2,3,4,5,5,7,6,9,4,1,2,3,4,5,5,7,8,3,0,1,2,3,4,5,6,7,5,9,0,1,2,3,4,5,6,7,3,9,3,1,2,3,4,5,6,7,3,9,\\\n",
    "     0,1,2,3,4,5,8,7,3,3,0,1,2,3,4,5,5,7,3,3,0,1,2,3,4,5,5,2,8,3,0,1,2,3,4,5,6,7,8,9,0,8,2,3,4,5,6,7,8,9,0,7,2,3,4,5,5,7,3,9,\\\n",
    "     9,1,2,3,4,5,3,7,8,3,0,1,2,3,4,5,5,7,8,3,0,1,2,3,4,5,8,7,8,3])}\n",
    "  elif optimizer == 'AdaGrad':\n",
    "   if epochs == 200:\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([\n",
    "         0,1,2,3,4,5,8,3,6,3,0,8,2,3,4,5,5,7,3,4,0,5,2,3,4,5,8,2,3,3,0,8,2,3,4,5,3,7,8,3,6,1,2,3,4,5,8,2,8,3,\\\n",
    "         0,1,2,3,4,5,6,7,3,8,0,8,2,3,4,5,5,7,3,3,6,1,2,3,4,5,8,2,8,3,1,1,2,3,4,5,5,2,3,9,0,8,2,3,4,5,8,2,8,3,\\\n",
    "         2,1,2,3,4,5,3,2,8,3,9,1,2,3,4,5,5,3,3,3,6,1,2,3,4,5,5,1,3,3,0,1,2,3,4,5,5,2,8,3,0,1,2,3,4,5,5,3,8,3,\\\n",
    "         0,8,2,3,4,5,3,2,3,9,0,1,2,3,4,5,5,7,3,9,0,1,2,3,4,3,5,2,3,3,7,8,2,3,4,5,5,7,8,9,1,1,2,3,4,5,6,7,3,2])}\n",
    "  elif optimizer == 'AdaMax':\n",
    "   if epochs == 200:\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([0,1,2,5,4,5,8,7,5,3,4,1,2,3,4,5,5,1,6,9,0,6,2,3,4,5,6,7,3,3,0,1,2,3,4,3,5,2,8,5,0,1,2,3,4,5,6,3,6,9,\\\n",
    "     0,1,2,3,4,5,5,7,2,3,0,1,2,3,4,5,5,2,8,9,9,1,2,3,4,5,5,7,8,9,0,1,2,3,4,5,5,3,8,3,0,1,2,3,4,5,5,7,8,3,0,1,2,3,4,5,6,7,8,3,\\\n",
    "     0,7,2,3,4,3,5,7,8,9,0,1,2,3,4,5,6,3,8,9,0,1,2,3,4,5,5,7,8,3,0,1,2,3,4,5,5,3,8,3,0,6,2,3,4,5,5,7,8,3,0,1,2,3,4,5,5,7,8,3,\\\n",
    "     0,1,2,3,4,5,5,3,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,3])}\n",
    "  elif optimizer == 'Nadam':\n",
    "   if epochs == 200:\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([0,1,2,3,4,5,5,2,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,5,7,8,3,0,1,2,3,4,5,6,2,6,4,0,1,2,3,4,5,6,7,8,5,\\\n",
    "     1,1,2,3,4,5,5,7,8,2,0,6,2,3,1,5,5,7,6,9,0,1,2,3,4,5,6,7,3,3,0,1,2,3,4,5,5,7,8,1,0,1,2,3,4,5,0,7,8,3,1,1,2,3,4,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,5,2,8,9,0,1,2,3,4,5,0,7,6,5,6,1,2,3,4,5,5,2,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,0,7,8,9,0,1,2,3,1,5,6,7,8,9,\\\n",
    "     0,1,2,3,4,5,5,7,8,3,0,7,2,3,4,5,0,7,8,4,0,1,2,3,4,3,5,7,3,3])}\n",
    "  elif optimizer == 'Ftrl':\n",
    "   if epochs == 200:\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([1,1,1,1,1,1,1,1,1,1]*20)}\n",
    "\n",
    " elif loss == 'mean_squared_error':\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([0,8,2,3,4,5,9,7,8,9,9,8,7,3,4,5,9,7,3,9,2,1,2,2,4,5,6,5,8,3,2,1,2,3,4,5,3,7,8,8,6,1,2,3,4,5,5,7,5,9,\\\n",
    "     2,6,2,3,9,5,6,3,8,9,0,1,2,3,4,5,6,2,8,3,0,8,2,5,9,3,6,3,8,3,0,8,2,3,4,5,9,2,3,8,2,5,2,3,4,5,8,7,8,9,2,1,2,3,4,5,6,2,8,9,\\\n",
    "     0,1,2,3,4,3,6,5,8,3,6,8,2,3,4,5,2,3,8,9,0,3,3,3,9,5,6,2,3,9,6,1,3,3,3,5,6,7,2,9,2,1,2,3,4,5,5,5,3,3,0,8,2,3,4,5,6,2,8,9,\\\n",
    "     9,8,2,3,4,5,3,2,3,3,0,1,2,3,4,3,5,1,6,9,0,8,2,3,4,3,8,3,8,9])}\n",
    " elif (loss == 'mean_absolute_error') & (ml == 0) & (epochs == 200):\n",
    "    perf = {'actual':np.array([0,1,2,3,4,5,6,7,8,9]*20),\n",
    "     'prediction':np.array([0,1,2,3,4,3,6,2,5,8,6,6,2,3,4,3,5,3,8,3,3,1,2,3,9,5,3,8,8,3,9,8,2,3,4,5,6,2,3,9,0,8,2,3,4,5,6,1,8,3,\\\n",
    "     0,8,2,3,9,5,3,7,5,2,3,8,2,3,8,5,9,3,2,3,2,3,2,3,4,5,2,5,8,9,0,1,2,3,4,5,6,7,6,3,0,6,2,3,8,3,6,1,8,5,0,1,2,3,9,5,6,7,8,9,\\\n",
    "     9,8,3,3,8,5,5,7,3,9,9,1,2,3,4,5,5,7,8,3,8,6,3,3,4,3,3,7,3,3,0,9,2,3,8,5,6,7,8,2,0,8,2,3,9,5,2,7,8,3,0,7,2,3,4,3,0,2,8,9,\\\n",
    "     9,8,2,3,4,5,3,7,8,3,0,1,2,3,4,5,6,8,3,3,4,1,2,3,4,5,5,7,8,3])}\n",
    "    \n",
    "    correctness_cnn = list(map(int, perf['actual'] == perf['prediction'])) # for statistical test\n",
    "\n",
    "contingency_matrix = pd.crosstab(perf['prediction'], perf['actual'], rownames=['Predicted'], colnames=['Actual'])\n",
    "\n",
    "# Confusion matrix calculated manually\n",
    "def cm(digits,classf):\n",
    "    CM = np.array([[0]*10]*10)\n",
    "    for i in range(len(digits)):\n",
    "        CM[digits[i]][classf[i]] = CM[digits[i]][classf[i]] + 1\n",
    "    return CM\n",
    "#print(contingency_matrix)\n",
    "# Plot contingency matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "plt.clf()\n",
    "res = sn.heatmap(contingency_matrix.T, annot=True, fmt='d', cmap=\"YlGnBu\", cbar=False)\n",
    "plt.title(f\"Sample size = {len(perf['actual'])}, Accuracy = {np.sum([contingency_matrix[i][i] for i in range(10)]) / np.sum(np.sum(contingency_matrix))}\")\n",
    "if mx == True:\n",
    "    plt.savefig(\"fig/cm.cnn_cce_\"+optimizer+'_'+str(nl1)+\"_\"+str(nl2)+f\"_d{dropout}.ml\"+str(ml)+\".e\"+str(epochs)+\".png\", bbox_inches='tight', dpi=100)\n",
    "else:\n",
    "    plt.savefig(\"fig/cm.nn_\"+str(nl1)+f\"_d{dropout}_\"+str(nl2)+\".ml\"+str(ml)+\".e\"+str(epochs)+\".png\", bbox_inches='tight', dpi=100)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness = perf['actual'] == perf['prediction']\n",
    "accuracy_cumulative = []\n",
    "for i in range(len(correctness)):\n",
    "    accuracy_cumulative.append(correctness[:i+1].sum()/(i+1))\n",
    "#accuracy_cumulative_nn_50_75_ml0_e200 = accuracy_cumulative\n",
    "#pd.DataFrame(accuracy_cumulative).to_csv('performance/accuracy_cumulative_nn_'+str(nl1)+'_d'+str(10*dropout)+'_'+str(nl2)+'_ml'+str(ml)+'_e'+str(epochs)+'.csv')\n",
    "pd.DataFrame(accuracy_cumulative).to_csv('performance/accuracy_cumulative_cnn_'+str(nl1)+'_'+str(nl2)+'_d'+str(dropout)+'_ml'+str(ml)+'_e'+str(epochs)+'.csv')\n",
    "#print(f\"accuracy_cumulative_nn_{nl1}_d{10*dropout}_{nl2}_ml{ml}_e{epochs} = {accuracy_cumulative}\")\n",
    "print(f\"accuracy_cumulative_cnn_cce_{optimizer}_{nl1}_{nl2}_d{dropout}_ml{ml}_e{epochs} = {accuracy_cumulative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_cumulative_nn_50_d0_25_ml0_e200 = [0.0, 0.5, 0.3333333333333333, 0.5, 0.6, 0.6666666666666666, 0.5714285714285714, 0.5, 0.4444444444444444, 0.4, 0.36363636363636365, 0.3333333333333333, 0.38461538461538464, 0.42857142857142855, 0.4, 0.375, 0.4117647058823529, 0.4444444444444444, 0.42105263157894735, 0.45, 0.42857142857142855, 0.45454545454545453, 0.4782608695652174, 0.5, 0.48, 0.5, 0.48148148148148145, 0.4642857142857143, 0.4827586206896552, 0.4666666666666667, 0.4838709677419355, 0.46875, 0.48484848484848486, 0.47058823529411764, 0.4857142857142857, 0.5, 0.5135135135135135, 0.5263157894736842, 0.5128205128205128, 0.5, 0.5121951219512195, 0.5238095238095238, 0.5348837209302325, 0.5454545454545454, 0.5555555555555556, 0.5434782608695652, 0.5531914893617021, 0.5416666666666666, 0.5306122448979592, 0.52, 0.5294117647058824, 0.5384615384615384, 0.5471698113207547, 0.5555555555555556, 0.5454545454545454, 0.5357142857142857, 0.5263157894736842, 0.5344827586206896, 0.5423728813559322, 0.5333333333333333, 0.5409836065573771, 0.5483870967741935, 0.5555555555555556, 0.5625, 0.5692307692307692, 0.5757575757575758, 0.582089552238806, 0.5735294117647058, 0.5797101449275363, 0.5714285714285714, 0.5774647887323944, 0.5694444444444444, 0.5616438356164384, 0.5675675675675675, 0.5733333333333334, 0.5657894736842105, 0.5714285714285714, 0.5769230769230769, 0.569620253164557, 0.5625, 0.5679012345679012, 0.5609756097560976, 0.5662650602409639, 0.5714285714285714, 0.5764705882352941, 0.5697674418604651, 0.5632183908045977, 0.5568181818181818, 0.5617977528089888, 0.5555555555555556, 0.5604395604395604, 0.5652173913043478, 0.5591397849462365, 0.5638297872340425, 0.5684210526315789, 0.5729166666666666, 0.5670103092783505, 0.5612244897959183, 0.5555555555555556, 0.56, 0.5643564356435643, 0.5588235294117647, 0.5631067961165048, 0.5673076923076923, 0.5714285714285714, 0.5754716981132075, 0.5700934579439252, 0.5648148148148148, 0.5596330275229358, 0.5636363636363636, 0.5585585585585585, 0.5535714285714286, 0.5575221238938053, 0.5614035087719298, 0.5652173913043478, 0.5689655172413793, 0.5641025641025641, 0.559322033898305, 0.5630252100840336, 0.5583333333333333, 0.5537190082644629, 0.5573770491803278, 0.5609756097560976, 0.5645161290322581, 0.568, 0.5714285714285714, 0.5669291338582677, 0.5625, 0.5581395348837209, 0.5538461538461539, 0.549618320610687, 0.5454545454545454, 0.5488721804511278, 0.5522388059701493, 0.5555555555555556, 0.5588235294117647, 0.5620437956204379, 0.5579710144927537, 0.5539568345323741, 0.5571428571428572, 0.5602836879432624, 0.5633802816901409, 0.5664335664335665, 0.5694444444444444, 0.5724137931034483, 0.5753424657534246, 0.5782312925170068, 0.5743243243243243, 0.5771812080536913, 0.5733333333333334, 0.5761589403973509, 0.5723684210526315, 0.5751633986928104, 0.577922077922078, 0.5806451612903226, 0.5833333333333334, 0.5796178343949044, 0.5759493670886076, 0.5723270440251572, 0.56875, 0.5652173913043478, 0.5679012345679012, 0.5644171779141104, 0.5670731707317073, 0.5696969696969697, 0.572289156626506, 0.5688622754491018, 0.5654761904761905, 0.5680473372781065, 0.5705882352941176, 0.5730994152046783, 0.5697674418604651, 0.5722543352601156, 0.5747126436781609, 0.5714285714285714, 0.5738636363636364, 0.5706214689265536, 0.5730337078651685, 0.5754189944134078, 0.5777777777777777, 0.580110497237569, 0.5769230769230769, 0.5792349726775956, 0.5815217391304348, 0.5837837837837838, 0.5860215053763441, 0.5828877005347594, 0.5851063829787234, 0.582010582010582, 0.5842105263157895, 0.5863874345549738, 0.5885416666666666, 0.5906735751295337, 0.5927835051546392, 0.5948717948717949, 0.5969387755102041, 0.5939086294416244, 0.5909090909090909, 0.592964824120603, 0.59]\n",
    "accuracy_cumulative_nn_50_d0_50_ml0_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.7142857142857143, 0.75, 0.6666666666666666, 0.7, 0.7272727272727273, 0.75, 0.7692307692307693, 0.7857142857142857, 0.8, 0.8125, 0.8235294117647058, 0.8333333333333334, 0.7894736842105263, 0.8, 0.8095238095238095, 0.8181818181818182, 0.8260869565217391, 0.8333333333333334, 0.84, 0.8461538461538461, 0.8518518518518519, 0.8214285714285714, 0.8275862068965517, 0.8333333333333334, 0.8064516129032258, 0.8125, 0.8181818181818182, 0.8235294117647058, 0.8, 0.8055555555555556, 0.7837837837837838, 0.7631578947368421, 0.7692307692307693, 0.75, 0.7560975609756098, 0.7380952380952381, 0.7441860465116279, 0.75, 0.7555555555555555, 0.7608695652173914, 0.7446808510638298, 0.75, 0.7346938775510204, 0.72, 0.7254901960784313, 0.7115384615384616, 0.7169811320754716, 0.7222222222222222, 0.7272727272727273, 0.7142857142857143, 0.7192982456140351, 0.7068965517241379, 0.6949152542372882, 0.7, 0.7049180327868853, 0.7096774193548387, 0.7142857142857143, 0.703125, 0.7076923076923077, 0.696969696969697, 0.7014925373134329, 0.6911764705882353, 0.6811594202898551, 0.6714285714285714, 0.676056338028169, 0.6805555555555556, 0.684931506849315, 0.6891891891891891, 0.6933333333333334, 0.6973684210526315, 0.7012987012987013, 0.6923076923076923, 0.6962025316455697, 0.6875, 0.691358024691358, 0.6951219512195121, 0.6987951807228916, 0.7023809523809523, 0.7058823529411765, 0.7093023255813954, 0.7126436781609196, 0.7045454545454546, 0.7078651685393258, 0.7111111111111111, 0.7142857142857143, 0.717391304347826, 0.7204301075268817, 0.723404255319149, 0.7263157894736842, 0.7291666666666666, 0.7319587628865979, 0.7346938775510204, 0.7373737373737373, 0.73, 0.7227722772277227, 0.7254901960784313, 0.7281553398058253, 0.7307692307692307, 0.7238095238095238, 0.7169811320754716, 0.719626168224299, 0.7129629629629629, 0.7155963302752294, 0.7090909090909091, 0.7117117117117117, 0.7142857142857143, 0.7168141592920354, 0.7192982456140351, 0.7217391304347827, 0.7241379310344828, 0.7264957264957265, 0.7203389830508474, 0.7226890756302521, 0.7166666666666667, 0.7107438016528925, 0.7131147540983607, 0.7154471544715447, 0.717741935483871, 0.72, 0.7222222222222222, 0.7244094488188977, 0.71875, 0.7209302325581395, 0.7153846153846154, 0.7099236641221374, 0.7045454545454546, 0.706766917293233, 0.7089552238805971, 0.7111111111111111, 0.7132352941176471, 0.7153284671532847, 0.717391304347826, 0.7122302158273381, 0.7071428571428572, 0.7021276595744681, 0.704225352112676, 0.7062937062937062, 0.7083333333333334, 0.7103448275862069, 0.7123287671232876, 0.7074829931972789, 0.7027027027027027, 0.7046979865771812, 0.7066666666666667, 0.7086092715231788, 0.7105263157894737, 0.7124183006535948, 0.7142857142857143, 0.7161290322580646, 0.717948717948718, 0.7197452229299363, 0.7151898734177216, 0.7169811320754716, 0.7125, 0.7080745341614907, 0.7098765432098766, 0.7116564417177914, 0.7134146341463414, 0.7151515151515152, 0.7168674698795181, 0.718562874251497, 0.7142857142857143, 0.7100591715976331, 0.7058823529411765, 0.7076023391812866, 0.7093023255813954, 0.7109826589595376, 0.7126436781609196, 0.7085714285714285, 0.7102272727272727, 0.7062146892655368, 0.7078651685393258, 0.7094972067039106, 0.7055555555555556, 0.7071823204419889, 0.7087912087912088, 0.7103825136612022, 0.7119565217391305, 0.7081081081081081, 0.7096774193548387, 0.7112299465240641, 0.7074468085106383, 0.7037037037037037, 0.7, 0.7015706806282722, 0.703125, 0.7046632124352331, 0.7061855670103093, 0.7076923076923077, 0.7040816326530612, 0.7055837563451777, 0.702020202020202, 0.6984924623115578, 0.7, 0.7014925373134329, 0.7029702970297029, 0.7044334975369458, 0.7058823529411765, 0.7073170731707317, 0.7087378640776699, 0.7101449275362319, 0.7067307692307693, 0.7081339712918661, 0.7047619047619048, 0.7061611374407583, 0.7075471698113207, 0.7089201877934272, 0.7102803738317757, 0.7116279069767442, 0.7129629629629629, 0.7142857142857143, 0.7155963302752294, 0.7168949771689498, 0.7136363636363636, 0.7149321266968326, 0.7117117117117117, 0.7130044843049327, 0.7142857142857143, 0.7155555555555555, 0.7168141592920354, 0.7180616740088106, 0.7192982456140351, 0.7161572052401747, 0.7130434782608696, 0.70995670995671, 0.7068965517241379, 0.7081545064377682, 0.7094017094017094, 0.7106382978723405, 0.711864406779661, 0.7088607594936709, 0.7100840336134454, 0.7112970711297071, 0.7125, 0.7136929460580913, 0.7148760330578512, 0.7160493827160493, 0.7172131147540983, 0.7183673469387755, 0.7195121951219512, 0.7206477732793523, 0.7217741935483871, 0.7188755020080321, 0.72, 0.7211155378486056, 0.7182539682539683, 0.7193675889328063, 0.7204724409448819, 0.7215686274509804, 0.72265625, 0.7237354085603113, 0.7248062015503876, 0.722007722007722, 0.7192307692307692, 0.7203065134099617, 0.7213740458015268, 0.7224334600760456, 0.7234848484848485, 0.7245283018867924, 0.7255639097744361, 0.7265917602996255, 0.7238805970149254, 0.724907063197026, 0.7222222222222222, 0.7232472324723247, 0.7205882352941176, 0.7216117216117216, 0.7226277372262774, 0.7236363636363636, 0.7210144927536232, 0.7220216606498195, 0.7230215827338129, 0.7204301075268817, 0.7178571428571429, 0.7153024911032029, 0.7127659574468085, 0.7137809187279152, 0.7147887323943662, 0.7157894736842105, 0.7132867132867133, 0.7142857142857143, 0.7118055555555556, 0.71280276816609, 0.7137931034482758, 0.7147766323024055, 0.7157534246575342, 0.7167235494880546, 0.717687074829932, 0.7186440677966102, 0.7195945945945946, 0.7205387205387206, 0.7181208053691275, 0.7157190635451505, 0.7133333333333334, 0.7142857142857143, 0.7152317880794702, 0.7161716171617162, 0.7171052631578947, 0.7180327868852459, 0.7156862745098039, 0.7133550488599348, 0.7142857142857143, 0.7152103559870551, 0.7129032258064516, 0.7138263665594855, 0.7147435897435898, 0.7156549520766773, 0.7165605095541401, 0.7174603174603175, 0.7183544303797469, 0.7160883280757098, 0.7169811320754716, 0.7178683385579937, 0.715625, 0.7165109034267912, 0.717391304347826, 0.718266253869969, 0.7191358024691358, 0.72, 0.7208588957055214, 0.7186544342507645, 0.7164634146341463, 0.7142857142857143, 0.7121212121212122, 0.7129909365558912, 0.713855421686747, 0.7117117117117117, 0.7125748502994012, 0.7134328358208956, 0.7142857142857143, 0.7151335311572701, 0.7130177514792899, 0.7109144542772862, 0.711764705882353, 0.7126099706744868, 0.7134502923976608, 0.7142857142857143, 0.7151162790697675, 0.7159420289855073, 0.7138728323699421, 0.7146974063400576, 0.7155172413793104, 0.7134670487106017, 0.7114285714285714, 0.7122507122507122, 0.7102272727272727, 0.7082152974504249, 0.7090395480225988, 0.7098591549295775, 0.7106741573033708, 0.711484593837535, 0.7094972067039106, 0.7075208913649025, 0.7055555555555556, 0.7063711911357341, 0.7071823204419889, 0.7079889807162535, 0.7087912087912088, 0.7095890410958904, 0.7103825136612022, 0.7084468664850136, 0.7092391304347826, 0.7073170731707317, 0.7054054054054054, 0.706199460916442, 0.706989247311828, 0.707774798927614, 0.7085561497326203, 0.7093333333333334, 0.7101063829787234, 0.7108753315649867, 0.708994708994709, 0.7071240105540897, 0.7078947368421052, 0.7086614173228346, 0.7094240837696335, 0.7101827676240209, 0.7109375, 0.7116883116883117, 0.7124352331606217, 0.7131782945736435, 0.711340206185567, 0.712082262210797, 0.7128205128205128, 0.7135549872122762, 0.7142857142857143, 0.7150127226463104, 0.7157360406091371, 0.7164556962025317, 0.7171717171717171, 0.7153652392947103, 0.7160804020100503, 0.7142857142857143, 0.715, 0.71571072319202, 0.7164179104477612, 0.71712158808933, 0.7178217821782178, 0.7160493827160493, 0.7167487684729064, 0.7174447174447175, 0.7156862745098039, 0.7163814180929096, 0.7146341463414634, 0.7153284671532847, 0.7160194174757282, 0.7167070217917676, 0.714975845410628, 0.7132530120481928, 0.7139423076923077, 0.7122302158273381, 0.7129186602870813, 0.711217183770883, 0.7119047619047619, 0.7125890736342043, 0.7132701421800948, 0.7139479905437353, 0.714622641509434, 0.7152941176470589, 0.715962441314554, 0.7166276346604216, 0.7172897196261683, 0.717948717948718, 0.7162790697674418, 0.7146171693735499, 0.7129629629629629, 0.7136258660508084, 0.7142857142857143, 0.7149425287356321, 0.7155963302752294, 0.7162471395881007, 0.7146118721461188, 0.7129840546697038, 0.7113636363636363, 0.7120181405895691, 0.7104072398190046, 0.7110609480812641, 0.7094594594594594, 0.7101123595505618, 0.7107623318385651, 0.7114093959731543, 0.7098214285714286, 0.7082405345211581, 0.7088888888888889, 0.7073170731707317, 0.7079646017699115, 0.7064017660044151, 0.7070484581497798, 0.7076923076923077, 0.7083333333333334, 0.7089715536105032, 0.7074235807860262, 0.7058823529411765, 0.7043478260869566, 0.7049891540130152, 0.7056277056277056, 0.7062634989200864, 0.7068965517241379, 0.7053763440860215, 0.7060085836909872, 0.7044967880085653, 0.7029914529914529, 0.7036247334754797, 0.7021276595744681, 0.70276008492569, 0.7033898305084746, 0.7040169133192389, 0.7046413502109705, 0.7052631578947368, 0.7058823529411765, 0.7064989517819706, 0.7050209205020921, 0.7056367432150313, 0.7041666666666667, 0.7047817047817048, 0.7033195020746889, 0.7039337474120083, 0.7045454545454546, 0.7030927835051546, 0.7016460905349794, 0.702258726899384, 0.7008196721311475, 0.6993865030674846, 0.6979591836734694, 0.6985743380855397, 0.6971544715447154, 0.6977687626774848, 0.6983805668016194, 0.6989898989898989, 0.6975806451612904, 0.6961770623742455, 0.6967871485943775, 0.6953907815631263, 0.694]\n",
    "accuracy_cumulative_nn_50_d0_75_ml0_e200 = [1.0, 0.5, 0.3333333333333333, 0.5, 0.4, 0.5, 0.42857142857142855, 0.375, 0.3333333333333333, 0.3, 0.36363636363636365, 0.4166666666666667, 0.46153846153846156, 0.5, 0.5333333333333333, 0.5625, 0.5882352941176471, 0.5555555555555556, 0.5263157894736842, 0.5, 0.5238095238095238, 0.5, 0.5217391304347826, 0.5416666666666666, 0.52, 0.5384615384615384, 0.5185185185185185, 0.5357142857142857, 0.5517241379310345, 0.5333333333333333, 0.5483870967741935, 0.53125, 0.5454545454545454, 0.5588235294117647, 0.5714285714285714, 0.5555555555555556, 0.5405405405405406, 0.5526315789473685, 0.5641025641025641, 0.55, 0.5365853658536586, 0.5238095238095238, 0.5348837209302325, 0.5454545454545454, 0.5555555555555556, 0.5652173913043478, 0.5531914893617021, 0.5625, 0.5714285714285714, 0.56, 0.5686274509803921, 0.5769230769230769, 0.5849056603773585, 0.5740740740740741, 0.5818181818181818, 0.5714285714285714, 0.5614035087719298, 0.5689655172413793, 0.559322033898305, 0.55, 0.5409836065573771, 0.532258064516129, 0.5396825396825397, 0.53125, 0.5384615384615384, 0.5454545454545454, 0.5373134328358209, 0.5441176470588235, 0.5362318840579711, 0.5285714285714286, 0.5352112676056338, 0.5277777777777778, 0.5342465753424658, 0.5405405405405406, 0.5466666666666666, 0.5394736842105263, 0.5454545454545454, 0.5384615384615384, 0.5316455696202531, 0.525, 0.5308641975308642, 0.524390243902439, 0.5301204819277109, 0.5357142857142857, 0.5294117647058824, 0.5348837209302325, 0.5287356321839081, 0.5340909090909091, 0.5393258426966292, 0.5444444444444444, 0.5384615384615384, 0.5434782608695652, 0.5483870967741935, 0.5425531914893617, 0.5368421052631579, 0.5416666666666666, 0.5463917525773195, 0.5408163265306123, 0.5353535353535354, 0.54, 0.5445544554455446, 0.5490196078431373, 0.5533980582524272, 0.5576923076923077, 0.5619047619047619, 0.5660377358490566, 0.5607476635514018, 0.5555555555555556, 0.5504587155963303, 0.5454545454545454, 0.5495495495495496, 0.5535714285714286, 0.5575221238938053, 0.5526315789473685, 0.5478260869565217, 0.5517241379310345, 0.5470085470085471, 0.5423728813559322, 0.5462184873949579, 0.5416666666666666, 0.5371900826446281, 0.5409836065573771, 0.5447154471544715, 0.5483870967741935, 0.544, 0.5476190476190477, 0.5433070866141733, 0.546875, 0.5503875968992248, 0.5461538461538461, 0.549618320610687, 0.553030303030303, 0.556390977443609, 0.5522388059701493, 0.5555555555555556, 0.5588235294117647, 0.5620437956204379, 0.5579710144927537, 0.5611510791366906, 0.5571428571428572, 0.5602836879432624, 0.5563380281690141, 0.5594405594405595, 0.5625, 0.5655172413793104, 0.5684931506849316, 0.564625850340136, 0.5608108108108109, 0.5637583892617449, 0.5666666666666667, 0.5695364238410596, 0.5723684210526315, 0.5751633986928104, 0.577922077922078, 0.5806451612903226, 0.5833333333333334, 0.5796178343949044, 0.5759493670886076, 0.5723270440251572, 0.575, 0.5714285714285714, 0.5740740740740741, 0.5766871165644172, 0.5792682926829268, 0.5818181818181818, 0.5843373493975904, 0.5808383233532934, 0.5773809523809523, 0.5739644970414202, 0.5705882352941176, 0.5672514619883041, 0.5697674418604651, 0.5722543352601156, 0.5747126436781609, 0.5771428571428572, 0.5738636363636364, 0.576271186440678, 0.5730337078651685, 0.5698324022346368, 0.5722222222222222, 0.574585635359116, 0.5714285714285714, 0.5737704918032787, 0.5760869565217391, 0.5783783783783784, 0.5806451612903226, 0.5775401069518716, 0.574468085106383, 0.5714285714285714, 0.5684210526315789, 0.5706806282722513, 0.5729166666666666, 0.5751295336787565, 0.5773195876288659, 0.5794871794871795, 0.5816326530612245, 0.5786802030456852, 0.5757575757575758, 0.5728643216080402, 0.57]\n",
    "plt.plot(accuracy_cumulative_nn_50_d0_50_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_d0_25_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_d0_75_ml0_e200)\n",
    "plt.xlabel('trials')\n",
    "plt.xlim(0,len(accuracy_cumulative_nn_50_d0_25_ml0_e200))\n",
    "plt.ylim(0,1.01)\n",
    "plt.ylabel('cumulative accuracy')\n",
    "plt.legend(['n2 = 50', 'n2 = 25', 'n2 = 75'])\n",
    "plt.title('Accuracy for different no. of neurons in layer 2 of NN');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_cumulative_nn_50_d0_50_ml0_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.7142857142857143, 0.75, 0.6666666666666666, 0.7, 0.7272727272727273, 0.75, 0.7692307692307693, 0.7857142857142857, 0.8, 0.8125, 0.8235294117647058, 0.8333333333333334, 0.7894736842105263, 0.8, 0.8095238095238095, 0.8181818181818182, 0.8260869565217391, 0.8333333333333334, 0.84, 0.8461538461538461, 0.8518518518518519, 0.8214285714285714, 0.8275862068965517, 0.8333333333333334, 0.8064516129032258, 0.8125, 0.8181818181818182, 0.8235294117647058, 0.8, 0.8055555555555556, 0.7837837837837838, 0.7631578947368421, 0.7692307692307693, 0.75, 0.7560975609756098, 0.7380952380952381, 0.7441860465116279, 0.75, 0.7555555555555555, 0.7608695652173914, 0.7446808510638298, 0.75, 0.7346938775510204, 0.72, 0.7254901960784313, 0.7115384615384616, 0.7169811320754716, 0.7222222222222222, 0.7272727272727273, 0.7142857142857143, 0.7192982456140351, 0.7068965517241379, 0.6949152542372882, 0.7, 0.7049180327868853, 0.7096774193548387, 0.7142857142857143, 0.703125, 0.7076923076923077, 0.696969696969697, 0.7014925373134329, 0.6911764705882353, 0.6811594202898551, 0.6714285714285714, 0.676056338028169, 0.6805555555555556, 0.684931506849315, 0.6891891891891891, 0.6933333333333334, 0.6973684210526315, 0.7012987012987013, 0.6923076923076923, 0.6962025316455697, 0.6875, 0.691358024691358, 0.6951219512195121, 0.6987951807228916, 0.7023809523809523, 0.7058823529411765, 0.7093023255813954, 0.7126436781609196, 0.7045454545454546, 0.7078651685393258, 0.7111111111111111, 0.7142857142857143, 0.717391304347826, 0.7204301075268817, 0.723404255319149, 0.7263157894736842, 0.7291666666666666, 0.7319587628865979, 0.7346938775510204, 0.7373737373737373, 0.73, 0.7227722772277227, 0.7254901960784313, 0.7281553398058253, 0.7307692307692307, 0.7238095238095238, 0.7169811320754716, 0.719626168224299, 0.7129629629629629, 0.7155963302752294, 0.7090909090909091, 0.7117117117117117, 0.7142857142857143, 0.7168141592920354, 0.7192982456140351, 0.7217391304347827, 0.7241379310344828, 0.7264957264957265, 0.7203389830508474, 0.7226890756302521, 0.7166666666666667, 0.7107438016528925, 0.7131147540983607, 0.7154471544715447, 0.717741935483871, 0.72, 0.7222222222222222, 0.7244094488188977, 0.71875, 0.7209302325581395, 0.7153846153846154, 0.7099236641221374, 0.7045454545454546, 0.706766917293233, 0.7089552238805971, 0.7111111111111111, 0.7132352941176471, 0.7153284671532847, 0.717391304347826, 0.7122302158273381, 0.7071428571428572, 0.7021276595744681, 0.704225352112676, 0.7062937062937062, 0.7083333333333334, 0.7103448275862069, 0.7123287671232876, 0.7074829931972789, 0.7027027027027027, 0.7046979865771812, 0.7066666666666667, 0.7086092715231788, 0.7105263157894737, 0.7124183006535948, 0.7142857142857143, 0.7161290322580646, 0.717948717948718, 0.7197452229299363, 0.7151898734177216, 0.7169811320754716, 0.7125, 0.7080745341614907, 0.7098765432098766, 0.7116564417177914, 0.7134146341463414, 0.7151515151515152, 0.7168674698795181, 0.718562874251497, 0.7142857142857143, 0.7100591715976331, 0.7058823529411765, 0.7076023391812866, 0.7093023255813954, 0.7109826589595376, 0.7126436781609196, 0.7085714285714285, 0.7102272727272727, 0.7062146892655368, 0.7078651685393258, 0.7094972067039106, 0.7055555555555556, 0.7071823204419889, 0.7087912087912088, 0.7103825136612022, 0.7119565217391305, 0.7081081081081081, 0.7096774193548387, 0.7112299465240641, 0.7074468085106383, 0.7037037037037037, 0.7, 0.7015706806282722, 0.703125, 0.7046632124352331, 0.7061855670103093, 0.7076923076923077, 0.7040816326530612, 0.7055837563451777, 0.702020202020202, 0.6984924623115578, 0.7, 0.7014925373134329, 0.7029702970297029, 0.7044334975369458, 0.7058823529411765, 0.7073170731707317, 0.7087378640776699, 0.7101449275362319, 0.7067307692307693, 0.7081339712918661, 0.7047619047619048, 0.7061611374407583, 0.7075471698113207, 0.7089201877934272, 0.7102803738317757, 0.7116279069767442, 0.7129629629629629, 0.7142857142857143, 0.7155963302752294, 0.7168949771689498, 0.7136363636363636, 0.7149321266968326, 0.7117117117117117, 0.7130044843049327, 0.7142857142857143, 0.7155555555555555, 0.7168141592920354, 0.7180616740088106, 0.7192982456140351, 0.7161572052401747, 0.7130434782608696, 0.70995670995671, 0.7068965517241379, 0.7081545064377682, 0.7094017094017094, 0.7106382978723405, 0.711864406779661, 0.7088607594936709, 0.7100840336134454, 0.7112970711297071, 0.7125, 0.7136929460580913, 0.7148760330578512, 0.7160493827160493, 0.7172131147540983, 0.7183673469387755, 0.7195121951219512, 0.7206477732793523, 0.7217741935483871, 0.7188755020080321, 0.72, 0.7211155378486056, 0.7182539682539683, 0.7193675889328063, 0.7204724409448819, 0.7215686274509804, 0.72265625, 0.7237354085603113, 0.7248062015503876, 0.722007722007722, 0.7192307692307692, 0.7203065134099617, 0.7213740458015268, 0.7224334600760456, 0.7234848484848485, 0.7245283018867924, 0.7255639097744361, 0.7265917602996255, 0.7238805970149254, 0.724907063197026, 0.7222222222222222, 0.7232472324723247, 0.7205882352941176, 0.7216117216117216, 0.7226277372262774, 0.7236363636363636, 0.7210144927536232, 0.7220216606498195, 0.7230215827338129, 0.7204301075268817, 0.7178571428571429, 0.7153024911032029, 0.7127659574468085, 0.7137809187279152, 0.7147887323943662, 0.7157894736842105, 0.7132867132867133, 0.7142857142857143, 0.7118055555555556, 0.71280276816609, 0.7137931034482758, 0.7147766323024055, 0.7157534246575342, 0.7167235494880546, 0.717687074829932, 0.7186440677966102, 0.7195945945945946, 0.7205387205387206, 0.7181208053691275, 0.7157190635451505, 0.7133333333333334, 0.7142857142857143, 0.7152317880794702, 0.7161716171617162, 0.7171052631578947, 0.7180327868852459, 0.7156862745098039, 0.7133550488599348, 0.7142857142857143, 0.7152103559870551, 0.7129032258064516, 0.7138263665594855, 0.7147435897435898, 0.7156549520766773, 0.7165605095541401, 0.7174603174603175, 0.7183544303797469, 0.7160883280757098, 0.7169811320754716, 0.7178683385579937, 0.715625, 0.7165109034267912, 0.717391304347826, 0.718266253869969, 0.7191358024691358, 0.72, 0.7208588957055214, 0.7186544342507645, 0.7164634146341463, 0.7142857142857143, 0.7121212121212122, 0.7129909365558912, 0.713855421686747, 0.7117117117117117, 0.7125748502994012, 0.7134328358208956, 0.7142857142857143, 0.7151335311572701, 0.7130177514792899, 0.7109144542772862, 0.711764705882353, 0.7126099706744868, 0.7134502923976608, 0.7142857142857143, 0.7151162790697675, 0.7159420289855073, 0.7138728323699421, 0.7146974063400576, 0.7155172413793104, 0.7134670487106017, 0.7114285714285714, 0.7122507122507122, 0.7102272727272727, 0.7082152974504249, 0.7090395480225988, 0.7098591549295775, 0.7106741573033708, 0.711484593837535, 0.7094972067039106, 0.7075208913649025, 0.7055555555555556, 0.7063711911357341, 0.7071823204419889, 0.7079889807162535, 0.7087912087912088, 0.7095890410958904, 0.7103825136612022, 0.7084468664850136, 0.7092391304347826, 0.7073170731707317, 0.7054054054054054, 0.706199460916442, 0.706989247311828, 0.707774798927614, 0.7085561497326203, 0.7093333333333334, 0.7101063829787234, 0.7108753315649867, 0.708994708994709, 0.7071240105540897, 0.7078947368421052, 0.7086614173228346, 0.7094240837696335, 0.7101827676240209, 0.7109375, 0.7116883116883117, 0.7124352331606217, 0.7131782945736435, 0.711340206185567, 0.712082262210797, 0.7128205128205128, 0.7135549872122762, 0.7142857142857143, 0.7150127226463104, 0.7157360406091371, 0.7164556962025317, 0.7171717171717171, 0.7153652392947103, 0.7160804020100503, 0.7142857142857143, 0.715, 0.71571072319202, 0.7164179104477612, 0.71712158808933, 0.7178217821782178, 0.7160493827160493, 0.7167487684729064, 0.7174447174447175, 0.7156862745098039, 0.7163814180929096, 0.7146341463414634, 0.7153284671532847, 0.7160194174757282, 0.7167070217917676, 0.714975845410628, 0.7132530120481928, 0.7139423076923077, 0.7122302158273381, 0.7129186602870813, 0.711217183770883, 0.7119047619047619, 0.7125890736342043, 0.7132701421800948, 0.7139479905437353, 0.714622641509434, 0.7152941176470589, 0.715962441314554, 0.7166276346604216, 0.7172897196261683, 0.717948717948718, 0.7162790697674418, 0.7146171693735499, 0.7129629629629629, 0.7136258660508084, 0.7142857142857143, 0.7149425287356321, 0.7155963302752294, 0.7162471395881007, 0.7146118721461188, 0.7129840546697038, 0.7113636363636363, 0.7120181405895691, 0.7104072398190046, 0.7110609480812641, 0.7094594594594594, 0.7101123595505618, 0.7107623318385651, 0.7114093959731543, 0.7098214285714286, 0.7082405345211581, 0.7088888888888889, 0.7073170731707317, 0.7079646017699115, 0.7064017660044151, 0.7070484581497798, 0.7076923076923077, 0.7083333333333334, 0.7089715536105032, 0.7074235807860262, 0.7058823529411765, 0.7043478260869566, 0.7049891540130152, 0.7056277056277056, 0.7062634989200864, 0.7068965517241379, 0.7053763440860215, 0.7060085836909872, 0.7044967880085653, 0.7029914529914529, 0.7036247334754797, 0.7021276595744681, 0.70276008492569, 0.7033898305084746, 0.7040169133192389, 0.7046413502109705, 0.7052631578947368, 0.7058823529411765, 0.7064989517819706, 0.7050209205020921, 0.7056367432150313, 0.7041666666666667, 0.7047817047817048, 0.7033195020746889, 0.7039337474120083, 0.7045454545454546, 0.7030927835051546, 0.7016460905349794, 0.702258726899384, 0.7008196721311475, 0.6993865030674846, 0.6979591836734694, 0.6985743380855397, 0.6971544715447154, 0.6977687626774848, 0.6983805668016194, 0.6989898989898989, 0.6975806451612904, 0.6961770623742455, 0.6967871485943775, 0.6953907815631263, 0.694]\n",
    "accuracy_cumulative_nn_50_d2_50_ml0_e200 = [1.0, 0.5, 0.6666666666666666, 0.75, 0.8, 0.6666666666666666, 0.5714285714285714, 0.5, 0.4444444444444444, 0.5, 0.5454545454545454, 0.5, 0.5384615384615384, 0.5714285714285714, 0.6, 0.625, 0.6470588235294118, 0.6666666666666666, 0.631578947368421, 0.65, 0.6666666666666666, 0.6818181818181818, 0.6956521739130435, 0.7083333333333334, 0.72, 0.7307692307692307, 0.7037037037037037, 0.6785714285714286, 0.6896551724137931, 0.7, 0.7096774193548387, 0.71875, 0.7272727272727273, 0.7352941176470589, 0.7428571428571429, 0.75, 0.7297297297297297, 0.7105263157894737, 0.717948717948718, 0.7, 0.7073170731707317, 0.6904761904761905, 0.6976744186046512, 0.7045454545454546, 0.7111111111111111, 0.717391304347826, 0.723404255319149, 0.7083333333333334, 0.6938775510204082, 0.68, 0.6862745098039216, 0.6730769230769231, 0.6792452830188679, 0.6851851851851852, 0.6909090909090909, 0.6964285714285714, 0.6842105263157895, 0.6724137931034483, 0.6779661016949152, 0.6833333333333333, 0.6885245901639344, 0.6935483870967742, 0.6984126984126984, 0.703125, 0.6923076923076923, 0.6818181818181818, 0.6716417910447762, 0.6617647058823529, 0.6521739130434783, 0.6571428571428571, 0.6619718309859155, 0.6527777777777778, 0.6575342465753424, 0.6621621621621622, 0.6666666666666666, 0.6710526315789473, 0.6753246753246753, 0.6666666666666666, 0.6582278481012658, 0.65, 0.6419753086419753, 0.6341463414634146, 0.6385542168674698, 0.6428571428571429, 0.6352941176470588, 0.627906976744186, 0.6206896551724138, 0.6136363636363636, 0.6067415730337079, 0.6, 0.6043956043956044, 0.5978260869565217, 0.5913978494623656, 0.5957446808510638, 0.6, 0.6041666666666666, 0.5979381443298969, 0.5918367346938775, 0.5858585858585859, 0.59, 0.594059405940594, 0.5980392156862745, 0.6019417475728155, 0.6057692307692307, 0.6095238095238096, 0.6132075471698113, 0.616822429906542, 0.6111111111111112, 0.6146788990825688, 0.6090909090909091, 0.6036036036036037, 0.5982142857142857, 0.6017699115044248, 0.6052631578947368, 0.6086956521739131, 0.603448275862069, 0.5982905982905983, 0.5932203389830508, 0.5966386554621849, 0.5916666666666667, 0.5950413223140496, 0.5983606557377049, 0.6016260162601627, 0.6048387096774194, 0.6, 0.6031746031746031, 0.6062992125984252, 0.609375, 0.6046511627906976, 0.6, 0.5954198473282443, 0.5984848484848485, 0.6015037593984962, 0.6044776119402985, 0.6, 0.6029411764705882, 0.6058394160583942, 0.6014492753623188, 0.60431654676259, 0.6071428571428571, 0.6099290780141844, 0.6056338028169014, 0.6083916083916084, 0.6111111111111112, 0.6137931034482759, 0.6095890410958904, 0.6122448979591837, 0.6081081081081081, 0.6040268456375839, 0.6066666666666667, 0.6026490066225165, 0.6052631578947368, 0.6078431372549019, 0.6103896103896104, 0.6129032258064516, 0.6153846153846154, 0.6178343949044586, 0.6139240506329114, 0.6163522012578616, 0.6125, 0.6149068322981367, 0.6172839506172839, 0.6196319018404908, 0.6219512195121951, 0.6242424242424243, 0.6204819277108434, 0.6167664670658682, 0.6130952380952381, 0.6153846153846154, 0.6176470588235294, 0.6198830409356725, 0.622093023255814, 0.6242774566473989, 0.6264367816091954, 0.6285714285714286, 0.6306818181818182, 0.6271186440677966, 0.6292134831460674, 0.6312849162011173, 0.6333333333333333, 0.6353591160220995, 0.6373626373626373, 0.639344262295082, 0.6413043478260869, 0.6378378378378379, 0.6397849462365591, 0.6363636363636364, 0.6382978723404256, 0.6349206349206349, 0.631578947368421, 0.6282722513089005, 0.6302083333333334, 0.6321243523316062, 0.634020618556701, 0.6307692307692307, 0.6326530612244898, 0.6294416243654822, 0.6262626262626263, 0.6231155778894473, 0.625]\n",
    "accuracy_cumulative_nn_50_d5_50_ml0_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.875, 0.7777777777777778, 0.7, 0.7272727272727273, 0.75, 0.7692307692307693, 0.7857142857142857, 0.7333333333333333, 0.75, 0.7647058823529411, 0.7777777777777778, 0.7894736842105263, 0.8, 0.8095238095238095, 0.8181818181818182, 0.8260869565217391, 0.8333333333333334, 0.84, 0.8461538461538461, 0.8518518518518519, 0.8214285714285714, 0.7931034482758621, 0.8, 0.8064516129032258, 0.78125, 0.7878787878787878, 0.7941176470588235, 0.7714285714285715, 0.7777777777777778, 0.7567567567567568, 0.7631578947368421, 0.7692307692307693, 0.775, 0.7560975609756098, 0.7619047619047619, 0.7674418604651163, 0.7727272727272727, 0.7777777777777778, 0.7608695652173914, 0.7659574468085106, 0.7708333333333334, 0.7551020408163265, 0.74, 0.7254901960784313, 0.7307692307692307, 0.7169811320754716, 0.7222222222222222, 0.7272727272727273, 0.7321428571428571, 0.7192982456140351, 0.7068965517241379, 0.711864406779661, 0.7, 0.7049180327868853, 0.7096774193548387, 0.7142857142857143, 0.71875, 0.7230769230769231, 0.7272727272727273, 0.7313432835820896, 0.7205882352941176, 0.7101449275362319, 0.7142857142857143, 0.704225352112676, 0.7083333333333334, 0.7123287671232876, 0.7162162162162162, 0.72, 0.7236842105263158, 0.7142857142857143, 0.7051282051282052, 0.6962025316455697, 0.6875, 0.691358024691358, 0.6951219512195121, 0.6987951807228916, 0.7023809523809523, 0.7058823529411765, 0.7093023255813954, 0.7126436781609196, 0.7159090909090909, 0.7078651685393258, 0.7111111111111111, 0.7142857142857143, 0.7065217391304348, 0.6989247311827957, 0.7021276595744681, 0.7052631578947368, 0.7083333333333334, 0.7010309278350515, 0.7040816326530612, 0.696969696969697, 0.69, 0.693069306930693, 0.6862745098039216, 0.6893203883495146, 0.6923076923076923, 0.6857142857142857, 0.6792452830188679, 0.6728971962616822, 0.6666666666666666, 0.6697247706422018, 0.6727272727272727, 0.6666666666666666, 0.6696428571428571, 0.672566371681416, 0.6754385964912281, 0.6695652173913044, 0.6724137931034483, 0.6752136752136753, 0.6694915254237288, 0.6638655462184874, 0.6583333333333333, 0.6528925619834711, 0.6475409836065574, 0.6422764227642277, 0.6451612903225806, 0.648, 0.6507936507936508, 0.6456692913385826, 0.640625, 0.6434108527131783, 0.6461538461538462, 0.6412213740458015, 0.6439393939393939, 0.6466165413533834, 0.6492537313432836, 0.6444444444444445, 0.6470588235294118, 0.6496350364963503, 0.644927536231884, 0.6474820143884892, 0.65, 0.6524822695035462, 0.647887323943662, 0.6503496503496503, 0.6527777777777778, 0.6482758620689655, 0.6506849315068494, 0.6462585034013606, 0.6418918918918919, 0.6375838926174496, 0.64, 0.6423841059602649, 0.6381578947368421, 0.6405228758169934, 0.6428571428571429, 0.6451612903225806, 0.6474358974358975, 0.643312101910828, 0.6392405063291139, 0.6352201257861635, 0.63125, 0.6273291925465838, 0.6296296296296297, 0.6319018404907976, 0.6341463414634146, 0.6363636363636364, 0.6385542168674698, 0.6407185628742516, 0.6428571428571429, 0.6390532544378699, 0.6411764705882353, 0.6432748538011696, 0.6453488372093024, 0.6473988439306358, 0.6494252873563219, 0.6514285714285715, 0.6534090909090909, 0.655367231638418, 0.651685393258427, 0.6480446927374302, 0.6444444444444445, 0.6464088397790055, 0.6428571428571429, 0.644808743169399, 0.6467391304347826, 0.6432432432432432, 0.6451612903225806, 0.6470588235294118, 0.6436170212765957, 0.6402116402116402, 0.6421052631578947, 0.6387434554973822, 0.6354166666666666, 0.6373056994818653, 0.6391752577319587, 0.6410256410256411, 0.6428571428571429, 0.6395939086294417, 0.6414141414141414, 0.6381909547738693, 0.635]\n",
    "accuracy_cumulative_nn_50_d8_50_ml0_e200 = [0.0, 0.5, 0.6666666666666666, 0.75, 0.8, 0.8333333333333334, 0.7142857142857143, 0.625, 0.6666666666666666, 0.6, 0.6363636363636364, 0.5833333333333334, 0.5384615384615384, 0.5714285714285714, 0.5333333333333333, 0.5625, 0.5294117647058824, 0.5, 0.5263157894736842, 0.5, 0.5238095238095238, 0.5, 0.5217391304347826, 0.5416666666666666, 0.56, 0.5769230769230769, 0.5555555555555556, 0.5357142857142857, 0.5517241379310345, 0.5666666666666667, 0.5483870967741935, 0.5625, 0.5757575757575758, 0.5882352941176471, 0.5714285714285714, 0.5833333333333334, 0.5675675675675675, 0.5789473684210527, 0.5897435897435898, 0.6, 0.5853658536585366, 0.5952380952380952, 0.6046511627906976, 0.6136363636363636, 0.6222222222222222, 0.6304347826086957, 0.6382978723404256, 0.6458333333333334, 0.6530612244897959, 0.64, 0.6470588235294118, 0.6346153846153846, 0.6226415094339622, 0.6296296296296297, 0.6363636363636364, 0.6428571428571429, 0.6491228070175439, 0.6379310344827587, 0.6271186440677966, 0.6166666666666667, 0.6065573770491803, 0.5967741935483871, 0.5873015873015873, 0.578125, 0.5846153846153846, 0.5909090909090909, 0.582089552238806, 0.5735294117647058, 0.5797101449275363, 0.5714285714285714, 0.5774647887323944, 0.5833333333333334, 0.589041095890411, 0.5945945945945946, 0.6, 0.6052631578947368, 0.5974025974025974, 0.5897435897435898, 0.5949367088607594, 0.5875, 0.5925925925925926, 0.5975609756097561, 0.6024096385542169, 0.6071428571428571, 0.611764705882353, 0.6162790697674418, 0.6206896551724138, 0.6136363636363636, 0.6179775280898876, 0.6111111111111112, 0.6043956043956044, 0.6086956521739131, 0.6129032258064516, 0.6170212765957447, 0.6210526315789474, 0.625, 0.6185567010309279, 0.6122448979591837, 0.6161616161616161, 0.61, 0.6138613861386139, 0.6176470588235294, 0.6213592233009708, 0.625, 0.6285714285714286, 0.6320754716981132, 0.6355140186915887, 0.6388888888888888, 0.6330275229357798, 0.6363636363636364, 0.6306306306306306, 0.625, 0.6283185840707964, 0.6228070175438597, 0.6260869565217392, 0.6293103448275862, 0.6324786324786325, 0.635593220338983, 0.6302521008403361, 0.625, 0.628099173553719, 0.6311475409836066, 0.6341463414634146, 0.6370967741935484, 0.632, 0.6349206349206349, 0.6377952755905512, 0.6328125, 0.627906976744186, 0.6230769230769231, 0.6259541984732825, 0.6287878787878788, 0.631578947368421, 0.6343283582089553, 0.6370370370370371, 0.6323529411764706, 0.6277372262773723, 0.6304347826086957, 0.6330935251798561, 0.6285714285714286, 0.6312056737588653, 0.6267605633802817, 0.6293706293706294, 0.6319444444444444, 0.6344827586206897, 0.636986301369863, 0.6326530612244898, 0.6283783783783784, 0.6241610738255033, 0.6266666666666667, 0.6225165562913907, 0.618421052631579, 0.6209150326797386, 0.6233766233766234, 0.6193548387096774, 0.6217948717948718, 0.6242038216560509, 0.620253164556962, 0.6163522012578616, 0.6125, 0.6149068322981367, 0.6111111111111112, 0.6134969325153374, 0.6158536585365854, 0.6181818181818182, 0.6204819277108434, 0.6167664670658682, 0.6130952380952381, 0.6094674556213018, 0.6058823529411764, 0.6023391812865497, 0.5988372093023255, 0.6011560693641619, 0.603448275862069, 0.6, 0.6022727272727273, 0.5988700564971752, 0.601123595505618, 0.5977653631284916, 0.5944444444444444, 0.5911602209944752, 0.5879120879120879, 0.5901639344262295, 0.592391304347826, 0.5891891891891892, 0.5913978494623656, 0.5882352941176471, 0.5904255319148937, 0.5873015873015873, 0.5894736842105263, 0.5916230366492147, 0.5885416666666666, 0.5906735751295337, 0.5927835051546392, 0.5897435897435898, 0.5867346938775511, 0.583756345177665, 0.5808080808080808, 0.5829145728643216, 0.58]\n",
    "plt.plot(accuracy_cumulative_nn_50_d0_50_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_d2_50_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_d5_50_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_d8_50_ml0_e200)\n",
    "plt.xlabel('trials')\n",
    "plt.xlim(0,len(accuracy_cumulative_nn_50_d8_50_ml0_e200))\n",
    "plt.ylim(0,1.01)\n",
    "plt.ylabel('cumulative accuracy')\n",
    "plt.legend(['d = 0', 'd = 0.2', 'd = 0.5', 'd = 0.8'])\n",
    "plt.title('Accuracy of NN model for different dropout rate before layer 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_cumulative_nn_50_d0_50_ml0_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.7142857142857143, 0.75, 0.6666666666666666, 0.7, 0.7272727272727273, 0.75, 0.7692307692307693, 0.7857142857142857, 0.8, 0.8125, 0.8235294117647058, 0.8333333333333334, 0.7894736842105263, 0.8, 0.8095238095238095, 0.8181818181818182, 0.8260869565217391, 0.8333333333333334, 0.84, 0.8461538461538461, 0.8518518518518519, 0.8214285714285714, 0.8275862068965517, 0.8333333333333334, 0.8064516129032258, 0.8125, 0.8181818181818182, 0.8235294117647058, 0.8, 0.8055555555555556, 0.7837837837837838, 0.7631578947368421, 0.7692307692307693, 0.75, 0.7560975609756098, 0.7380952380952381, 0.7441860465116279, 0.75, 0.7555555555555555, 0.7608695652173914, 0.7446808510638298, 0.75, 0.7346938775510204, 0.72, 0.7254901960784313, 0.7115384615384616, 0.7169811320754716, 0.7222222222222222, 0.7272727272727273, 0.7142857142857143, 0.7192982456140351, 0.7068965517241379, 0.6949152542372882, 0.7, 0.7049180327868853, 0.7096774193548387, 0.7142857142857143, 0.703125, 0.7076923076923077, 0.696969696969697, 0.7014925373134329, 0.6911764705882353, 0.6811594202898551, 0.6714285714285714, 0.676056338028169, 0.6805555555555556, 0.684931506849315, 0.6891891891891891, 0.6933333333333334, 0.6973684210526315, 0.7012987012987013, 0.6923076923076923, 0.6962025316455697, 0.6875, 0.691358024691358, 0.6951219512195121, 0.6987951807228916, 0.7023809523809523, 0.7058823529411765, 0.7093023255813954, 0.7126436781609196, 0.7045454545454546, 0.7078651685393258, 0.7111111111111111, 0.7142857142857143, 0.717391304347826, 0.7204301075268817, 0.723404255319149, 0.7263157894736842, 0.7291666666666666, 0.7319587628865979, 0.7346938775510204, 0.7373737373737373, 0.73, 0.7227722772277227, 0.7254901960784313, 0.7281553398058253, 0.7307692307692307, 0.7238095238095238, 0.7169811320754716, 0.719626168224299, 0.7129629629629629, 0.7155963302752294, 0.7090909090909091, 0.7117117117117117, 0.7142857142857143, 0.7168141592920354, 0.7192982456140351, 0.7217391304347827, 0.7241379310344828, 0.7264957264957265, 0.7203389830508474, 0.7226890756302521, 0.7166666666666667, 0.7107438016528925, 0.7131147540983607, 0.7154471544715447, 0.717741935483871, 0.72, 0.7222222222222222, 0.7244094488188977, 0.71875, 0.7209302325581395, 0.7153846153846154, 0.7099236641221374, 0.7045454545454546, 0.706766917293233, 0.7089552238805971, 0.7111111111111111, 0.7132352941176471, 0.7153284671532847, 0.717391304347826, 0.7122302158273381, 0.7071428571428572, 0.7021276595744681, 0.704225352112676, 0.7062937062937062, 0.7083333333333334, 0.7103448275862069, 0.7123287671232876, 0.7074829931972789, 0.7027027027027027, 0.7046979865771812, 0.7066666666666667, 0.7086092715231788, 0.7105263157894737, 0.7124183006535948, 0.7142857142857143, 0.7161290322580646, 0.717948717948718, 0.7197452229299363, 0.7151898734177216, 0.7169811320754716, 0.7125, 0.7080745341614907, 0.7098765432098766, 0.7116564417177914, 0.7134146341463414, 0.7151515151515152, 0.7168674698795181, 0.718562874251497, 0.7142857142857143, 0.7100591715976331, 0.7058823529411765, 0.7076023391812866, 0.7093023255813954, 0.7109826589595376, 0.7126436781609196, 0.7085714285714285, 0.7102272727272727, 0.7062146892655368, 0.7078651685393258, 0.7094972067039106, 0.7055555555555556, 0.7071823204419889, 0.7087912087912088, 0.7103825136612022, 0.7119565217391305, 0.7081081081081081, 0.7096774193548387, 0.7112299465240641, 0.7074468085106383, 0.7037037037037037, 0.7, 0.7015706806282722, 0.703125, 0.7046632124352331, 0.7061855670103093, 0.7076923076923077, 0.7040816326530612, 0.7055837563451777, 0.702020202020202, 0.6984924623115578, 0.7, 0.7014925373134329, 0.7029702970297029, 0.7044334975369458, 0.7058823529411765, 0.7073170731707317, 0.7087378640776699, 0.7101449275362319, 0.7067307692307693, 0.7081339712918661, 0.7047619047619048, 0.7061611374407583, 0.7075471698113207, 0.7089201877934272, 0.7102803738317757, 0.7116279069767442, 0.7129629629629629, 0.7142857142857143, 0.7155963302752294, 0.7168949771689498, 0.7136363636363636, 0.7149321266968326, 0.7117117117117117, 0.7130044843049327, 0.7142857142857143, 0.7155555555555555, 0.7168141592920354, 0.7180616740088106, 0.7192982456140351, 0.7161572052401747, 0.7130434782608696, 0.70995670995671, 0.7068965517241379, 0.7081545064377682, 0.7094017094017094, 0.7106382978723405, 0.711864406779661, 0.7088607594936709, 0.7100840336134454, 0.7112970711297071, 0.7125, 0.7136929460580913, 0.7148760330578512, 0.7160493827160493, 0.7172131147540983, 0.7183673469387755, 0.7195121951219512, 0.7206477732793523, 0.7217741935483871, 0.7188755020080321, 0.72, 0.7211155378486056, 0.7182539682539683, 0.7193675889328063, 0.7204724409448819, 0.7215686274509804, 0.72265625, 0.7237354085603113, 0.7248062015503876, 0.722007722007722, 0.7192307692307692, 0.7203065134099617, 0.7213740458015268, 0.7224334600760456, 0.7234848484848485, 0.7245283018867924, 0.7255639097744361, 0.7265917602996255, 0.7238805970149254, 0.724907063197026, 0.7222222222222222, 0.7232472324723247, 0.7205882352941176, 0.7216117216117216, 0.7226277372262774, 0.7236363636363636, 0.7210144927536232, 0.7220216606498195, 0.7230215827338129, 0.7204301075268817, 0.7178571428571429, 0.7153024911032029, 0.7127659574468085, 0.7137809187279152, 0.7147887323943662, 0.7157894736842105, 0.7132867132867133, 0.7142857142857143, 0.7118055555555556, 0.71280276816609, 0.7137931034482758, 0.7147766323024055, 0.7157534246575342, 0.7167235494880546, 0.717687074829932, 0.7186440677966102, 0.7195945945945946, 0.7205387205387206, 0.7181208053691275, 0.7157190635451505, 0.7133333333333334, 0.7142857142857143, 0.7152317880794702, 0.7161716171617162, 0.7171052631578947, 0.7180327868852459, 0.7156862745098039, 0.7133550488599348, 0.7142857142857143, 0.7152103559870551, 0.7129032258064516, 0.7138263665594855, 0.7147435897435898, 0.7156549520766773, 0.7165605095541401, 0.7174603174603175, 0.7183544303797469, 0.7160883280757098, 0.7169811320754716, 0.7178683385579937, 0.715625, 0.7165109034267912, 0.717391304347826, 0.718266253869969, 0.7191358024691358, 0.72, 0.7208588957055214, 0.7186544342507645, 0.7164634146341463, 0.7142857142857143, 0.7121212121212122, 0.7129909365558912, 0.713855421686747, 0.7117117117117117, 0.7125748502994012, 0.7134328358208956, 0.7142857142857143, 0.7151335311572701, 0.7130177514792899, 0.7109144542772862, 0.711764705882353, 0.7126099706744868, 0.7134502923976608, 0.7142857142857143, 0.7151162790697675, 0.7159420289855073, 0.7138728323699421, 0.7146974063400576, 0.7155172413793104, 0.7134670487106017, 0.7114285714285714, 0.7122507122507122, 0.7102272727272727, 0.7082152974504249, 0.7090395480225988, 0.7098591549295775, 0.7106741573033708, 0.711484593837535, 0.7094972067039106, 0.7075208913649025, 0.7055555555555556, 0.7063711911357341, 0.7071823204419889, 0.7079889807162535, 0.7087912087912088, 0.7095890410958904, 0.7103825136612022, 0.7084468664850136, 0.7092391304347826, 0.7073170731707317, 0.7054054054054054, 0.706199460916442, 0.706989247311828, 0.707774798927614, 0.7085561497326203, 0.7093333333333334, 0.7101063829787234, 0.7108753315649867, 0.708994708994709, 0.7071240105540897, 0.7078947368421052, 0.7086614173228346, 0.7094240837696335, 0.7101827676240209, 0.7109375, 0.7116883116883117, 0.7124352331606217, 0.7131782945736435, 0.711340206185567, 0.712082262210797, 0.7128205128205128, 0.7135549872122762, 0.7142857142857143, 0.7150127226463104, 0.7157360406091371, 0.7164556962025317, 0.7171717171717171, 0.7153652392947103, 0.7160804020100503, 0.7142857142857143, 0.715, 0.71571072319202, 0.7164179104477612, 0.71712158808933, 0.7178217821782178, 0.7160493827160493, 0.7167487684729064, 0.7174447174447175, 0.7156862745098039, 0.7163814180929096, 0.7146341463414634, 0.7153284671532847, 0.7160194174757282, 0.7167070217917676, 0.714975845410628, 0.7132530120481928, 0.7139423076923077, 0.7122302158273381, 0.7129186602870813, 0.711217183770883, 0.7119047619047619, 0.7125890736342043, 0.7132701421800948, 0.7139479905437353, 0.714622641509434, 0.7152941176470589, 0.715962441314554, 0.7166276346604216, 0.7172897196261683, 0.717948717948718, 0.7162790697674418, 0.7146171693735499, 0.7129629629629629, 0.7136258660508084, 0.7142857142857143, 0.7149425287356321, 0.7155963302752294, 0.7162471395881007, 0.7146118721461188, 0.7129840546697038, 0.7113636363636363, 0.7120181405895691, 0.7104072398190046, 0.7110609480812641, 0.7094594594594594, 0.7101123595505618, 0.7107623318385651, 0.7114093959731543, 0.7098214285714286, 0.7082405345211581, 0.7088888888888889, 0.7073170731707317, 0.7079646017699115, 0.7064017660044151, 0.7070484581497798, 0.7076923076923077, 0.7083333333333334, 0.7089715536105032, 0.7074235807860262, 0.7058823529411765, 0.7043478260869566, 0.7049891540130152, 0.7056277056277056, 0.7062634989200864, 0.7068965517241379, 0.7053763440860215, 0.7060085836909872, 0.7044967880085653, 0.7029914529914529, 0.7036247334754797, 0.7021276595744681, 0.70276008492569, 0.7033898305084746, 0.7040169133192389, 0.7046413502109705, 0.7052631578947368, 0.7058823529411765, 0.7064989517819706, 0.7050209205020921, 0.7056367432150313, 0.7041666666666667, 0.7047817047817048, 0.7033195020746889, 0.7039337474120083, 0.7045454545454546, 0.7030927835051546, 0.7016460905349794, 0.702258726899384, 0.7008196721311475, 0.6993865030674846, 0.6979591836734694, 0.6985743380855397, 0.6971544715447154, 0.6977687626774848, 0.6983805668016194, 0.6989898989898989, 0.6975806451612904, 0.6961770623742455, 0.6967871485943775, 0.6953907815631263, 0.694]\n",
    "accuracy_cumulative_nn_50_d0_50_ml0_e400 = [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.7142857142857143, 0.625, 0.6666666666666666, 0.6, 0.6363636363636364, 0.5833333333333334, 0.6153846153846154, 0.6428571428571429, 0.6666666666666666, 0.625, 0.5882352941176471, 0.6111111111111112, 0.5789473684210527, 0.6, 0.6190476190476191, 0.5909090909090909, 0.6086956521739131, 0.625, 0.64, 0.6153846153846154, 0.5925925925925926, 0.5714285714285714, 0.5517241379310345, 0.5333333333333333, 0.5161290322580645, 0.5, 0.5151515151515151, 0.5294117647058824, 0.5428571428571428, 0.5555555555555556, 0.5405405405405406, 0.5263157894736842, 0.5384615384615384, 0.525, 0.5121951219512195, 0.5238095238095238, 0.5348837209302325, 0.5454545454545454, 0.5555555555555556, 0.5652173913043478, 0.574468085106383, 0.5625, 0.5510204081632653, 0.56, 0.5686274509803921, 0.5769230769230769, 0.5849056603773585, 0.5925925925925926, 0.6, 0.5892857142857143, 0.5789473684210527, 0.5689655172413793, 0.559322033898305, 0.55, 0.5573770491803278, 0.5645161290322581, 0.5714285714285714, 0.578125, 0.5846153846153846, 0.5909090909090909, 0.5970149253731343, 0.5882352941176471, 0.5942028985507246, 0.5857142857142857, 0.5774647887323944, 0.5833333333333334, 0.589041095890411, 0.5945945945945946, 0.6, 0.5921052631578947, 0.5844155844155844, 0.5769230769230769, 0.5822784810126582, 0.575, 0.5679012345679012, 0.5609756097560976, 0.5662650602409639, 0.5714285714285714, 0.5764705882352941, 0.5813953488372093, 0.5747126436781609, 0.5795454545454546, 0.5730337078651685, 0.5777777777777777, 0.5824175824175825, 0.5760869565217391, 0.5806451612903226, 0.5851063829787234, 0.5894736842105263, 0.59375, 0.5979381443298969, 0.6020408163265306, 0.5959595959595959, 0.6, 0.594059405940594, 0.5980392156862745, 0.5922330097087378, 0.5961538461538461, 0.6, 0.6037735849056604, 0.5981308411214953, 0.6018518518518519, 0.5963302752293578, 0.5909090909090909, 0.5945945945945946, 0.5982142857142857, 0.6017699115044248, 0.6052631578947368, 0.6086956521739131, 0.6120689655172413, 0.6068376068376068, 0.6016949152542372, 0.6050420168067226, 0.6083333333333333, 0.6115702479338843, 0.6147540983606558, 0.6178861788617886, 0.6209677419354839, 0.616, 0.6111111111111112, 0.6062992125984252, 0.6015625, 0.6046511627906976, 0.6, 0.6030534351145038, 0.6060606060606061, 0.6090225563909775, 0.6119402985074627, 0.6074074074074074, 0.6029411764705882, 0.5985401459854015, 0.5942028985507246, 0.5899280575539568, 0.5857142857142857, 0.5886524822695035, 0.5915492957746479, 0.5944055944055944, 0.5972222222222222, 0.593103448275862, 0.5958904109589042, 0.5986394557823129, 0.5945945945945946, 0.5906040268456376, 0.5933333333333334, 0.5960264900662252, 0.5986842105263158, 0.6013071895424836, 0.6038961038961039, 0.6064516129032258, 0.6089743589743589, 0.6114649681528662, 0.6075949367088608, 0.610062893081761, 0.60625, 0.6024844720496895, 0.6049382716049383, 0.6073619631901841, 0.6097560975609756, 0.6121212121212121, 0.6144578313253012, 0.6107784431137725, 0.6071428571428571, 0.6035502958579881, 0.6, 0.6023391812865497, 0.6046511627906976, 0.6069364161849711, 0.603448275862069, 0.6057142857142858, 0.6079545454545454, 0.6045197740112994, 0.6067415730337079, 0.6033519553072626, 0.6055555555555555, 0.6077348066298343, 0.6098901098901099, 0.6120218579234973, 0.6141304347826086, 0.6162162162162163, 0.6182795698924731, 0.6203208556149733, 0.6170212765957447, 0.6137566137566137, 0.6105263157894737, 0.6073298429319371, 0.609375, 0.6113989637305699, 0.6134020618556701, 0.6153846153846154, 0.6122448979591837, 0.6142131979695431, 0.6111111111111112, 0.6130653266331658, 0.61, 0.6069651741293532, 0.6089108910891089, 0.6108374384236454, 0.6127450980392157, 0.6146341463414634, 0.616504854368932, 0.6135265700483091, 0.6105769230769231, 0.6124401913875598, 0.6095238095238096, 0.6113744075829384, 0.6132075471698113, 0.6150234741784038, 0.616822429906542, 0.6186046511627907, 0.6157407407407407, 0.6129032258064516, 0.6100917431192661, 0.6073059360730594, 0.6045454545454545, 0.6018099547511312, 0.5990990990990991, 0.600896860986547, 0.6026785714285714, 0.6, 0.5973451327433629, 0.5947136563876652, 0.5964912280701754, 0.5938864628820961, 0.591304347826087, 0.5887445887445888, 0.5905172413793104, 0.592274678111588, 0.594017094017094, 0.5914893617021276, 0.5932203389830508, 0.5949367088607594, 0.5966386554621849, 0.5941422594142259, 0.5916666666666667, 0.5933609958506224, 0.5950413223140496, 0.5967078189300411, 0.5983606557377049, 0.6, 0.6016260162601627, 0.6032388663967612, 0.6008064516129032, 0.6024096385542169, 0.6, 0.601593625498008, 0.6031746031746031, 0.6047430830039525, 0.6062992125984252, 0.6078431372549019, 0.60546875, 0.603112840466926, 0.6046511627906976, 0.6023166023166023, 0.6, 0.5977011494252874, 0.5992366412213741, 0.6007604562737643, 0.6022727272727273, 0.6037735849056604, 0.6052631578947368, 0.602996254681648, 0.6044776119402985, 0.6022304832713755, 0.6, 0.6014760147601476, 0.6029411764705882, 0.6043956043956044, 0.6058394160583942, 0.6072727272727273, 0.605072463768116, 0.6028880866425993, 0.6007194244604317, 0.5985663082437276, 0.5964285714285714, 0.594306049822064, 0.5957446808510638, 0.5971731448763251, 0.5985915492957746, 0.6, 0.6013986013986014, 0.6027874564459931, 0.6006944444444444, 0.6020761245674741, 0.6, 0.6013745704467354, 0.6027397260273972, 0.6040955631399317, 0.6054421768707483, 0.6033898305084746, 0.6013513513513513, 0.5993265993265994, 0.5973154362416108, 0.5986622073578596, 0.5966666666666667, 0.5946843853820598, 0.5960264900662252, 0.5973597359735974, 0.5986842105263158, 0.6, 0.6013071895424836, 0.5993485342019544, 0.5974025974025974, 0.5954692556634305, 0.5935483870967742, 0.594855305466238, 0.5961538461538461, 0.597444089456869, 0.5987261146496815, 0.6, 0.5981012658227848, 0.5962145110410094, 0.5943396226415094, 0.5956112852664577, 0.59375, 0.5950155763239875, 0.593167701863354, 0.5944272445820433, 0.595679012345679, 0.5969230769230769, 0.598159509202454, 0.599388379204893, 0.5975609756097561, 0.5987841945288754, 0.5969696969696969, 0.5981873111782477, 0.5993975903614458, 0.6006006006006006, 0.6017964071856288, 0.6029850746268657, 0.6011904761904762, 0.599406528189911, 0.6005917159763313, 0.5988200589970502, 0.5970588235294118, 0.5982404692082112, 0.5994152046783626, 0.6005830903790087, 0.6017441860465116, 0.6028985507246377, 0.6011560693641619, 0.6023054755043228, 0.6005747126436781, 0.6017191977077364, 0.6, 0.6011396011396012, 0.5994318181818182, 0.6005665722379604, 0.6016949152542372, 0.6028169014084507, 0.601123595505618, 0.5994397759103641, 0.5977653631284916, 0.596100278551532, 0.5944444444444444, 0.5955678670360111, 0.5966850828729282, 0.5977961432506887, 0.5989010989010989, 0.6, 0.5983606557377049, 0.5994550408719346, 0.5978260869565217, 0.5962059620596206, 0.5945945945945946, 0.5956873315363881, 0.5967741935483871, 0.5978552278820375, 0.5989304812834224, 0.6, 0.598404255319149, 0.5994694960212201, 0.5978835978835979, 0.5989445910290238, 0.5973684210526315, 0.5984251968503937, 0.5968586387434555, 0.597911227154047, 0.5989583333333334, 0.6, 0.5984455958549223, 0.5968992248062015, 0.595360824742268, 0.5938303341902313, 0.5923076923076923, 0.5933503836317136, 0.5943877551020408, 0.5954198473282443, 0.5964467005076142, 0.5949367088607594, 0.5959595959595959, 0.5944584382871536, 0.592964824120603, 0.5939849624060151, 0.5925, 0.5935162094763092, 0.5945273631840796, 0.5955334987593052, 0.5965346534653465, 0.5975308641975309, 0.5985221674876847, 0.597051597051597, 0.5955882352941176, 0.5941320293398533, 0.5926829268292683, 0.5936739659367397, 0.5922330097087378, 0.5907990314769975, 0.5917874396135265, 0.5903614457831325, 0.5889423076923077, 0.5899280575539568, 0.5885167464114832, 0.5894988066825776, 0.5880952380952381, 0.5890736342042755, 0.590047393364929, 0.5910165484633569, 0.5919811320754716, 0.5905882352941176, 0.5915492957746479, 0.5901639344262295, 0.5887850467289719, 0.5874125874125874, 0.5883720930232558, 0.5893271461716937, 0.5902777777777778, 0.5912240184757506, 0.5921658986175116, 0.593103448275862, 0.591743119266055, 0.5903890160183066, 0.589041095890411, 0.5876993166287016, 0.5863636363636363, 0.5873015873015873, 0.5859728506787331, 0.5869074492099323, 0.5878378378378378, 0.5865168539325842, 0.5852017937219731, 0.5861297539149888, 0.5848214285714286, 0.5835189309576837, 0.5822222222222222, 0.5831485587583148, 0.584070796460177, 0.5849889624724062, 0.5859030837004405, 0.5846153846153846, 0.5855263157894737, 0.5864332603938731, 0.5873362445414847, 0.5882352941176471, 0.5869565217391305, 0.5878524945770065, 0.5887445887445888, 0.5896328293736501, 0.5905172413793104, 0.5913978494623656, 0.592274678111588, 0.5910064239828694, 0.5897435897435898, 0.5884861407249466, 0.5893617021276596, 0.5902335456475584, 0.5889830508474576, 0.5898520084566596, 0.5907172995780591, 0.5915789473684211, 0.5903361344537815, 0.5911949685534591, 0.5899581589958159, 0.5887265135699373, 0.5875, 0.5883575883575883, 0.5871369294605809, 0.587991718426501, 0.5888429752066116, 0.5896907216494846, 0.588477366255144, 0.5893223819301848, 0.5881147540983607, 0.5869120654396728, 0.5857142857142857, 0.5845213849287169, 0.5853658536585366, 0.5862068965517241, 0.5870445344129555, 0.5878787878787879, 0.5887096774193549, 0.5875251509054326, 0.5863453815261044, 0.5871743486973948, 0.588]\n",
    "plt.plot(accuracy_cumulative_nn_50_d0_50_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_d0_50_ml0_e400)\n",
    "plt.xlabel('trials')\n",
    "plt.xlim(0,len(accuracy_cumulative_nn_50_d0_50_ml0_e200))\n",
    "plt.ylabel('cumulative accuracy')\n",
    "plt.ylim(0.5,1.01)\n",
    "plt.legend(['epochs = 200: reference', 'epochs = 400: longer training'])\n",
    "plt.title('Longer training of NN model leads to overfitting');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe overfitting: the accuracy for 400 training epochs (0.61 after 200 trials) is significantly lower than for 200 epochs (0.694 after 500 trials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_cumulative_nn_50_50_ml1600_e200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_cumulative_nn_50_50_ml1600_e200[500-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_cumulative_nn_50_d0_50_ml0_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.7142857142857143, 0.75, 0.6666666666666666, 0.7, 0.7272727272727273, 0.75, 0.7692307692307693, 0.7857142857142857, 0.8, 0.8125, 0.8235294117647058, 0.8333333333333334, 0.7894736842105263, 0.8, 0.8095238095238095, 0.8181818181818182, 0.8260869565217391, 0.8333333333333334, 0.84, 0.8461538461538461, 0.8518518518518519, 0.8214285714285714, 0.8275862068965517, 0.8333333333333334, 0.8064516129032258, 0.8125, 0.8181818181818182, 0.8235294117647058, 0.8, 0.8055555555555556, 0.7837837837837838, 0.7631578947368421, 0.7692307692307693, 0.75, 0.7560975609756098, 0.7380952380952381, 0.7441860465116279, 0.75, 0.7555555555555555, 0.7608695652173914, 0.7446808510638298, 0.75, 0.7346938775510204, 0.72, 0.7254901960784313, 0.7115384615384616, 0.7169811320754716, 0.7222222222222222, 0.7272727272727273, 0.7142857142857143, 0.7192982456140351, 0.7068965517241379, 0.6949152542372882, 0.7, 0.7049180327868853, 0.7096774193548387, 0.7142857142857143, 0.703125, 0.7076923076923077, 0.696969696969697, 0.7014925373134329, 0.6911764705882353, 0.6811594202898551, 0.6714285714285714, 0.676056338028169, 0.6805555555555556, 0.684931506849315, 0.6891891891891891, 0.6933333333333334, 0.6973684210526315, 0.7012987012987013, 0.6923076923076923, 0.6962025316455697, 0.6875, 0.691358024691358, 0.6951219512195121, 0.6987951807228916, 0.7023809523809523, 0.7058823529411765, 0.7093023255813954, 0.7126436781609196, 0.7045454545454546, 0.7078651685393258, 0.7111111111111111, 0.7142857142857143, 0.717391304347826, 0.7204301075268817, 0.723404255319149, 0.7263157894736842, 0.7291666666666666, 0.7319587628865979, 0.7346938775510204, 0.7373737373737373, 0.73, 0.7227722772277227, 0.7254901960784313, 0.7281553398058253, 0.7307692307692307, 0.7238095238095238, 0.7169811320754716, 0.719626168224299, 0.7129629629629629, 0.7155963302752294, 0.7090909090909091, 0.7117117117117117, 0.7142857142857143, 0.7168141592920354, 0.7192982456140351, 0.7217391304347827, 0.7241379310344828, 0.7264957264957265, 0.7203389830508474, 0.7226890756302521, 0.7166666666666667, 0.7107438016528925, 0.7131147540983607, 0.7154471544715447, 0.717741935483871, 0.72, 0.7222222222222222, 0.7244094488188977, 0.71875, 0.7209302325581395, 0.7153846153846154, 0.7099236641221374, 0.7045454545454546, 0.706766917293233, 0.7089552238805971, 0.7111111111111111, 0.7132352941176471, 0.7153284671532847, 0.717391304347826, 0.7122302158273381, 0.7071428571428572, 0.7021276595744681, 0.704225352112676, 0.7062937062937062, 0.7083333333333334, 0.7103448275862069, 0.7123287671232876, 0.7074829931972789, 0.7027027027027027, 0.7046979865771812, 0.7066666666666667, 0.7086092715231788, 0.7105263157894737, 0.7124183006535948, 0.7142857142857143, 0.7161290322580646, 0.717948717948718, 0.7197452229299363, 0.7151898734177216, 0.7169811320754716, 0.7125, 0.7080745341614907, 0.7098765432098766, 0.7116564417177914, 0.7134146341463414, 0.7151515151515152, 0.7168674698795181, 0.718562874251497, 0.7142857142857143, 0.7100591715976331, 0.7058823529411765, 0.7076023391812866, 0.7093023255813954, 0.7109826589595376, 0.7126436781609196, 0.7085714285714285, 0.7102272727272727, 0.7062146892655368, 0.7078651685393258, 0.7094972067039106, 0.7055555555555556, 0.7071823204419889, 0.7087912087912088, 0.7103825136612022, 0.7119565217391305, 0.7081081081081081, 0.7096774193548387, 0.7112299465240641, 0.7074468085106383, 0.7037037037037037, 0.7, 0.7015706806282722, 0.703125, 0.7046632124352331, 0.7061855670103093, 0.7076923076923077, 0.7040816326530612, 0.7055837563451777, 0.702020202020202, 0.6984924623115578, 0.7, 0.7014925373134329, 0.7029702970297029, 0.7044334975369458, 0.7058823529411765, 0.7073170731707317, 0.7087378640776699, 0.7101449275362319, 0.7067307692307693, 0.7081339712918661, 0.7047619047619048, 0.7061611374407583, 0.7075471698113207, 0.7089201877934272, 0.7102803738317757, 0.7116279069767442, 0.7129629629629629, 0.7142857142857143, 0.7155963302752294, 0.7168949771689498, 0.7136363636363636, 0.7149321266968326, 0.7117117117117117, 0.7130044843049327, 0.7142857142857143, 0.7155555555555555, 0.7168141592920354, 0.7180616740088106, 0.7192982456140351, 0.7161572052401747, 0.7130434782608696, 0.70995670995671, 0.7068965517241379, 0.7081545064377682, 0.7094017094017094, 0.7106382978723405, 0.711864406779661, 0.7088607594936709, 0.7100840336134454, 0.7112970711297071, 0.7125, 0.7136929460580913, 0.7148760330578512, 0.7160493827160493, 0.7172131147540983, 0.7183673469387755, 0.7195121951219512, 0.7206477732793523, 0.7217741935483871, 0.7188755020080321, 0.72, 0.7211155378486056, 0.7182539682539683, 0.7193675889328063, 0.7204724409448819, 0.7215686274509804, 0.72265625, 0.7237354085603113, 0.7248062015503876, 0.722007722007722, 0.7192307692307692, 0.7203065134099617, 0.7213740458015268, 0.7224334600760456, 0.7234848484848485, 0.7245283018867924, 0.7255639097744361, 0.7265917602996255, 0.7238805970149254, 0.724907063197026, 0.7222222222222222, 0.7232472324723247, 0.7205882352941176, 0.7216117216117216, 0.7226277372262774, 0.7236363636363636, 0.7210144927536232, 0.7220216606498195, 0.7230215827338129, 0.7204301075268817, 0.7178571428571429, 0.7153024911032029, 0.7127659574468085, 0.7137809187279152, 0.7147887323943662, 0.7157894736842105, 0.7132867132867133, 0.7142857142857143, 0.7118055555555556, 0.71280276816609, 0.7137931034482758, 0.7147766323024055, 0.7157534246575342, 0.7167235494880546, 0.717687074829932, 0.7186440677966102, 0.7195945945945946, 0.7205387205387206, 0.7181208053691275, 0.7157190635451505, 0.7133333333333334, 0.7142857142857143, 0.7152317880794702, 0.7161716171617162, 0.7171052631578947, 0.7180327868852459, 0.7156862745098039, 0.7133550488599348, 0.7142857142857143, 0.7152103559870551, 0.7129032258064516, 0.7138263665594855, 0.7147435897435898, 0.7156549520766773, 0.7165605095541401, 0.7174603174603175, 0.7183544303797469, 0.7160883280757098, 0.7169811320754716, 0.7178683385579937, 0.715625, 0.7165109034267912, 0.717391304347826, 0.718266253869969, 0.7191358024691358, 0.72, 0.7208588957055214, 0.7186544342507645, 0.7164634146341463, 0.7142857142857143, 0.7121212121212122, 0.7129909365558912, 0.713855421686747, 0.7117117117117117, 0.7125748502994012, 0.7134328358208956, 0.7142857142857143, 0.7151335311572701, 0.7130177514792899, 0.7109144542772862, 0.711764705882353, 0.7126099706744868, 0.7134502923976608, 0.7142857142857143, 0.7151162790697675, 0.7159420289855073, 0.7138728323699421, 0.7146974063400576, 0.7155172413793104, 0.7134670487106017, 0.7114285714285714, 0.7122507122507122, 0.7102272727272727, 0.7082152974504249, 0.7090395480225988, 0.7098591549295775, 0.7106741573033708, 0.711484593837535, 0.7094972067039106, 0.7075208913649025, 0.7055555555555556, 0.7063711911357341, 0.7071823204419889, 0.7079889807162535, 0.7087912087912088, 0.7095890410958904, 0.7103825136612022, 0.7084468664850136, 0.7092391304347826, 0.7073170731707317, 0.7054054054054054, 0.706199460916442, 0.706989247311828, 0.707774798927614, 0.7085561497326203, 0.7093333333333334, 0.7101063829787234, 0.7108753315649867, 0.708994708994709, 0.7071240105540897, 0.7078947368421052, 0.7086614173228346, 0.7094240837696335, 0.7101827676240209, 0.7109375, 0.7116883116883117, 0.7124352331606217, 0.7131782945736435, 0.711340206185567, 0.712082262210797, 0.7128205128205128, 0.7135549872122762, 0.7142857142857143, 0.7150127226463104, 0.7157360406091371, 0.7164556962025317, 0.7171717171717171, 0.7153652392947103, 0.7160804020100503, 0.7142857142857143, 0.715, 0.71571072319202, 0.7164179104477612, 0.71712158808933, 0.7178217821782178, 0.7160493827160493, 0.7167487684729064, 0.7174447174447175, 0.7156862745098039, 0.7163814180929096, 0.7146341463414634, 0.7153284671532847, 0.7160194174757282, 0.7167070217917676, 0.714975845410628, 0.7132530120481928, 0.7139423076923077, 0.7122302158273381, 0.7129186602870813, 0.711217183770883, 0.7119047619047619, 0.7125890736342043, 0.7132701421800948, 0.7139479905437353, 0.714622641509434, 0.7152941176470589, 0.715962441314554, 0.7166276346604216, 0.7172897196261683, 0.717948717948718, 0.7162790697674418, 0.7146171693735499, 0.7129629629629629, 0.7136258660508084, 0.7142857142857143, 0.7149425287356321, 0.7155963302752294, 0.7162471395881007, 0.7146118721461188, 0.7129840546697038, 0.7113636363636363, 0.7120181405895691, 0.7104072398190046, 0.7110609480812641, 0.7094594594594594, 0.7101123595505618, 0.7107623318385651, 0.7114093959731543, 0.7098214285714286, 0.7082405345211581, 0.7088888888888889, 0.7073170731707317, 0.7079646017699115, 0.7064017660044151, 0.7070484581497798, 0.7076923076923077, 0.7083333333333334, 0.7089715536105032, 0.7074235807860262, 0.7058823529411765, 0.7043478260869566, 0.7049891540130152, 0.7056277056277056, 0.7062634989200864, 0.7068965517241379, 0.7053763440860215, 0.7060085836909872, 0.7044967880085653, 0.7029914529914529, 0.7036247334754797, 0.7021276595744681, 0.70276008492569, 0.7033898305084746, 0.7040169133192389, 0.7046413502109705, 0.7052631578947368, 0.7058823529411765, 0.7064989517819706, 0.7050209205020921, 0.7056367432150313, 0.7041666666666667, 0.7047817047817048, 0.7033195020746889, 0.7039337474120083, 0.7045454545454546, 0.7030927835051546, 0.7016460905349794, 0.702258726899384, 0.7008196721311475, 0.6993865030674846, 0.6979591836734694, 0.6985743380855397, 0.6971544715447154, 0.6977687626774848, 0.6983805668016194, 0.6989898989898989, 0.6975806451612904, 0.6961770623742455, 0.6967871485943775, 0.6953907815631263, 0.694]\n",
    "accuracy_cumulative_nn_50_50_ml200_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.8571428571428571, 0.875, 0.7777777777777778, 0.8, 0.7272727272727273, 0.75, 0.7692307692307693, 0.7857142857142857, 0.8, 0.8125, 0.8235294117647058, 0.8333333333333334, 0.8421052631578947, 0.85, 0.8095238095238095, 0.8181818181818182, 0.8260869565217391, 0.8333333333333334, 0.84, 0.8461538461538461, 0.8518518518518519, 0.8571428571428571, 0.8620689655172413, 0.8666666666666667, 0.8387096774193549, 0.84375, 0.8484848484848485, 0.8235294117647058, 0.8285714285714286, 0.8333333333333334, 0.8378378378378378, 0.8421052631578947, 0.8461538461538461, 0.85, 0.8292682926829268, 0.8333333333333334, 0.8372093023255814, 0.8409090909090909, 0.8444444444444444, 0.8478260869565217, 0.851063829787234, 0.8541666666666666, 0.8571428571428571, 0.86, 0.8627450980392157, 0.8653846153846154, 0.8679245283018868, 0.8703703703703703, 0.8727272727272727, 0.875, 0.8771929824561403, 0.8793103448275862, 0.8813559322033898, 0.8833333333333333, 0.8852459016393442, 0.8870967741935484, 0.8888888888888888, 0.890625, 0.8923076923076924, 0.8939393939393939, 0.8805970149253731, 0.8823529411764706, 0.8840579710144928, 0.8857142857142857, 0.8873239436619719, 0.875, 0.8767123287671232, 0.8783783783783784, 0.88, 0.881578947368421, 0.8701298701298701, 0.8717948717948718, 0.8734177215189873, 0.875, 0.8765432098765432, 0.8780487804878049, 0.8795180722891566, 0.8809523809523809, 0.8823529411764706, 0.8837209302325582, 0.8850574712643678, 0.8863636363636364, 0.8876404494382022, 0.8888888888888888, 0.8901098901098901, 0.8913043478260869, 0.8924731182795699, 0.8936170212765957, 0.8947368421052632, 0.8958333333333334, 0.8969072164948454, 0.8979591836734694, 0.898989898989899, 0.89, 0.8910891089108911, 0.8921568627450981, 0.883495145631068, 0.8846153846153846, 0.8857142857142857, 0.8867924528301887, 0.8785046728971962, 0.8796296296296297, 0.8807339449541285, 0.8818181818181818, 0.8828828828828829, 0.8839285714285714, 0.8849557522123894, 0.8859649122807017, 0.8869565217391304, 0.8879310344827587, 0.8888888888888888, 0.8813559322033898, 0.8823529411764706, 0.8833333333333333, 0.8842975206611571, 0.8852459016393442, 0.8861788617886179, 0.8870967741935484, 0.888, 0.8888888888888888, 0.889763779527559, 0.890625, 0.8914728682170543, 0.8846153846153846, 0.8854961832061069, 0.8863636363636364, 0.8872180451127819, 0.8880597014925373, 0.8888888888888888, 0.8897058823529411, 0.8905109489051095, 0.8913043478260869, 0.8920863309352518, 0.8928571428571429, 0.8936170212765957, 0.8943661971830986, 0.8951048951048951, 0.8958333333333334, 0.896551724137931, 0.8972602739726028, 0.8979591836734694, 0.8986486486486487, 0.8993288590604027, 0.9, 0.9006622516556292, 0.9013157894736842, 0.9019607843137256, 0.9025974025974026, 0.9032258064516128, 0.903846153846154, 0.9044585987261148, 0.9050632911392406, 0.9056603773584906, 0.90625, 0.906832298136646, 0.9074074074074074, 0.9079754601226994, 0.9085365853658536, 0.9030303030303032, 0.8975903614457831, 0.8922155688622755, 0.8928571428571429, 0.893491124260355, 0.888235294117647, 0.8830409356725146, 0.8837209302325582, 0.884393063583815, 0.8793103448275862, 0.88, 0.8806818181818182, 0.8757062146892656, 0.8764044943820225, 0.8770949720670391, 0.8777777777777778, 0.8784530386740331, 0.8791208791208791, 0.8797814207650273, 0.8804347826086957, 0.8810810810810811, 0.8817204301075269, 0.8823529411764706, 0.8829787234042553, 0.8835978835978836, 0.8842105263157894, 0.8795811518324608, 0.8802083333333334, 0.8808290155440415, 0.8814432989690721, 0.882051282051282, 0.8826530612244898, 0.883248730964467, 0.8838383838383839, 0.8844221105527639, 0.885, 0.8855721393034826, 0.8861386138613861, 0.8866995073891626, 0.8872549019607843, 0.8878048780487805, 0.8883495145631068, 0.8888888888888888, 0.8846153846153846, 0.8851674641148325, 0.8809523809523809, 0.8767772511848341, 0.8773584905660378, 0.8779342723004695, 0.8738317757009346, 0.8744186046511628, 0.875, 0.8755760368663594, 0.8761467889908257, 0.8767123287671232, 0.8772727272727273, 0.8778280542986425, 0.8783783783783784, 0.8789237668161435, 0.8794642857142857, 0.88, 0.8805309734513275, 0.8810572687224669, 0.881578947368421, 0.8777292576419214, 0.8739130434782608, 0.8744588744588745, 0.875, 0.8755364806866953, 0.8760683760683761, 0.8765957446808511, 0.8771186440677966, 0.8776371308016878, 0.8781512605042017, 0.8744769874476988, 0.875, 0.8755186721991701, 0.8760330578512396, 0.8765432098765432, 0.8770491803278688, 0.8775510204081632, 0.8739837398373984, 0.8744939271255061, 0.875, 0.8755020080321285, 0.872, 0.8725099601593626, 0.8690476190476191, 0.8695652173913043, 0.8661417322834646, 0.8666666666666667, 0.8671875, 0.8638132295719845, 0.8604651162790697, 0.861003861003861, 0.8576923076923076, 0.8582375478927203, 0.8587786259541985, 0.8593155893536122, 0.8598484848484849, 0.8603773584905661, 0.8609022556390977, 0.8614232209737828, 0.8619402985074627, 0.862453531598513, 0.8629629629629629, 0.8597785977859779, 0.8566176470588235, 0.8571428571428571, 0.8576642335766423, 0.8581818181818182, 0.8586956521739131, 0.8592057761732852, 0.8597122302158273, 0.8566308243727598, 0.8535714285714285, 0.8540925266903915, 0.8546099290780141, 0.8551236749116607, 0.852112676056338, 0.8526315789473684, 0.8531468531468531, 0.8501742160278746, 0.8472222222222222, 0.8477508650519031, 0.8482758620689655, 0.8487972508591065, 0.8493150684931506, 0.8498293515358362, 0.8503401360544217, 0.8508474576271187, 0.8513513513513513, 0.8518518518518519, 0.8489932885906041, 0.8461538461538461, 0.8466666666666667, 0.8471760797342193, 0.847682119205298, 0.8481848184818482, 0.8486842105263158, 0.8491803278688524, 0.8496732026143791, 0.8501628664495114, 0.8506493506493507, 0.8511326860841424, 0.8516129032258064, 0.8520900321543409, 0.8493589743589743, 0.8498402555910544, 0.8503184713375797, 0.8507936507936508, 0.8481012658227848, 0.8485804416403786, 0.8490566037735849, 0.8495297805642633, 0.85, 0.8504672897196262, 0.8509316770186336, 0.8513931888544891, 0.8518518518518519, 0.8523076923076923, 0.852760736196319, 0.8532110091743119, 0.8536585365853658, 0.8541033434650456, 0.8545454545454545, 0.8549848942598187, 0.8524096385542169, 0.8528528528528528, 0.8532934131736527, 0.8537313432835821, 0.8511904761904762, 0.8516320474777448, 0.8520710059171598, 0.8525073746312685, 0.8529411764705882, 0.8533724340175953, 0.8538011695906432, 0.8542274052478134, 0.8546511627906976, 0.855072463768116, 0.8554913294797688, 0.8559077809798271, 0.8563218390804598, 0.8567335243553008, 0.8571428571428571, 0.8575498575498576, 0.8579545454545454, 0.8583569405099151, 0.8587570621468926, 0.8591549295774648, 0.8567415730337079, 0.8543417366946778, 0.8519553072625698, 0.8523676880222841, 0.85, 0.850415512465374, 0.850828729281768, 0.8512396694214877, 0.8516483516483516, 0.852054794520548, 0.8524590163934426, 0.8528610354223434, 0.8532608695652174, 0.8536585365853658, 0.8540540540540541, 0.8544474393530997, 0.8548387096774194, 0.8552278820375335, 0.8556149732620321, 0.856, 0.8563829787234043, 0.8567639257294429, 0.8571428571428571, 0.8575197889182058, 0.8552631578947368, 0.8556430446194225, 0.856020942408377, 0.856396866840731, 0.8567708333333334, 0.8571428571428571, 0.8549222797927462, 0.8527131782945736, 0.8530927835051546, 0.8534704370179949, 0.8538461538461538, 0.8542199488491049, 0.8545918367346939, 0.8549618320610687, 0.8553299492385786, 0.8556962025316456, 0.8535353535353535, 0.853904282115869, 0.8542713567839196, 0.8546365914786967, 0.855, 0.8553615960099751, 0.8532338308457711, 0.8535980148883374, 0.8514851485148515, 0.8518518518518519, 0.8497536945812808, 0.8501228501228502, 0.8504901960784313, 0.8508557457212714, 0.8512195121951219, 0.851581508515815, 0.8495145631067961, 0.8498789346246973, 0.8502415458937198, 0.8481927710843373, 0.8485576923076923, 0.8489208633093526, 0.8492822966507177, 0.8496420047732697, 0.8476190476190476, 0.8479809976247031, 0.8483412322274881, 0.8463356973995272, 0.8443396226415094, 0.8447058823529412, 0.8450704225352113, 0.8430913348946136, 0.8434579439252337, 0.8414918414918415, 0.8418604651162791, 0.839907192575406, 0.8402777777777778, 0.8406466512702079, 0.8410138248847926, 0.8390804597701149, 0.8394495412844036, 0.8398169336384439, 0.8401826484018264, 0.8405466970387244, 0.8409090909090909, 0.8412698412698413, 0.8416289592760181, 0.8419864559819413, 0.8423423423423423, 0.8426966292134831, 0.8430493273542601, 0.843400447427293, 0.84375, 0.844097995545657, 0.8422222222222222, 0.8425720620842572, 0.8429203539823009, 0.8432671081677704, 0.8436123348017621, 0.8417582417582418, 0.8421052631578947, 0.8424507658643327, 0.8406113537117904, 0.840958605664488, 0.8391304347826087, 0.8394793926247288, 0.8376623376623377, 0.838012958963283, 0.8383620689655172, 0.8387096774193549, 0.8390557939914163, 0.8394004282655246, 0.8397435897435898, 0.8400852878464818, 0.8404255319148937, 0.8407643312101911, 0.8411016949152542, 0.8414376321353065, 0.8417721518987342, 0.8421052631578947, 0.8403361344537815, 0.8385744234800838, 0.8389121338912134, 0.8392484342379958, 0.8375, 0.8357588357588358, 0.8360995850622407, 0.8364389233954451, 0.8367768595041323, 0.8371134020618557, 0.8374485596707819, 0.837782340862423, 0.8360655737704918, 0.83640081799591, 0.8346938775510204, 0.835030549898167, 0.8353658536585366, 0.8356997971602435, 0.8360323886639676, 0.8363636363636363, 0.8366935483870968, 0.8370221327967807, 0.8373493975903614, 0.8376753507014028, 0.836, 0.8363273453093812, 0.8366533864541833, 0.8369781312127237, 0.8373015873015873, 0.8356435643564356, 0.8359683794466403, 0.8362919132149902, 0.8366141732283464, 0.8349705304518664, 0.8352941176470589, 0.8356164383561644, 0.8359375, 0.8362573099415205, 0.8346303501945526, 0.8349514563106796, 0.8352713178294574, 0.8355899419729207, 0.8359073359073359, 0.8362235067437379, 0.8365384615384616, 0.836852207293666, 0.8371647509578544, 0.8374760994263862, 0.8377862595419847, 0.8380952380952381, 0.8384030418250951, 0.8387096774193549, 0.8390151515151515, 0.8393194706994329, 0.8377358490566038, 0.8380414312617702, 0.8383458646616542, 0.8386491557223265, 0.8389513108614233, 0.8392523364485981, 0.8395522388059702, 0.839851024208566, 0.8401486988847584, 0.8404452690166976, 0.8407407407407408, 0.8410351201478743, 0.8413284132841329, 0.8416206261510129, 0.8419117647058824, 0.8403669724770643, 0.8406593406593407, 0.8409506398537477, 0.8412408759124088, 0.8397085610200364, 0.84, 0.8402903811252269, 0.8405797101449275, 0.840867992766727, 0.8411552346570397, 0.8414414414414414, 0.841726618705036, 0.8420107719928187, 0.8422939068100358, 0.8425760286225402, 0.8428571428571429, 0.8431372549019608, 0.8434163701067615, 0.8436944937833037, 0.8439716312056738, 0.8442477876106195, 0.8445229681978799, 0.8447971781305115, 0.8450704225352113, 0.8453427065026362, 0.8456140350877193, 0.8441330998248686, 0.8426573426573427, 0.8429319371727748, 0.8432055749128919, 0.8434782608695652, 0.8420138888888888, 0.8422876949740035, 0.842560553633218, 0.842832469775475, 0.843103448275862, 0.8433734939759037, 0.8436426116838488, 0.8439108061749572, 0.8441780821917808, 0.8444444444444444, 0.8447098976109215, 0.8449744463373083, 0.8452380952380952, 0.8455008488964346, 0.8440677966101695, 0.8443316412859561, 0.8445945945945946, 0.8448566610455311, 0.8451178451178452, 0.8453781512605042, 0.8439597315436241, 0.8442211055276382, 0.842809364548495, 0.8430717863105175, 0.8433333333333334, 0.8435940099833611, 0.8438538205980066, 0.8441127694859039, 0.8443708609271523, 0.8446280991735537, 0.8448844884488449, 0.8451400329489291, 0.8453947368421053, 0.8456486042692939, 0.8459016393442623, 0.8461538461538461, 0.8464052287581699, 0.8466557911908646, 0.8469055374592834, 0.8471544715447155, 0.8457792207792207, 0.8460291734197731, 0.8462783171521036, 0.8465266558966075, 0.8467741935483871, 0.8470209339774557, 0.8472668810289389, 0.8475120385232745, 0.8477564102564102, 0.848, 0.8466453674121406, 0.8452950558213717, 0.8455414012738853, 0.8457869634340223, 0.846031746031746, 0.8462757527733756, 0.8465189873417721, 0.8467614533965245, 0.8470031545741324, 0.8472440944881889, 0.8474842767295597, 0.8461538461538461, 0.8463949843260188, 0.8466353677621283, 0.846875, 0.8455538221528861, 0.8457943925233645, 0.8460342146189735, 0.8462732919254659, 0.8465116279069768, 0.846749226006192, 0.8469860896445132, 0.8472222222222222, 0.847457627118644, 0.8476923076923077, 0.847926267281106, 0.848159509202454, 0.8468606431852986, 0.845565749235474, 0.8458015267175573, 0.8460365853658537, 0.8462709284627092, 0.8449848024316109, 0.8452200303490136, 0.8454545454545455, 0.8441754916792739, 0.8444108761329305, 0.8446455505279035, 0.8448795180722891, 0.8451127819548873, 0.8453453453453453, 0.8455772113943029, 0.8458083832335329, 0.8460388639760837, 0.8462686567164179, 0.8464977645305514, 0.8467261904761905, 0.8469539375928677, 0.8471810089020771, 0.8474074074074074, 0.8461538461538461, 0.844903988183161, 0.8451327433628318, 0.8438880706921944, 0.8441176470588235, 0.8443465491923642, 0.844574780058651, 0.8448023426061494, 0.8450292397660819, 0.8437956204379562, 0.8440233236151603, 0.8442503639010189, 0.8444767441860465, 0.8432510885341074, 0.8434782608695652, 0.8422575976845152, 0.8424855491329479, 0.8427128427128427, 0.8414985590778098, 0.841726618705036, 0.8419540229885057, 0.8421807747489239, 0.8424068767908309, 0.8426323319027181, 0.8428571428571429, 0.8430813124108416, 0.8433048433048433, 0.8421052631578947, 0.8423295454545454, 0.8425531914893617, 0.8427762039660056, 0.8415841584158416, 0.8418079096045198, 0.842031029619182, 0.8422535211267606, 0.8424753867791842, 0.8426966292134831, 0.8429172510518934, 0.8417366946778712, 0.8419580419580419, 0.8421787709497207, 0.8423988842398884, 0.8426183844011143, 0.8428372739916551, 0.8430555555555556, 0.8432732316227461, 0.8434903047091413, 0.8437067773167358, 0.8439226519337016, 0.8441379310344828, 0.8443526170798898, 0.8431911966987621, 0.8434065934065934, 0.8436213991769548, 0.8438356164383561, 0.8440492476060192, 0.8442622950819673, 0.844474761255116, 0.8446866485013624, 0.8448979591836735, 0.84375, 0.8426051560379919, 0.8414634146341463, 0.8403247631935047, 0.8405405405405405, 0.8394062078272605, 0.839622641509434, 0.8398384925975774, 0.8387096774193549, 0.8389261744966443, 0.839142091152815, 0.8393574297188755, 0.839572192513369, 0.8397863818424566, 0.84, 0.8402130492676432, 0.8404255319148937, 0.8406374501992032, 0.8408488063660478, 0.8410596026490066, 0.83994708994709, 0.8401585204755614, 0.8390501319261213, 0.839262187088274, 0.8394736842105263, 0.8396846254927727, 0.8398950131233596, 0.8401048492791612, 0.8403141361256544, 0.8405228758169935, 0.8407310704960835, 0.8409387222946545, 0.8411458333333334, 0.8413524057217165, 0.8415584415584415, 0.8417639429312581, 0.8419689119170984, 0.8421733505821475, 0.8423772609819121, 0.8425806451612903, 0.8427835051546392, 0.842985842985843, 0.8431876606683805, 0.8433889602053916, 0.8435897435897436, 0.8437900128040973, 0.8439897698209718, 0.8441890166028098, 0.8443877551020408, 0.8445859872611465, 0.8447837150127226, 0.8449809402795425, 0.8451776649746193, 0.844106463878327, 0.8443037974683544, 0.8445006321112516, 0.8446969696969697, 0.8448928121059268, 0.845088161209068, 0.8452830188679246, 0.8442211055276382, 0.8444165621079046, 0.8446115288220551, 0.8448060075093867, 0.845, 0.8451935081148564, 0.8453865336658354, 0.8455790784557908, 0.845771144278607, 0.8459627329192546, 0.8461538461538461, 0.8463444857496902, 0.8465346534653465, 0.8467243510506799, 0.8469135802469135, 0.8471023427866831, 0.8472906403940886, 0.8474784747847478, 0.8476658476658476, 0.8478527607361963, 0.8468137254901961, 0.847001223990208, 0.8471882640586798, 0.8473748473748474, 0.8475609756097561, 0.8477466504263094, 0.8467153284671532, 0.8469015795868773, 0.8470873786407767, 0.8472727272727273, 0.847457627118644, 0.8476420798065296, 0.8478260869565217, 0.8480096501809409, 0.8481927710843373, 0.8471720818291215, 0.8473557692307693, 0.8463385354141657, 0.8453237410071942, 0.8455089820359282, 0.8456937799043063, 0.8446833930704899, 0.8436754176610979, 0.8438617401668653, 0.844047619047619, 0.8442330558858502, 0.8444180522565321, 0.8446026097271648, 0.8447867298578199, 0.8449704142011835, 0.8451536643026005, 0.8453364817001181, 0.8455188679245284, 0.8457008244994111, 0.8458823529411764, 0.8460634547591069, 0.846244131455399, 0.8464243845252052, 0.8466042154566745, 0.8467836257309942, 0.8469626168224299, 0.8471411901983664, 0.8473193473193473, 0.8474970896391153, 0.8476744186046512, 0.8478513356562137, 0.8480278422273781, 0.8482039397450754, 0.8483796296296297, 0.8485549132947977, 0.8475750577367206, 0.8477508650519031, 0.8467741935483871, 0.8469505178365938, 0.8471264367816091, 0.8473019517795637, 0.8474770642201835, 0.847651775486827, 0.8478260869565217, 0.848, 0.8470319634703196, 0.8472063854047891, 0.8473804100227791, 0.8475540386803185, 0.8477272727272728, 0.8479001135073779, 0.8480725623582767, 0.8482446206115515, 0.8484162895927602, 0.848587570621469, 0.8487584650112867, 0.8489289740698985, 0.8490990990990991, 0.8492688413948256, 0.849438202247191, 0.8496071829405163, 0.8497757847533632, 0.8499440089585666, 0.8501118568232662, 0.8502793296089386, 0.8504464285714286, 0.850613154960981, 0.8507795100222717, 0.8509454949944383, 0.8511111111111112, 0.8501664816870145, 0.8503325942350333, 0.8493909191583611, 0.8495575221238938, 0.8497237569060774, 0.8487858719646799, 0.8489525909592062, 0.8480176211453745, 0.847084708470847, 0.8461538461538461, 0.8463227222832053, 0.8464912280701754, 0.8466593647316539, 0.8457330415754923, 0.8459016393442623, 0.8460698689956332, 0.8462377317339149, 0.8464052287581699, 0.8465723612622416, 0.8467391304347827, 0.8469055374592834, 0.8470715835140998, 0.847237269772481, 0.8474025974025974, 0.8464864864864865, 0.8466522678185745, 0.8468176914778857, 0.8469827586206896, 0.8460710441334769, 0.8451612903225807, 0.8453276047261009, 0.8444206008583691, 0.8445873526259379, 0.8447537473233405, 0.8449197860962567, 0.844017094017094, 0.8441835645677694, 0.8443496801705757, 0.8445154419595314, 0.8446808510638298, 0.8437832093517534, 0.8439490445859873, 0.8441145281018028, 0.8442796610169492, 0.8444444444444444, 0.8446088794926004, 0.8447729672650475, 0.8449367088607594, 0.845100105374078, 0.8452631578947368, 0.8443743427970557, 0.8445378151260504, 0.844700944386149, 0.8448637316561844, 0.8450261780104712, 0.8451882845188284, 0.845350052246604, 0.8455114822546973, 0.8456725755995829, 0.8458333333333333, 0.8459937565036421, 0.8451143451143451, 0.8452751817237798, 0.8454356846473029, 0.8455958549222798, 0.8457556935817805, 0.8459152016546019, 0.8450413223140496, 0.8452012383900929, 0.845360824742268, 0.8455200823892894, 0.845679012345679, 0.8458376156217883, 0.8459958932238193, 0.8461538461538461, 0.8463114754098361, 0.8454452405322416, 0.84560327198364, 0.8457609805924413, 0.8459183673469388, 0.8460754332313966, 0.8462321792260692, 0.8463886063072228, 0.8465447154471545, 0.8456852791878172, 0.845841784989858, 0.8449848024316109, 0.8451417004048583, 0.8452982810920121, 0.8454545454545455, 0.8456104944500504, 0.8457661290322581, 0.8459214501510574, 0.8460764587525151, 0.8462311557788945, 0.8453815261044176, 0.8455366098294884, 0.845691382765531, 0.8458458458458459, 0.846, 0.8461538461538461, 0.846307385229541, 0.8464606181455633, 0.8466135458167331, 0.8467661691542289, 0.8469184890656064, 0.8470705064548163, 0.8472222222222222, 0.8473736372646185, 0.8465346534653465, 0.8466864490603363, 0.8468379446640316, 0.8469891411648569, 0.8471400394477318, 0.8472906403940886, 0.8474409448818898, 0.8475909537856441, 0.8477406679764243, 0.8478900883218842, 0.8470588235294118, 0.8462291870714985, 0.8463796477495108, 0.8465298142717498, 0.8466796875, 0.8468292682926829, 0.8469785575048733, 0.8461538461538461, 0.8463035019455253, 0.8464528668610302, 0.8466019417475729, 0.8467507274490785, 0.8468992248062015, 0.8470474346563408, 0.8462282398452611, 0.8463768115942029, 0.8465250965250966, 0.8466730954676953, 0.8458574181117534, 0.8460057747834456, 0.8461538461538461, 0.8463016330451489, 0.8464491362763915, 0.8465963566634708, 0.8467432950191571, 0.84688995215311, 0.847036328871893, 0.8471824259789876, 0.8473282442748091, 0.8465204957102002, 0.8466666666666667, 0.8468125594671742, 0.846958174904943, 0.8471035137701804, 0.8472485768500949, 0.8473933649289099, 0.8475378787878788, 0.847682119205298, 0.8478260869565217, 0.8479697828139755, 0.8481132075471698, 0.8482563619227145, 0.8483992467043314, 0.8485418626528692, 0.8486842105263158, 0.8488262910798122, 0.848968105065666, 0.8491096532333646, 0.849250936329588, 0.8493919550982226, 0.8485981308411215, 0.8487394957983193, 0.8488805970149254, 0.8490214352283317, 0.8491620111731844, 0.8493023255813954, 0.8494423791821561, 0.8495821727019499, 0.849721706864564, 0.8498609823911029, 0.85, 0.8501387604070305, 0.8502772643253235, 0.850415512465374, 0.8505535055350554, 0.8506912442396314, 0.850828729281768, 0.8509659613615456, 0.8511029411764706, 0.8512396694214877, 0.8513761467889909, 0.851512373968836, 0.8516483516483516, 0.8517840805123513, 0.8519195612431444, 0.852054794520548, 0.8521897810218978, 0.8523245214220602, 0.8524590163934426, 0.8525932666060054, 0.8527272727272728, 0.8528610354223434, 0.852994555353902, 0.8531278331822303, 0.8532608695652174, 0.8524886877828054, 0.8526220614828209, 0.8518518518518519, 0.851985559566787, 0.8521190261496844, 0.8522522522522522, 0.8523852385238524, 0.8525179856115108, 0.8526504941599281, 0.8527827648114902, 0.852017937219731, 0.8521505376344086, 0.8522829006266786, 0.8524150268336315, 0.8525469168900804, 0.8526785714285714, 0.8519179304192686, 0.8520499108734403, 0.8521816562778273, 0.8523131672597865, 0.8524444444444444, 0.8525754884547069, 0.8527062999112689, 0.8528368794326241, 0.8529672276350753, 0.8530973451327434, 0.8532272325375774, 0.8533568904593639, 0.853486319505737, 0.8536155202821869, 0.8537444933920705, 0.8538732394366197, 0.8540017590149517, 0.8541300527240774, 0.8542581211589113, 0.8543859649122807, 0.8545135845749343, 0.8546409807355516, 0.8547681539807525, 0.8548951048951049, 0.8541484716157205, 0.8542757417102966, 0.8535309503051438, 0.8527874564459931, 0.8520452567449956, 0.851304347826087]\n",
    "accuracy_cumulative_nn_50_50_ml400_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.9166666666666666, 0.9230769230769231, 0.9285714285714286, 0.9333333333333333, 0.9375, 0.9411764705882353, 0.9444444444444444, 0.9473684210526315, 0.95, 0.9523809523809523, 0.9545454545454546, 0.9565217391304348, 0.9583333333333334, 0.96, 0.9615384615384616, 0.9629629629629629, 0.9642857142857143, 0.9655172413793104, 0.9666666666666667, 0.967741935483871, 0.96875, 0.9696969696969697, 0.9705882352941176, 0.9714285714285714, 0.9722222222222222, 0.972972972972973, 0.9736842105263158, 0.9743589743589743, 0.975, 0.975609756097561, 0.9761904761904762, 0.9767441860465116, 0.9772727272727273, 0.9777777777777777, 0.9782608695652174, 0.9787234042553191, 0.9791666666666666, 0.9795918367346939, 0.98, 0.9803921568627451, 0.9807692307692307, 0.9811320754716981, 0.9814814814814815, 0.9818181818181818, 0.9821428571428571, 0.9824561403508771, 0.9827586206896551, 0.9830508474576272, 0.9666666666666667, 0.9672131147540983, 0.967741935483871, 0.9682539682539683, 0.953125, 0.9538461538461539, 0.9545454545454546, 0.9402985074626866, 0.9411764705882353, 0.927536231884058, 0.9285714285714286, 0.9295774647887324, 0.9305555555555556, 0.9315068493150684, 0.9324324324324325, 0.9333333333333333, 0.9342105263157895, 0.935064935064935, 0.9358974358974359, 0.9367088607594937, 0.9375, 0.9382716049382716, 0.9390243902439024, 0.9397590361445783, 0.9404761904761905, 0.9411764705882353, 0.9418604651162791, 0.9425287356321839, 0.9431818181818182, 0.9438202247191011, 0.9444444444444444, 0.945054945054945, 0.9456521739130435, 0.946236559139785, 0.9468085106382979, 0.9473684210526315, 0.9479166666666666, 0.9484536082474226, 0.9489795918367347, 0.9494949494949495, 0.95, 0.9405940594059405, 0.9411764705882353, 0.941747572815534, 0.9423076923076923, 0.9428571428571428, 0.9433962264150944, 0.9439252336448598, 0.9444444444444444, 0.944954128440367, 0.9454545454545454, 0.9459459459459459, 0.9464285714285714, 0.9469026548672567, 0.9473684210526315, 0.9478260869565217, 0.9482758620689655, 0.9487179487179487, 0.9491525423728814, 0.9495798319327731, 0.95, 0.9504132231404959, 0.9508196721311475, 0.9512195121951219, 0.9516129032258065, 0.952, 0.9523809523809523, 0.952755905511811, 0.953125, 0.9534883720930233, 0.9538461538461539, 0.9541984732824428, 0.9545454545454546, 0.9548872180451128, 0.9552238805970149, 0.9481481481481482, 0.9485294117647058, 0.948905109489051, 0.9492753623188406, 0.9496402877697842, 0.95, 0.950354609929078, 0.9507042253521126, 0.951048951048951, 0.9513888888888888, 0.9517241379310345, 0.952054794520548, 0.9523809523809523, 0.9527027027027027, 0.9530201342281879, 0.9533333333333334, 0.9536423841059603, 0.9473684210526315, 0.9477124183006536, 0.948051948051948, 0.9483870967741935, 0.9487179487179487, 0.9490445859872612, 0.9493670886075949, 0.949685534591195, 0.94375, 0.9440993788819876, 0.9444444444444444, 0.9447852760736196, 0.9451219512195121, 0.9454545454545454, 0.9457831325301205, 0.9461077844311377, 0.9464285714285714, 0.9467455621301775, 0.9470588235294117, 0.9473684210526315, 0.9476744186046512, 0.9421965317919075, 0.9425287356321839, 0.9428571428571428, 0.9431818181818182, 0.943502824858757, 0.9438202247191011, 0.9441340782122905, 0.9444444444444444, 0.9447513812154696, 0.945054945054945, 0.9453551912568307, 0.9456521739130435, 0.9459459459459459, 0.946236559139785, 0.946524064171123, 0.9468085106382979, 0.9470899470899471, 0.9421052631578948, 0.9424083769633508, 0.9427083333333334, 0.9430051813471503, 0.9432989690721649, 0.9435897435897436, 0.9438775510204082, 0.9441624365482234, 0.9444444444444444, 0.9447236180904522, 0.945, 0.945273631840796, 0.9455445544554455, 0.9458128078817734, 0.946078431372549, 0.9463414634146341, 0.9466019417475728, 0.9468599033816425, 0.9471153846153846, 0.9473684210526315, 0.9428571428571428, 0.943127962085308, 0.9433962264150944, 0.9436619718309859, 0.9439252336448598, 0.9441860465116279, 0.9444444444444444, 0.9447004608294931, 0.944954128440367, 0.9452054794520548, 0.9454545454545454, 0.9457013574660633, 0.9459459459459459, 0.9461883408071748, 0.9464285714285714, 0.9466666666666667, 0.9469026548672567, 0.9427312775330396, 0.9385964912280702, 0.9388646288209607, 0.9391304347826087, 0.9393939393939394, 0.9396551724137931, 0.9399141630901288, 0.9401709401709402, 0.9404255319148936, 0.940677966101695, 0.9409282700421941, 0.9411764705882353, 0.9414225941422594, 0.9375, 0.9377593360995851, 0.9380165289256198, 0.9382716049382716, 0.9385245901639344, 0.9387755102040817, 0.9390243902439024, 0.9352226720647774, 0.9354838709677419, 0.9317269076305221, 0.928, 0.9282868525896414, 0.9285714285714286, 0.9288537549407114, 0.9291338582677166, 0.9294117647058824, 0.9296875, 0.9260700389105059, 0.9263565891472868, 0.9266409266409267, 0.926923076923077, 0.9272030651340997, 0.9274809160305344, 0.9277566539923955, 0.928030303030303, 0.9283018867924528, 0.9285714285714286, 0.9288389513108615, 0.9291044776119403, 0.929368029739777, 0.9296296296296296, 0.9298892988929889, 0.9301470588235294, 0.9304029304029304, 0.9306569343065694, 0.9309090909090909, 0.9311594202898551, 0.9314079422382672, 0.9316546762589928, 0.931899641577061, 0.9321428571428572, 0.9323843416370107, 0.9326241134751773, 0.9328621908127208, 0.9330985915492958, 0.9333333333333333, 0.9335664335664335, 0.9337979094076655, 0.9340277777777778, 0.9342560553633218, 0.9310344827586207, 0.9278350515463918, 0.928082191780822, 0.9283276450511946, 0.9285714285714286, 0.9254237288135593, 0.9256756756756757, 0.9259259259259259, 0.9261744966442953, 0.9264214046822743, 0.9266666666666666, 0.9269102990033222, 0.9271523178807947, 0.9273927392739274, 0.9276315789473685, 0.9278688524590164, 0.9281045751633987, 0.9283387622149837, 0.9285714285714286, 0.9288025889967637, 0.9290322580645162, 0.9292604501607717, 0.9294871794871795, 0.9297124600638977, 0.9299363057324841, 0.9301587301587302, 0.9272151898734177, 0.9274447949526814, 0.9245283018867925, 0.9216300940438872, 0.921875, 0.9190031152647975, 0.9192546583850931, 0.9195046439628483, 0.9197530864197531, 0.92, 0.9202453987730062, 0.9204892966360856, 0.9207317073170732, 0.9209726443768997, 0.9181818181818182, 0.918429003021148, 0.9186746987951807, 0.918918918918919, 0.9191616766467066, 0.9194029850746268, 0.9196428571428571, 0.9198813056379822, 0.9201183431952663, 0.9174041297935103, 0.9176470588235294, 0.9178885630498533, 0.9181286549707602, 0.9183673469387755, 0.9186046511627907, 0.9188405797101449, 0.9190751445086706, 0.9164265129682997, 0.9166666666666666, 0.9169054441260746, 0.9171428571428571, 0.9173789173789174, 0.9176136363636364, 0.9178470254957507, 0.9180790960451978, 0.9183098591549296, 0.9185393258426966, 0.9187675070028011, 0.9189944134078212, 0.9164345403899722, 0.9166666666666666, 0.9168975069252078, 0.9171270718232044, 0.9173553719008265, 0.9175824175824175, 0.9178082191780822, 0.9180327868852459, 0.9182561307901907, 0.9184782608695652, 0.9186991869918699, 0.918918918918919, 0.9191374663072777, 0.9193548387096774, 0.9195710455764075, 0.9197860962566845, 0.92, 0.9202127659574468, 0.9204244031830239, 0.9206349206349206, 0.920844327176781, 0.9210526315789473, 0.9212598425196851, 0.9214659685863874, 0.9216710182767625, 0.921875, 0.9194805194805195, 0.9196891191709845, 0.9198966408268734, 0.9201030927835051, 0.9177377892030848, 0.9179487179487179, 0.9181585677749361, 0.9183673469387755, 0.9185750636132316, 0.9187817258883249, 0.9189873417721519, 0.9191919191919192, 0.9193954659949622, 0.9170854271356784, 0.9172932330827067, 0.9175, 0.9177057356608479, 0.917910447761194, 0.9181141439205955, 0.9183168316831684, 0.9185185185185185, 0.9187192118226601, 0.918918918918919, 0.9191176470588235, 0.9193154034229829, 0.9195121951219513, 0.9197080291970803, 0.9199029126213593, 0.9200968523002422, 0.9202898550724637, 0.9204819277108434, 0.9206730769230769, 0.9184652278177458, 0.9186602870813397, 0.918854415274463, 0.919047619047619, 0.9192399049881235, 0.919431279620853, 0.9196217494089834, 0.9198113207547169, 0.92, 0.92018779342723, 0.9203747072599532, 0.9205607476635514, 0.9207459207459208, 0.9209302325581395, 0.9211136890951276, 0.9212962962962963, 0.9214780600461894, 0.9216589861751152, 0.9218390804597701, 0.9220183486238532, 0.919908466819222, 0.9200913242009132, 0.9202733485193622, 0.9204545454545454, 0.9206349206349206, 0.920814479638009, 0.9209932279909706, 0.9211711711711712, 0.9213483146067416, 0.92152466367713, 0.9217002237136466, 0.921875, 0.9220489977728286, 0.9222222222222223, 0.9201773835920177, 0.9203539823008849, 0.9205298013245033, 0.920704845814978, 0.9208791208791208, 0.9210526315789473, 0.9212253829321663, 0.9213973799126638, 0.9215686274509803, 0.9217391304347826, 0.9219088937093276, 0.922077922077922, 0.9222462203023758, 0.9224137931034483, 0.9225806451612903, 0.9227467811158798, 0.9207708779443254, 0.9209401709401709, 0.9211087420042644, 0.9212765957446809, 0.921443736730361, 0.9216101694915254, 0.9217758985200846, 0.9219409282700421, 0.9221052631578948, 0.9222689075630253, 0.9224318658280922, 0.9225941422594143, 0.9227557411273486, 0.9229166666666667, 0.9230769230769231, 0.9232365145228216, 0.9233954451345756, 0.9235537190082644, 0.9237113402061856, 0.9238683127572016, 0.9240246406570842, 0.9241803278688525, 0.9243353783231084, 0.9244897959183673, 0.924643584521385, 0.9247967479674797, 0.9249492900608519, 0.9251012145748988, 0.9252525252525252, 0.9254032258064516, 0.9255533199195171, 0.9257028112449799, 0.9238476953907816, 0.924]\n",
    "accuracy_cumulative_nn_50_50_ml600_e200_old = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9824561403508771, 0.9827586206896551, 0.9830508474576272, 0.9833333333333333, 0.9836065573770492, 0.9838709677419355, 0.9841269841269841, 0.96875, 0.9692307692307692, 0.9696969696969697, 0.9701492537313433, 0.9705882352941176, 0.9710144927536232, 0.9571428571428572, 0.9577464788732394, 0.9583333333333334, 0.958904109589041, 0.9594594594594594, 0.9466666666666667, 0.9473684210526315, 0.948051948051948, 0.9487179487179487, 0.9493670886075949, 0.95, 0.9506172839506173, 0.9512195121951219, 0.9518072289156626, 0.9523809523809523, 0.9529411764705882, 0.9534883720930233, 0.9540229885057471, 0.9545454545454546, 0.9550561797752809, 0.9555555555555556, 0.9560439560439561, 0.9565217391304348, 0.956989247311828, 0.9574468085106383, 0.9578947368421052, 0.9583333333333334, 0.9587628865979382, 0.9591836734693877, 0.9595959595959596, 0.96, 0.9603960396039604, 0.9607843137254902, 0.9611650485436893, 0.9615384615384616, 0.9619047619047619, 0.9622641509433962, 0.9626168224299065, 0.9629629629629629, 0.963302752293578, 0.9545454545454546, 0.9459459459459459, 0.9464285714285714, 0.9469026548672567, 0.9473684210526315, 0.9478260869565217, 0.9482758620689655, 0.9487179487179487, 0.9491525423728814, 0.9495798319327731, 0.95, 0.9504132231404959, 0.9508196721311475, 0.9512195121951219, 0.9516129032258065, 0.952, 0.9523809523809523, 0.952755905511811, 0.953125, 0.9457364341085271, 0.9461538461538461, 0.9465648854961832, 0.946969696969697, 0.9473684210526315, 0.9477611940298507, 0.9481481481481482, 0.9485294117647058, 0.948905109489051, 0.9420289855072463, 0.9424460431654677, 0.9357142857142857, 0.9361702127659575, 0.9366197183098591, 0.9370629370629371, 0.9375, 0.9379310344827586, 0.9383561643835616, 0.9387755102040817, 0.9391891891891891, 0.9395973154362416, 0.94, 0.9403973509933775, 0.9407894736842105, 0.9411764705882353, 0.9415584415584416, 0.9419354838709677, 0.9423076923076923, 0.9426751592356688, 0.9430379746835443, 0.9433962264150944, 0.94375, 0.9440993788819876, 0.9444444444444444, 0.9447852760736196, 0.9451219512195121, 0.9454545454545454, 0.9457831325301205, 0.9461077844311377, 0.9464285714285714, 0.9467455621301775, 0.9470588235294117, 0.9473684210526315, 0.9476744186046512, 0.9479768786127167, 0.9482758620689655, 0.9485714285714286, 0.9488636363636364, 0.9491525423728814, 0.949438202247191, 0.9497206703910615, 0.95, 0.9502762430939227, 0.9505494505494505, 0.9508196721311475, 0.9510869565217391, 0.9513513513513514, 0.9516129032258065, 0.9518716577540107, 0.9521276595744681, 0.9523809523809523, 0.9526315789473684, 0.9528795811518325, 0.953125, 0.9533678756476683, 0.9536082474226805, 0.9538461538461539, 0.9540816326530612, 0.9543147208121827, 0.9545454545454546, 0.9547738693467337, 0.955, 0.9502487562189055, 0.9504950495049505, 0.9507389162561576, 0.9509803921568627, 0.9512195121951219, 0.9514563106796117, 0.9516908212560387, 0.9519230769230769, 0.9521531100478469, 0.9523809523809523, 0.95260663507109, 0.9528301886792453, 0.9530516431924883, 0.9532710280373832, 0.9488372093023256, 0.9490740740740741, 0.9493087557603687, 0.9495412844036697, 0.9497716894977168, 0.9454545454545454, 0.9457013574660633, 0.9459459459459459, 0.9461883408071748, 0.9464285714285714, 0.9466666666666667, 0.9469026548672567, 0.947136563876652, 0.9473684210526315, 0.9475982532751092, 0.9478260869565217, 0.9437229437229437, 0.9439655172413793, 0.944206008583691, 0.9444444444444444, 0.9446808510638298, 0.9449152542372882, 0.9451476793248945, 0.9453781512605042, 0.9414225941422594, 0.9375, 0.9377593360995851, 0.9380165289256198, 0.9382716049382716, 0.9385245901639344, 0.9387755102040817, 0.9390243902439024, 0.9392712550607287, 0.9395161290322581, 0.9397590361445783, 0.94, 0.9402390438247012, 0.9404761904761905, 0.9407114624505929, 0.9409448818897638, 0.9411764705882353, 0.94140625, 0.9416342412451362, 0.9418604651162791, 0.9420849420849421, 0.9423076923076923, 0.9425287356321839, 0.9427480916030534, 0.9429657794676806, 0.9431818181818182, 0.9433962264150944, 0.9398496240601504, 0.9400749063670412, 0.9402985074626866, 0.9405204460966543, 0.937037037037037, 0.9372693726937269, 0.9375, 0.9377289377289377, 0.9379562043795621, 0.9381818181818182, 0.9384057971014492, 0.9386281588447654, 0.9388489208633094, 0.9390681003584229, 0.9392857142857143, 0.9395017793594306, 0.9397163120567376, 0.9399293286219081, 0.9401408450704225, 0.9403508771929825, 0.9405594405594405, 0.9407665505226481, 0.9409722222222222, 0.9411764705882353, 0.9413793103448276, 0.9415807560137457, 0.9417808219178082, 0.9419795221843004, 0.9421768707482994, 0.9423728813559322, 0.9425675675675675, 0.9427609427609428, 0.9429530201342282, 0.9431438127090301, 0.9433333333333334, 0.9435215946843853, 0.9437086092715232, 0.9438943894389439, 0.944078947368421, 0.9442622950819672, 0.9444444444444444, 0.9413680781758957, 0.9415584415584416, 0.941747572815534, 0.9419354838709677, 0.9421221864951769, 0.9423076923076923, 0.9424920127795527, 0.9426751592356688, 0.9428571428571428, 0.9430379746835443, 0.943217665615142, 0.9433962264150944, 0.9435736677115988, 0.94375, 0.9439252336448598, 0.9440993788819876, 0.9442724458204335, 0.9444444444444444, 0.9446153846153846, 0.9447852760736196, 0.944954128440367, 0.9451219512195121, 0.9452887537993921, 0.9454545454545454, 0.945619335347432, 0.9457831325301205, 0.9459459459459459, 0.9461077844311377, 0.9462686567164179, 0.9464285714285714, 0.9465875370919882, 0.9467455621301775, 0.9469026548672567, 0.9470588235294117, 0.9472140762463344, 0.9473684210526315, 0.9475218658892128, 0.9476744186046512, 0.9478260869565217, 0.9479768786127167, 0.9481268011527377, 0.9482758620689655, 0.9484240687679083, 0.9485714285714286, 0.9458689458689459, 0.9431818181818182, 0.943342776203966, 0.943502824858757, 0.9436619718309859, 0.9438202247191011, 0.9439775910364145, 0.9441340782122905, 0.9442896935933147, 0.9416666666666667, 0.9390581717451524, 0.9392265193370166, 0.9393939393939394, 0.9395604395604396, 0.9397260273972603, 0.9398907103825137, 0.9400544959128065, 0.9402173913043478, 0.940379403794038, 0.9405405405405406, 0.9407008086253369, 0.9408602150537635, 0.9410187667560321, 0.9411764705882353, 0.9413333333333334, 0.9414893617021277, 0.9416445623342176, 0.9417989417989417, 0.941952506596306, 0.9421052631578948, 0.9422572178477691, 0.9424083769633508, 0.9425587467362925, 0.9427083333333334, 0.9428571428571428, 0.9430051813471503, 0.9431524547803618, 0.9432989690721649, 0.9434447300771208, 0.9435897435897436, 0.9437340153452686, 0.9438775510204082, 0.9440203562340967, 0.9441624365482234, 0.9443037974683545, 0.9444444444444444, 0.9445843828715366, 0.9447236180904522, 0.9448621553884712, 0.945, 0.942643391521197, 0.9427860696517413, 0.9429280397022333, 0.943069306930693, 0.9432098765432099, 0.9433497536945813, 0.9434889434889435, 0.9411764705882353, 0.941320293398533, 0.9414634146341463, 0.9416058394160584, 0.941747572815534, 0.9418886198547215, 0.9420289855072463, 0.9421686746987952, 0.9423076923076923, 0.9424460431654677, 0.9425837320574163, 0.9427207637231504, 0.9428571428571428, 0.9429928741092637, 0.943127962085308, 0.9432624113475178, 0.9433962264150944, 0.9435294117647058, 0.9436619718309859, 0.9437939110070258, 0.9439252336448598, 0.9440559440559441, 0.9418604651162791, 0.9419953596287703, 0.9421296296296297, 0.9422632794457275, 0.9423963133640553, 0.9425287356321839, 0.9426605504587156, 0.9427917620137299, 0.9429223744292238, 0.9430523917995444, 0.9431818181818182, 0.9433106575963719, 0.9434389140271493, 0.9435665914221218, 0.9436936936936937, 0.9438202247191011, 0.9439461883408071, 0.9440715883668904, 0.9441964285714286, 0.9443207126948775, 0.9444444444444444, 0.9445676274944568, 0.9446902654867256, 0.9448123620309051, 0.9449339207048458, 0.945054945054945, 0.9451754385964912, 0.9452954048140044, 0.9454148471615721, 0.9455337690631809, 0.9434782608695652, 0.9436008676789588, 0.9437229437229437, 0.9438444924406048, 0.9439655172413793, 0.9440860215053763, 0.944206008583691, 0.9443254817987152, 0.9444444444444444, 0.9445628997867804, 0.9446808510638298, 0.9426751592356688, 0.9427966101694916, 0.9408033826638478, 0.9409282700421941, 0.9389473684210526, 0.9390756302521008, 0.939203354297694, 0.9393305439330544, 0.9373695198329853, 0.9354166666666667, 0.9334719334719335, 0.9336099585062241, 0.9337474120082816, 0.9338842975206612, 0.934020618556701, 0.934156378600823, 0.9342915811088296, 0.9344262295081968, 0.934560327198364, 0.9326530612244898, 0.9327902240325866, 0.9329268292682927, 0.9330628803245437, 0.9331983805668016, 0.9333333333333333, 0.9334677419354839, 0.9336016096579477, 0.9337349397590361, 0.9338677354709419, 0.934]\n",
    "accuracy_cumulative_nn_50_50_ml600_e200 = [1.0, 1.0, 0.6666666666666666, 0.75, 0.8, 0.8333333333333334, 0.8571428571428571, 0.875, 0.8888888888888888, 0.9, 0.9090909090909091, 0.9166666666666666, 0.9230769230769231, 0.9285714285714286, 0.9333333333333333, 0.9375, 0.9411764705882353, 0.9444444444444444, 0.9473684210526315, 0.95, 0.9047619047619048, 0.9090909090909091, 0.9130434782608695, 0.9166666666666666, 0.92, 0.8846153846153846, 0.8888888888888888, 0.8928571428571429, 0.896551724137931, 0.9, 0.9032258064516129, 0.90625, 0.9090909090909091, 0.9117647058823529, 0.9142857142857143, 0.9166666666666666, 0.918918918918919, 0.9210526315789473, 0.9230769230769231, 0.925, 0.926829268292683, 0.9285714285714286, 0.9302325581395349, 0.9318181818181818, 0.9333333333333333, 0.9347826086956522, 0.9361702127659575, 0.9375, 0.9183673469387755, 0.92, 0.9215686274509803, 0.9230769230769231, 0.9245283018867925, 0.9259259259259259, 0.9272727272727272, 0.9285714285714286, 0.9298245614035088, 0.9310344827586207, 0.9322033898305084, 0.9333333333333333, 0.9180327868852459, 0.9193548387096774, 0.9047619047619048, 0.90625, 0.9076923076923077, 0.9090909090909091, 0.8955223880597015, 0.8970588235294118, 0.8985507246376812, 0.9, 0.9014084507042254, 0.9027777777777778, 0.9041095890410958, 0.9054054054054054, 0.9066666666666666, 0.9078947368421053, 0.8961038961038961, 0.8974358974358975, 0.8987341772151899, 0.9, 0.9012345679012346, 0.9024390243902439, 0.9036144578313253, 0.9047619047619048, 0.9058823529411765, 0.9069767441860465, 0.9080459770114943, 0.9090909090909091, 0.9101123595505618, 0.9111111111111111, 0.9010989010989011, 0.9021739130434783, 0.9032258064516129, 0.9042553191489362, 0.9052631578947369, 0.90625, 0.9072164948453608, 0.9081632653061225, 0.9090909090909091, 0.91, 0.9108910891089109, 0.9117647058823529, 0.912621359223301, 0.9134615384615384, 0.9142857142857143, 0.9150943396226415, 0.9158878504672897, 0.9166666666666666, 0.9174311926605505, 0.9181818181818182, 0.918918918918919, 0.9196428571428571, 0.9203539823008849, 0.9210526315789473, 0.9217391304347826, 0.9224137931034483, 0.9230769230769231, 0.923728813559322, 0.9243697478991597, 0.925, 0.9256198347107438, 0.9262295081967213, 0.926829268292683, 0.9193548387096774, 0.92, 0.9206349206349206, 0.9212598425196851, 0.921875, 0.9224806201550387, 0.9230769230769231, 0.9236641221374046, 0.9242424242424242, 0.924812030075188, 0.917910447761194, 0.9185185185185185, 0.9117647058823529, 0.9124087591240876, 0.9057971014492754, 0.9064748201438849, 0.9, 0.900709219858156, 0.9014084507042254, 0.9020979020979021, 0.9027777777777778, 0.903448275862069, 0.9041095890410958, 0.9047619047619048, 0.9054054054054054, 0.8993288590604027, 0.9, 0.9006622516556292, 0.9013157894736842, 0.9019607843137255, 0.9025974025974026, 0.9032258064516129, 0.9038461538461539, 0.9044585987261147, 0.9050632911392406, 0.9056603773584906, 0.90625, 0.906832298136646, 0.9012345679012346, 0.901840490797546, 0.9024390243902439, 0.9030303030303031, 0.8975903614457831, 0.8982035928143712, 0.8988095238095238, 0.8994082840236687, 0.9, 0.9005847953216374, 0.9011627906976745, 0.9017341040462428, 0.9022988505747126, 0.9028571428571428, 0.9034090909090909, 0.8983050847457628, 0.8932584269662921, 0.8938547486033519, 0.8944444444444445, 0.8950276243093923, 0.8956043956043956, 0.8961748633879781, 0.8967391304347826, 0.8972972972972973, 0.8978494623655914, 0.8983957219251337, 0.898936170212766, 0.8994708994708994, 0.9, 0.900523560209424, 0.9010416666666666, 0.9015544041450777, 0.9020618556701031, 0.9025641025641026, 0.9030612244897959, 0.9035532994923858, 0.9040404040404041, 0.9045226130653267, 0.905, 0.9054726368159204, 0.905940594059406, 0.9064039408866995, 0.9068627450980392, 0.9073170731707317, 0.9029126213592233, 0.9033816425120773, 0.9038461538461539, 0.9043062200956937, 0.9047619047619048, 0.9052132701421801, 0.9056603773584906, 0.9061032863849765, 0.9065420560747663, 0.9069767441860465, 0.9074074074074074, 0.9078341013824884, 0.908256880733945, 0.908675799086758, 0.9090909090909091, 0.9095022624434389, 0.9099099099099099, 0.9103139013452914, 0.9107142857142857, 0.9111111111111111, 0.911504424778761, 0.9118942731277533, 0.9122807017543859, 0.9126637554585153, 0.908695652173913, 0.9090909090909091, 0.9094827586206896, 0.9055793991416309, 0.9017094017094017, 0.902127659574468, 0.902542372881356, 0.9029535864978903, 0.9033613445378151, 0.9037656903765691, 0.9041666666666667, 0.9045643153526971, 0.9049586776859504, 0.9012345679012346, 0.8975409836065574, 0.8979591836734694, 0.8983739837398373, 0.8987854251012146, 0.8991935483870968, 0.8995983935742972, 0.9, 0.900398406374502, 0.9007936507936508, 0.9011857707509882, 0.9015748031496063, 0.9019607843137255, 0.90234375, 0.9027237354085603, 0.9031007751937985, 0.9034749034749034, 0.9038461538461539, 0.9042145593869731, 0.9045801526717557, 0.9049429657794676, 0.9053030303030303, 0.9056603773584906, 0.9060150375939849, 0.9063670411985019, 0.9067164179104478, 0.9070631970260223, 0.9074074074074074, 0.9077490774907749, 0.9080882352941176, 0.9084249084249084, 0.9087591240875912, 0.9090909090909091, 0.9094202898550725, 0.9097472924187726, 0.9100719424460432, 0.910394265232975, 0.9107142857142857, 0.9110320284697508, 0.9113475177304965, 0.911660777385159, 0.9119718309859155, 0.9122807017543859, 0.9125874125874126, 0.9128919860627178, 0.9131944444444444, 0.9134948096885813, 0.9137931034482759, 0.9140893470790378, 0.9143835616438356, 0.9146757679180887, 0.9149659863945578, 0.9152542372881356, 0.9121621621621622, 0.9124579124579124, 0.912751677852349, 0.9130434782608695, 0.9133333333333333, 0.9136212624584718, 0.9139072847682119, 0.9141914191419142, 0.9144736842105263, 0.9147540983606557, 0.9150326797385621, 0.9153094462540716, 0.9155844155844156, 0.9158576051779935, 0.9161290322580645, 0.9163987138263665, 0.9166666666666666, 0.9169329073482428, 0.9171974522292994, 0.9174603174603174, 0.9177215189873418, 0.917981072555205, 0.9150943396226415, 0.9153605015673981, 0.915625, 0.9158878504672897, 0.9161490683229814, 0.9164086687306502, 0.9166666666666666, 0.916923076923077, 0.9171779141104295, 0.9174311926605505, 0.9176829268292683, 0.9179331306990881, 0.9181818181818182, 0.918429003021148, 0.9186746987951807, 0.9159159159159159, 0.9161676646706587, 0.9164179104477612, 0.9136904761904762, 0.913946587537092, 0.9142011834319527, 0.9144542772861357, 0.9147058823529411, 0.9120234604105572, 0.9122807017543859, 0.9125364431486881, 0.9127906976744186, 0.9130434782608695, 0.9132947976878613, 0.9135446685878963, 0.9137931034482759, 0.9140401146131805, 0.9142857142857143, 0.9145299145299145, 0.9147727272727273, 0.9150141643059491, 0.9152542372881356, 0.9154929577464789, 0.9157303370786517, 0.9159663865546218, 0.9162011173184358, 0.9136490250696379, 0.9138888888888889, 0.9141274238227147, 0.914364640883978, 0.9146005509641874, 0.9148351648351648, 0.915068493150685, 0.9153005464480874, 0.9155313351498637, 0.9157608695652174, 0.9159891598915989, 0.9162162162162162, 0.9164420485175202, 0.9166666666666666, 0.9168900804289544, 0.9171122994652406, 0.9173333333333333, 0.9175531914893617, 0.9177718832891246, 0.917989417989418, 0.9182058047493403, 0.9184210526315789, 0.9186351706036745, 0.918848167539267, 0.9190600522193212, 0.9192708333333334, 0.9194805194805195, 0.9196891191709845, 0.9198966408268734, 0.9201030927835051, 0.9203084832904884, 0.9205128205128205, 0.9207161125319693, 0.9209183673469388, 0.9211195928753181, 0.9213197969543148, 0.9215189873417722, 0.9217171717171717, 0.9219143576826196, 0.9221105527638191, 0.9223057644110275, 0.9225, 0.9226932668329177, 0.9228855721393034, 0.9230769230769231, 0.9232673267326733, 0.9234567901234568, 0.9236453201970444, 0.9213759213759214, 0.9215686274509803, 0.921760391198044, 0.9219512195121952, 0.9221411192214112, 0.9223300970873787, 0.9225181598062954, 0.9227053140096618, 0.9228915662650602, 0.9230769230769231, 0.920863309352518, 0.9210526315789473, 0.9212410501193318, 0.9214285714285714, 0.9216152019002375, 0.9218009478672986, 0.9219858156028369, 0.9221698113207547, 0.9223529411764706, 0.9225352112676056, 0.9227166276346604, 0.9228971962616822, 0.9230769230769231, 0.9232558139534883, 0.9234338747099768, 0.9236111111111112, 0.9237875288683602, 0.923963133640553, 0.9241379310344827, 0.9243119266055045, 0.9244851258581236, 0.9223744292237442, 0.9225512528473804, 0.9227272727272727, 0.9229024943310657, 0.9230769230769231, 0.9232505643340858, 0.9234234234234234, 0.9235955056179775, 0.9237668161434978, 0.9239373601789709, 0.9241071428571429, 0.9242761692650334, 0.9244444444444444, 0.9246119733924612, 0.9247787610619469, 0.9227373068432672, 0.9229074889867841, 0.9230769230769231, 0.9232456140350878, 0.9234135667396062, 0.9235807860262009, 0.9237472766884531, 0.9239130434782609, 0.9240780911062907, 0.9242424242424242, 0.9244060475161987, 0.9245689655172413, 0.9247311827956989, 0.924892703862661, 0.9229122055674518, 0.9230769230769231, 0.9232409381663113, 0.9234042553191489, 0.9235668789808917, 0.923728813559322, 0.9238900634249472, 0.9240506329113924, 0.9242105263157895, 0.9243697478991597, 0.9245283018867925, 0.9246861924686193, 0.9248434237995825, 0.925, 0.9251559251559252, 0.9253112033195021, 0.9254658385093167, 0.9256198347107438, 0.9257731958762887, 0.9259259259259259, 0.9260780287474333, 0.9241803278688525, 0.9243353783231084, 0.9244897959183673, 0.924643584521385, 0.9247967479674797, 0.9249492900608519, 0.9251012145748988, 0.9252525252525252, 0.9254032258064516, 0.9235412474849095, 0.9236947791164659, 0.9238476953907816, 0.924]\n",
    "accuracy_cumulative_nn_50_50_ml800_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9444444444444444, 0.9473684210526315, 0.95, 0.9523809523809523, 0.9545454545454546, 0.9565217391304348, 0.9583333333333334, 0.96, 0.9615384615384616, 0.9629629629629629, 0.9642857142857143, 0.9655172413793104, 0.9666666666666667, 0.967741935483871, 0.96875, 0.9696969696969697, 0.9705882352941176, 0.9714285714285714, 0.9722222222222222, 0.972972972972973, 0.9736842105263158, 0.9743589743589743, 0.975, 0.975609756097561, 0.9761904761904762, 0.9767441860465116, 0.9772727272727273, 0.9777777777777777, 0.9782608695652174, 0.9787234042553191, 0.9791666666666666, 0.9795918367346939, 0.98, 0.9803921568627451, 0.9807692307692307, 0.9811320754716981, 0.9814814814814815, 0.9818181818181818, 0.9642857142857143, 0.9649122807017544, 0.9655172413793104, 0.9661016949152542, 0.9666666666666667, 0.9672131147540983, 0.967741935483871, 0.9682539682539683, 0.96875, 0.9692307692307692, 0.9696969696969697, 0.9701492537313433, 0.9705882352941176, 0.9710144927536232, 0.9571428571428572, 0.9577464788732394, 0.9583333333333334, 0.958904109589041, 0.9594594594594594, 0.96, 0.9605263157894737, 0.961038961038961, 0.9615384615384616, 0.9620253164556962, 0.9625, 0.9629629629629629, 0.9634146341463414, 0.963855421686747, 0.9642857142857143, 0.9647058823529412, 0.9651162790697675, 0.9655172413793104, 0.9659090909090909, 0.9550561797752809, 0.9555555555555556, 0.9560439560439561, 0.9565217391304348, 0.956989247311828, 0.9574468085106383, 0.9578947368421052, 0.9583333333333334, 0.9587628865979382, 0.9591836734693877, 0.9595959595959596, 0.96, 0.9603960396039604, 0.9509803921568627, 0.9514563106796117, 0.9519230769230769, 0.9523809523809523, 0.9528301886792453, 0.9532710280373832, 0.9537037037037037, 0.9541284403669725, 0.9545454545454546, 0.954954954954955, 0.9553571428571429, 0.9557522123893806, 0.956140350877193, 0.9565217391304348, 0.9568965517241379, 0.9572649572649573, 0.9576271186440678, 0.957983193277311, 0.9583333333333334, 0.9586776859504132, 0.9590163934426229, 0.959349593495935, 0.9596774193548387, 0.96, 0.9603174603174603, 0.9606299212598425, 0.9609375, 0.9612403100775194, 0.9615384615384616, 0.9618320610687023, 0.9621212121212122, 0.9624060150375939, 0.9626865671641791, 0.9629629629629629, 0.9632352941176471, 0.9635036496350365, 0.9565217391304348, 0.9568345323741008, 0.9571428571428572, 0.9574468085106383, 0.9577464788732394, 0.958041958041958, 0.9583333333333334, 0.9586206896551724, 0.958904109589041, 0.9591836734693877, 0.9594594594594594, 0.959731543624161, 0.96, 0.9602649006622517, 0.9605263157894737, 0.9607843137254902, 0.961038961038961, 0.9612903225806452, 0.9551282051282052, 0.9554140127388535, 0.9556962025316456, 0.9559748427672956, 0.95, 0.9503105590062112, 0.9506172839506173, 0.950920245398773, 0.9512195121951219, 0.9515151515151515, 0.9518072289156626, 0.9520958083832335, 0.9523809523809523, 0.9526627218934911, 0.9529411764705882, 0.9532163742690059, 0.9534883720930233, 0.953757225433526, 0.9540229885057471, 0.9542857142857143, 0.9545454545454546, 0.9548022598870056, 0.9550561797752809, 0.9553072625698324, 0.9555555555555556, 0.9558011049723757, 0.9560439560439561, 0.9562841530054644, 0.9565217391304348, 0.9567567567567568, 0.956989247311828, 0.9572192513368984, 0.9574468085106383, 0.9576719576719577, 0.9578947368421052, 0.9581151832460733, 0.9583333333333334, 0.9585492227979274, 0.9587628865979382, 0.958974358974359, 0.9591836734693877, 0.9593908629441624, 0.9595959595959596, 0.9597989949748744, 0.96, 0.9601990049751243, 0.9603960396039604, 0.9605911330049262, 0.9607843137254902, 0.9609756097560975, 0.9611650485436893, 0.961352657004831, 0.9615384615384616, 0.9617224880382775, 0.9571428571428572, 0.957345971563981, 0.9575471698113207, 0.9577464788732394, 0.9579439252336449, 0.958139534883721, 0.9583333333333334, 0.9585253456221198, 0.9587155963302753, 0.958904109589041, 0.9590909090909091, 0.9592760180995475, 0.9594594594594594, 0.9596412556053812, 0.9598214285714286, 0.96, 0.9601769911504425, 0.960352422907489, 0.9605263157894737, 0.9606986899563319, 0.9565217391304348, 0.9567099567099567, 0.9568965517241379, 0.9570815450643777, 0.9572649572649573, 0.9574468085106383, 0.9576271186440678, 0.9578059071729957, 0.957983193277311, 0.9581589958158996, 0.9583333333333334, 0.9585062240663901, 0.9586776859504132, 0.9588477366255144, 0.9590163934426229, 0.9591836734693877, 0.959349593495935, 0.9595141700404858, 0.9596774193548387, 0.9598393574297188, 0.96, 0.9601593625498008, 0.9603174603174603, 0.9604743083003953, 0.9606299212598425, 0.9607843137254902, 0.9609375, 0.9610894941634242, 0.9612403100775194, 0.9613899613899614, 0.9615384615384616, 0.9616858237547893, 0.9618320610687023, 0.9619771863117871, 0.9621212121212122, 0.9622641509433962, 0.9624060150375939, 0.9625468164794008, 0.9626865671641791, 0.9628252788104089, 0.9629629629629629, 0.9630996309963099, 0.9632352941176471, 0.9633699633699634, 0.9635036496350365, 0.9636363636363636, 0.9637681159420289, 0.9638989169675091, 0.960431654676259, 0.9605734767025089, 0.9607142857142857, 0.9608540925266904, 0.9609929078014184, 0.9611307420494699, 0.9612676056338029, 0.9614035087719298, 0.9615384615384616, 0.9616724738675958, 0.9618055555555556, 0.9619377162629758, 0.9620689655172414, 0.9621993127147767, 0.9623287671232876, 0.962457337883959, 0.9625850340136054, 0.9627118644067797, 0.9628378378378378, 0.9629629629629629, 0.9630872483221476, 0.9632107023411371, 0.9633333333333334, 0.9634551495016611, 0.9635761589403974, 0.9636963696369637, 0.9638157894736842, 0.9639344262295082, 0.9640522875816994, 0.9641693811074918, 0.9642857142857143, 0.9644012944983819, 0.964516129032258, 0.9646302250803859, 0.9647435897435898, 0.9648562300319489, 0.964968152866242, 0.9650793650793651, 0.9651898734177216, 0.9652996845425867, 0.9654088050314465, 0.9655172413793104, 0.965625, 0.9657320872274143, 0.9658385093167702, 0.9659442724458205, 0.9660493827160493, 0.9661538461538461, 0.9662576687116564, 0.9663608562691132, 0.9664634146341463, 0.9665653495440729, 0.9636363636363636, 0.9637462235649547, 0.963855421686747, 0.963963963963964, 0.9610778443113772, 0.9611940298507463, 0.9613095238095238, 0.9614243323442137, 0.9615384615384616, 0.9616519174041298, 0.961764705882353, 0.9618768328445748, 0.9619883040935673, 0.9620991253644315, 0.9622093023255814, 0.9623188405797102, 0.9624277456647399, 0.962536023054755, 0.9626436781609196, 0.9627507163323782, 0.9628571428571429, 0.9629629629629629, 0.9630681818181818, 0.9631728045325779, 0.963276836158192, 0.9633802816901409, 0.9634831460674157, 0.9635854341736695, 0.9636871508379888, 0.9637883008356546, 0.9638888888888889, 0.96398891966759, 0.9640883977900553, 0.9641873278236914, 0.9642857142857143, 0.9643835616438357, 0.9644808743169399, 0.9645776566757494, 0.9646739130434783, 0.964769647696477, 0.9648648648648649, 0.9649595687331537, 0.9650537634408602, 0.9651474530831099, 0.9652406417112299, 0.9653333333333334, 0.9654255319148937, 0.9655172413793104, 0.9656084656084656, 0.9656992084432717, 0.9657894736842105, 0.9658792650918635, 0.9659685863874345, 0.9660574412532638, 0.9661458333333334, 0.9662337662337662, 0.966321243523316, 0.9664082687338501, 0.9664948453608248, 0.9665809768637532, 0.9666666666666667, 0.9667519181585678, 0.9668367346938775, 0.9669211195928753, 0.9670050761421319, 0.9670886075949368, 0.9671717171717171, 0.9672544080604534, 0.9673366834170855, 0.9674185463659147, 0.9675, 0.9675810473815462, 0.9676616915422885, 0.967741935483871, 0.9678217821782178, 0.9679012345679012, 0.9679802955665024, 0.9680589680589681, 0.9681372549019608, 0.9682151589242054, 0.9682926829268292, 0.9683698296836983, 0.9684466019417476, 0.9685230024213075, 0.9685990338164251, 0.9686746987951808, 0.96875, 0.9688249400479616, 0.9688995215311005, 0.9689737470167065, 0.969047619047619, 0.9691211401425178, 0.9691943127962085, 0.9692671394799054, 0.9693396226415094, 0.9694117647058823, 0.9694835680751174, 0.9695550351288056, 0.969626168224299, 0.9696969696969697, 0.9697674418604652, 0.9698375870069605, 0.9699074074074074, 0.9699769053117783, 0.9700460829493087, 0.9701149425287356, 0.9701834862385321, 0.9702517162471396, 0.9703196347031964, 0.9681093394077449, 0.9681818181818181, 0.9682539682539683, 0.9683257918552036, 0.9661399548532731, 0.9662162162162162, 0.9662921348314607, 0.9663677130044843, 0.9642058165548099, 0.9642857142857143, 0.9643652561247216, 0.9644444444444444, 0.9645232815964523, 0.9646017699115044, 0.9646799116997793, 0.9647577092511013, 0.9648351648351648, 0.9649122807017544, 0.9649890590809628, 0.9650655021834061, 0.9651416122004357, 0.9652173913043478, 0.96529284164859, 0.9653679653679653, 0.9654427645788337, 0.9655172413793104, 0.9655913978494624, 0.9656652360515021, 0.9635974304068522, 0.9636752136752137, 0.9637526652452025, 0.9638297872340426, 0.9639065817409767, 0.9639830508474576, 0.9640591966173362, 0.9641350210970464, 0.9642105263157895, 0.9642857142857143, 0.9643605870020965, 0.9644351464435147, 0.964509394572025, 0.9645833333333333, 0.9646569646569647, 0.9647302904564315, 0.9648033126293996, 0.9648760330578512, 0.9649484536082474, 0.9650205761316872, 0.9650924024640657, 0.9651639344262295, 0.9652351738241309, 0.9653061224489796, 0.9653767820773931, 0.9654471544715447, 0.9655172413793104, 0.9655870445344129, 0.9656565656565657, 0.9657258064516129, 0.96579476861167, 0.9658634538152611, 0.9659318637274549, 0.966]\n",
    "accuracy_cumulative_nn_50_50_ml1000_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 0.9666666666666667, 0.967741935483871, 0.96875, 0.9696969696969697, 0.9705882352941176, 0.9714285714285714, 0.9722222222222222, 0.972972972972973, 0.9736842105263158, 0.9743589743589743, 0.975, 0.9512195121951219, 0.9523809523809523, 0.9534883720930233, 0.9545454545454546, 0.9555555555555556, 0.9565217391304348, 0.9574468085106383, 0.9583333333333334, 0.9591836734693877, 0.96, 0.9607843137254902, 0.9615384615384616, 0.9622641509433962, 0.9629629629629629, 0.9636363636363636, 0.9642857142857143, 0.9649122807017544, 0.9655172413793104, 0.9661016949152542, 0.9666666666666667, 0.9672131147540983, 0.9516129032258065, 0.9523809523809523, 0.953125, 0.9538461538461539, 0.9545454545454546, 0.9552238805970149, 0.9558823529411765, 0.9420289855072463, 0.9428571428571428, 0.9436619718309859, 0.9444444444444444, 0.9452054794520548, 0.9459459459459459, 0.9466666666666667, 0.9473684210526315, 0.948051948051948, 0.9487179487179487, 0.9493670886075949, 0.95, 0.9506172839506173, 0.9512195121951219, 0.9518072289156626, 0.9523809523809523, 0.9529411764705882, 0.9418604651162791, 0.9425287356321839, 0.9431818181818182, 0.9438202247191011, 0.9444444444444444, 0.945054945054945, 0.9456521739130435, 0.9354838709677419, 0.9361702127659575, 0.9368421052631579, 0.9375, 0.9381443298969072, 0.9387755102040817, 0.9393939393939394, 0.94, 0.9405940594059405, 0.9411764705882353, 0.941747572815534, 0.9423076923076923, 0.9428571428571428, 0.9433962264150944, 0.9439252336448598, 0.9444444444444444, 0.944954128440367, 0.9454545454545454, 0.9459459459459459, 0.9464285714285714, 0.9469026548672567, 0.9473684210526315, 0.9478260869565217, 0.9482758620689655, 0.9487179487179487, 0.9491525423728814, 0.9411764705882353, 0.9416666666666667, 0.9421487603305785, 0.9426229508196722, 0.943089430894309, 0.9435483870967742, 0.944, 0.9444444444444444, 0.9448818897637795, 0.9453125, 0.9457364341085271, 0.9461538461538461, 0.9465648854961832, 0.946969696969697, 0.9473684210526315, 0.9477611940298507, 0.9481481481481482, 0.9485294117647058, 0.948905109489051, 0.9492753623188406, 0.9496402877697842, 0.95, 0.950354609929078, 0.9507042253521126, 0.951048951048951, 0.9513888888888888, 0.9517241379310345, 0.952054794520548, 0.9523809523809523, 0.9527027027027027, 0.9530201342281879, 0.9533333333333334, 0.9536423841059603, 0.9539473684210527, 0.954248366013072, 0.9545454545454546, 0.9548387096774194, 0.9551282051282052, 0.9554140127388535, 0.9556962025316456, 0.9559748427672956, 0.95625, 0.9565217391304348, 0.9567901234567902, 0.9570552147239264, 0.9573170731707317, 0.9575757575757575, 0.9578313253012049, 0.9580838323353293, 0.9583333333333334, 0.9585798816568047, 0.9588235294117647, 0.9590643274853801, 0.9593023255813954, 0.9595375722543352, 0.9597701149425287, 0.96, 0.9602272727272727, 0.96045197740113, 0.9606741573033708, 0.9608938547486033, 0.9611111111111111, 0.9613259668508287, 0.9615384615384616, 0.9617486338797814, 0.9619565217391305, 0.9621621621621622, 0.9623655913978495, 0.9625668449197861, 0.9627659574468085, 0.9629629629629629, 0.9631578947368421, 0.9633507853403142, 0.9635416666666666, 0.9637305699481865, 0.9639175257731959, 0.9641025641025641, 0.9642857142857143, 0.9644670050761421, 0.9646464646464646, 0.964824120603015, 0.965, 0.9651741293532339, 0.9653465346534653, 0.9655172413793104, 0.9656862745098039, 0.9609756097560975, 0.9611650485436893, 0.961352657004831, 0.9615384615384616, 0.9617224880382775, 0.9619047619047619, 0.9620853080568721, 0.9622641509433962, 0.9624413145539906, 0.9626168224299065, 0.9627906976744186, 0.9629629629629629, 0.9631336405529954, 0.963302752293578, 0.9634703196347032, 0.9636363636363636, 0.9638009049773756, 0.963963963963964, 0.9641255605381166, 0.9642857142857143, 0.9644444444444444, 0.9601769911504425, 0.960352422907489, 0.9605263157894737, 0.9606986899563319, 0.9608695652173913, 0.961038961038961, 0.9612068965517241, 0.9613733905579399, 0.9615384615384616, 0.9617021276595744, 0.961864406779661, 0.9620253164556962, 0.9621848739495799, 0.9623430962343096, 0.9625, 0.9626556016597511, 0.9628099173553719, 0.9629629629629629, 0.9631147540983607, 0.9591836734693877, 0.959349593495935, 0.9595141700404858, 0.9596774193548387, 0.9558232931726908, 0.952, 0.952191235059761, 0.9523809523809523, 0.9525691699604744, 0.952755905511811, 0.9529411764705882, 0.953125, 0.953307392996109, 0.9534883720930233, 0.9498069498069498, 0.95, 0.9501915708812261, 0.950381679389313, 0.9467680608365019, 0.9431818181818182, 0.9433962264150944, 0.943609022556391, 0.9438202247191011, 0.9440298507462687, 0.9442379182156134, 0.9444444444444444, 0.9446494464944649, 0.9411764705882353, 0.9413919413919414, 0.9416058394160584, 0.9418181818181818, 0.9420289855072463, 0.9422382671480144, 0.9424460431654677, 0.9390681003584229, 0.9392857142857143, 0.9395017793594306, 0.9397163120567376, 0.9399293286219081, 0.9366197183098591, 0.9368421052631579, 0.9370629370629371, 0.9372822299651568, 0.9375, 0.9342560553633218, 0.9344827586206896, 0.9347079037800687, 0.934931506849315, 0.931740614334471, 0.9319727891156463, 0.9322033898305084, 0.9324324324324325, 0.9326599326599326, 0.9328859060402684, 0.9331103678929766, 0.9333333333333333, 0.9335548172757475, 0.9337748344370861, 0.933993399339934, 0.9342105263157895, 0.9344262295081968, 0.934640522875817, 0.9315960912052117, 0.9318181818181818, 0.9320388349514563, 0.932258064516129, 0.932475884244373, 0.9326923076923077, 0.9329073482428115, 0.9331210191082803, 0.9333333333333333, 0.9335443037974683, 0.9337539432176656, 0.9339622641509434, 0.9310344827586207, 0.93125, 0.9314641744548287, 0.9316770186335404, 0.9318885448916409, 0.9320987654320988, 0.9323076923076923, 0.9325153374233128, 0.9327217125382263, 0.9329268292682927, 0.9331306990881459, 0.9333333333333333, 0.9335347432024169, 0.9337349397590361, 0.933933933933934, 0.9341317365269461, 0.9343283582089552, 0.9345238095238095, 0.9347181008902077, 0.9349112426035503, 0.9351032448377581, 0.9352941176470588, 0.9325513196480938, 0.9327485380116959, 0.9300291545189504, 0.9273255813953488, 0.927536231884058, 0.9277456647398844, 0.9279538904899135, 0.9281609195402298, 0.9283667621776505, 0.9257142857142857, 0.9259259259259259, 0.9261363636363636, 0.9263456090651558, 0.9265536723163842, 0.9267605633802817, 0.9269662921348315, 0.9243697478991597, 0.9245810055865922, 0.924791086350975, 0.925, 0.925207756232687, 0.925414364640884, 0.9256198347107438, 0.9258241758241759, 0.9260273972602739, 0.9262295081967213, 0.9264305177111717, 0.9266304347826086, 0.926829268292683, 0.927027027027027, 0.9272237196765498, 0.9247311827956989, 0.9222520107238605, 0.9224598930481284, 0.9226666666666666, 0.9228723404255319, 0.9204244031830239, 0.9206349206349206, 0.920844327176781, 0.9210526315789473, 0.9212598425196851, 0.9214659685863874, 0.9216710182767625, 0.921875, 0.922077922077922, 0.9222797927461139, 0.9224806201550387, 0.9226804123711341, 0.922879177377892, 0.9230769230769231, 0.9232736572890026, 0.923469387755102, 0.9236641221374046, 0.9238578680203046, 0.9240506329113924, 0.9242424242424242, 0.924433249370277, 0.9246231155778895, 0.924812030075188, 0.925, 0.9251870324189526, 0.9253731343283582, 0.9255583126550868, 0.9257425742574258, 0.9259259259259259, 0.9236453201970444, 0.9213759213759214, 0.9215686274509803, 0.921760391198044, 0.9219512195121952, 0.9221411192214112, 0.9223300970873787, 0.9225181598062954, 0.9227053140096618, 0.9228915662650602, 0.9230769230769231, 0.9232613908872902, 0.9234449760765551, 0.9236276849642004, 0.9238095238095239, 0.9239904988123515, 0.9241706161137441, 0.9243498817966903, 0.9245283018867925, 0.9247058823529412, 0.9248826291079812, 0.9250585480093677, 0.9252336448598131, 0.9254079254079254, 0.9255813953488372, 0.925754060324826, 0.9259259259259259, 0.9260969976905312, 0.9262672811059908, 0.9264367816091954, 0.926605504587156, 0.9267734553775744, 0.9269406392694064, 0.9271070615034168, 0.9272727272727272, 0.927437641723356, 0.9276018099547512, 0.927765237020316, 0.9279279279279279, 0.9280898876404494, 0.9282511210762332, 0.9284116331096197, 0.9285714285714286, 0.9287305122494433, 0.9288888888888889, 0.9290465631929047, 0.9292035398230089, 0.9293598233995585, 0.9295154185022027, 0.9296703296703297, 0.9298245614035088, 0.9299781181619255, 0.9301310043668122, 0.9302832244008714, 0.9304347826086956, 0.93058568329718, 0.9307359307359307, 0.9308855291576674, 0.9310344827586207, 0.9311827956989247, 0.9313304721030042, 0.9314775160599572, 0.9316239316239316, 0.9317697228144989, 0.9319148936170213, 0.9320594479830149, 0.9322033898305084, 0.9323467230443975, 0.9324894514767933, 0.9326315789473684, 0.9327731092436975, 0.9329140461215933, 0.9330543933054394, 0.9331941544885177, 0.9333333333333333, 0.9334719334719335, 0.9336099585062241, 0.9337474120082816, 0.9338842975206612, 0.934020618556701, 0.934156378600823, 0.9342915811088296, 0.9344262295081968, 0.934560327198364, 0.9326530612244898, 0.9327902240325866, 0.9329268292682927, 0.9330628803245437, 0.9331983805668016, 0.9333333333333333, 0.9334677419354839, 0.9336016096579477, 0.9337349397590361, 0.9338677354709419, 0.934]\n",
    "accuracy_cumulative_nn_50_50_ml1200_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9090909090909091, 0.9166666666666666, 0.9230769230769231, 0.9285714285714286, 0.9333333333333333, 0.9375, 0.9411764705882353, 0.9444444444444444, 0.9473684210526315, 0.95, 0.9523809523809523, 0.9545454545454546, 0.9565217391304348, 0.9583333333333334, 0.96, 0.9615384615384616, 0.9629629629629629, 0.9642857142857143, 0.9655172413793104, 0.9666666666666667, 0.967741935483871, 0.96875, 0.9696969696969697, 0.9705882352941176, 0.9714285714285714, 0.9722222222222222, 0.972972972972973, 0.9736842105263158, 0.9743589743589743, 0.975, 0.975609756097561, 0.9761904761904762, 0.9767441860465116, 0.9772727272727273, 0.9777777777777777, 0.9782608695652174, 0.9787234042553191, 0.9791666666666666, 0.9795918367346939, 0.98, 0.9803921568627451, 0.9807692307692307, 0.9811320754716981, 0.9814814814814815, 0.9818181818181818, 0.9821428571428571, 0.9824561403508771, 0.9827586206896551, 0.9830508474576272, 0.9833333333333333, 0.9836065573770492, 0.9838709677419355, 0.9841269841269841, 0.984375, 0.9846153846153847, 0.9848484848484849, 0.9850746268656716, 0.9852941176470589, 0.9855072463768116, 0.9857142857142858, 0.9859154929577465, 0.9861111111111112, 0.9863013698630136, 0.9864864864864865, 0.9866666666666667, 0.9868421052631579, 0.987012987012987, 0.9871794871794872, 0.9873417721518988, 0.9875, 0.9876543209876543, 0.9878048780487805, 0.9879518072289156, 0.9880952380952381, 0.9882352941176471, 0.9883720930232558, 0.9885057471264368, 0.9886363636363636, 0.9887640449438202, 0.9888888888888889, 0.989010989010989, 0.9891304347826086, 0.989247311827957, 0.9893617021276596, 0.9894736842105263, 0.9895833333333334, 0.979381443298969, 0.9795918367346939, 0.9797979797979798, 0.98, 0.9801980198019802, 0.9803921568627451, 0.9805825242718447, 0.9807692307692307, 0.9809523809523809, 0.9811320754716981, 0.9813084112149533, 0.9814814814814815, 0.9724770642201835, 0.9727272727272728, 0.972972972972973, 0.9732142857142857, 0.9734513274336283, 0.9736842105263158, 0.9739130434782609, 0.9655172413793104, 0.9658119658119658, 0.9661016949152542, 0.9663865546218487, 0.9666666666666667, 0.9669421487603306, 0.9672131147540983, 0.967479674796748, 0.967741935483871, 0.968, 0.9682539682539683, 0.968503937007874, 0.96875, 0.9612403100775194, 0.9615384615384616, 0.9618320610687023, 0.9621212121212122, 0.9624060150375939, 0.9626865671641791, 0.9629629629629629, 0.9632352941176471, 0.9635036496350365, 0.9637681159420289, 0.9640287769784173, 0.9642857142857143, 0.9574468085106383, 0.9577464788732394, 0.958041958041958, 0.9583333333333334, 0.9586206896551724, 0.958904109589041, 0.9591836734693877, 0.9594594594594594, 0.959731543624161, 0.96, 0.9602649006622517, 0.9605263157894737, 0.9607843137254902, 0.961038961038961, 0.9612903225806452, 0.9615384615384616, 0.9617834394904459, 0.9620253164556962, 0.9622641509433962, 0.9625, 0.9627329192546584, 0.9629629629629629, 0.9631901840490797, 0.9634146341463414, 0.9636363636363636, 0.963855421686747, 0.9640718562874252, 0.9642857142857143, 0.9585798816568047, 0.9588235294117647, 0.9590643274853801, 0.9593023255813954, 0.9595375722543352, 0.9597701149425287, 0.96, 0.9602272727272727, 0.96045197740113, 0.9606741573033708, 0.9608938547486033, 0.9611111111111111, 0.9613259668508287, 0.9615384615384616, 0.9617486338797814, 0.9619565217391305, 0.9621621621621622, 0.9623655913978495, 0.9625668449197861, 0.9627659574468085, 0.9629629629629629, 0.9631578947368421, 0.9633507853403142, 0.9635416666666666, 0.9637305699481865, 0.9639175257731959, 0.9641025641025641, 0.9642857142857143, 0.9644670050761421, 0.9646464646464646, 0.964824120603015, 0.965, 0.9651741293532339, 0.9653465346534653, 0.9655172413793104, 0.9656862745098039, 0.9658536585365853, 0.9660194174757282, 0.966183574879227, 0.9663461538461539, 0.9617224880382775, 0.9619047619047619, 0.9620853080568721, 0.9622641509433962, 0.9624413145539906, 0.9626168224299065, 0.9627906976744186, 0.9629629629629629, 0.9631336405529954, 0.963302752293578, 0.9634703196347032, 0.9636363636363636, 0.9638009049773756, 0.9594594594594594, 0.9596412556053812, 0.9598214285714286, 0.96, 0.9601769911504425, 0.960352422907489, 0.9605263157894737, 0.9606986899563319, 0.9608695652173913, 0.961038961038961, 0.9612068965517241, 0.9613733905579399, 0.9615384615384616, 0.9574468085106383, 0.9576271186440678, 0.9578059071729957, 0.957983193277311, 0.9581589958158996, 0.9583333333333334, 0.9585062240663901, 0.9586776859504132, 0.9588477366255144, 0.9590163934426229, 0.9591836734693877, 0.959349593495935, 0.9595141700404858, 0.9596774193548387, 0.9598393574297188, 0.96, 0.9601593625498008, 0.9603174603174603, 0.9565217391304348, 0.952755905511811, 0.9529411764705882, 0.94921875, 0.9494163424124513, 0.9496124031007752, 0.9498069498069498, 0.95, 0.9501915708812261, 0.9465648854961832, 0.9467680608365019, 0.946969696969697, 0.9471698113207547, 0.9473684210526315, 0.947565543071161, 0.9477611940298507, 0.9479553903345725, 0.9481481481481482, 0.948339483394834, 0.9485294117647058, 0.9487179487179487, 0.948905109489051, 0.9490909090909091, 0.9492753623188406, 0.9494584837545126, 0.9496402877697842, 0.9498207885304659, 0.95, 0.9501779359430605, 0.950354609929078, 0.950530035335689, 0.9507042253521126, 0.9508771929824561, 0.951048951048951, 0.9512195121951219, 0.9513888888888888, 0.9515570934256056, 0.9517241379310345, 0.9518900343642611, 0.952054794520548, 0.9522184300341296, 0.9523809523809523, 0.9525423728813559, 0.9527027027027027, 0.9528619528619529, 0.9530201342281879, 0.9531772575250836, 0.9533333333333334, 0.9534883720930233, 0.9536423841059603, 0.9504950495049505, 0.9506578947368421, 0.9508196721311475, 0.9509803921568627, 0.9511400651465798, 0.9512987012987013, 0.9514563106796117, 0.9516129032258065, 0.9517684887459807, 0.9487179487179487, 0.9488817891373802, 0.945859872611465, 0.946031746031746, 0.9462025316455697, 0.9463722397476341, 0.9465408805031447, 0.9467084639498433, 0.946875, 0.9470404984423676, 0.9472049689440993, 0.9473684210526315, 0.9475308641975309, 0.9476923076923077, 0.9478527607361963, 0.9480122324159022, 0.948170731707317, 0.9483282674772037, 0.9484848484848485, 0.9486404833836858, 0.9487951807228916, 0.948948948948949, 0.9491017964071856, 0.9492537313432836, 0.9494047619047619, 0.9495548961424333, 0.9497041420118343, 0.9498525073746312, 0.95, 0.9501466275659824, 0.9502923976608187, 0.9504373177842566, 0.9505813953488372, 0.9507246376811594, 0.9508670520231214, 0.9510086455331412, 0.9511494252873564, 0.9512893982808023, 0.9514285714285714, 0.9515669515669516, 0.9517045454545454, 0.9518413597733711, 0.9519774011299436, 0.952112676056338, 0.952247191011236, 0.9523809523809523, 0.952513966480447, 0.9526462395543176, 0.9527777777777777, 0.9529085872576177, 0.9530386740331491, 0.953168044077135, 0.9532967032967034, 0.9534246575342465, 0.953551912568306, 0.9536784741144414, 0.9538043478260869, 0.9539295392953929, 0.9540540540540541, 0.954177897574124, 0.9543010752688172, 0.9544235924932976, 0.9545454545454546, 0.9546666666666667, 0.9521276595744681, 0.9522546419098143, 0.9523809523809523, 0.9525065963060686, 0.9526315789473684, 0.952755905511811, 0.9528795811518325, 0.9530026109660574, 0.953125, 0.9532467532467532, 0.9533678756476683, 0.9534883720930233, 0.9536082474226805, 0.9537275064267352, 0.9538461538461539, 0.9514066496163683, 0.951530612244898, 0.9516539440203562, 0.9517766497461929, 0.9518987341772152, 0.952020202020202, 0.9521410579345088, 0.9522613065326633, 0.9523809523809523, 0.9525, 0.9526184538653366, 0.9527363184079602, 0.9528535980148883, 0.9529702970297029, 0.9530864197530864, 0.9532019704433498, 0.9533169533169533, 0.9534313725490197, 0.9535452322738386, 0.9536585365853658, 0.9537712895377128, 0.9538834951456311, 0.9539951573849879, 0.9541062801932367, 0.9542168674698795, 0.9543269230769231, 0.9544364508393285, 0.9545454545454546, 0.954653937947494, 0.9547619047619048, 0.9548693586698337, 0.9549763033175356, 0.9550827423167849, 0.9551886792452831, 0.9552941176470588, 0.9553990610328639, 0.955503512880562, 0.955607476635514, 0.9557109557109557, 0.9558139534883721, 0.9559164733178654, 0.9560185185185185, 0.9561200923787528, 0.956221198156682, 0.9563218390804598, 0.9564220183486238, 0.9565217391304348, 0.95662100456621, 0.9567198177676538, 0.9568181818181818, 0.9569160997732427, 0.9570135746606335, 0.9571106094808126, 0.9572072072072072, 0.9573033707865168, 0.9573991031390134, 0.9574944071588367, 0.9575892857142857, 0.9576837416481069, 0.9555555555555556, 0.9556541019955654, 0.9557522123893806, 0.9558498896247241, 0.9559471365638766, 0.9560439560439561, 0.956140350877193, 0.9562363238512035, 0.9563318777292577, 0.9564270152505446, 0.9565217391304348, 0.9566160520607375, 0.9567099567099567, 0.9568034557235421, 0.9568965517241379, 0.956989247311828, 0.9570815450643777, 0.9571734475374732, 0.9572649572649573, 0.9573560767590619, 0.9574468085106383, 0.9575371549893843, 0.9576271186440678, 0.9577167019027484, 0.9578059071729957, 0.9578947368421052, 0.957983193277311, 0.9580712788259959, 0.9581589958158996, 0.9582463465553236, 0.9583333333333334, 0.9584199584199584, 0.9585062240663901, 0.9585921325051759, 0.9586776859504132, 0.9587628865979382, 0.9588477366255144, 0.9589322381930184, 0.9590163934426229, 0.9570552147239264, 0.9571428571428572, 0.9572301425661914, 0.9573170731707317, 0.9574036511156186, 0.9574898785425101, 0.9575757575757575, 0.9576612903225806, 0.9577464788732394, 0.9578313253012049, 0.9579158316633266, 0.958]\n",
    "accuracy_cumulative_nn_50_50_ml1400_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9900990099009901, 0.9901960784313726, 0.9902912621359223, 0.9903846153846154, 0.9904761904761905, 0.9905660377358491, 0.9906542056074766, 0.9907407407407407, 0.9908256880733946, 0.990909090909091, 0.990990990990991, 0.9910714285714286, 0.9911504424778761, 0.9912280701754386, 0.991304347826087, 0.9913793103448276, 0.9914529914529915, 0.9915254237288136, 0.9915966386554622, 0.9916666666666667, 0.9917355371900827, 0.9918032786885246, 0.991869918699187, 0.9919354838709677, 0.992, 0.9920634920634921, 0.9921259842519685, 0.9921875, 0.9922480620155039, 0.9923076923076923, 0.9923664122137404, 0.9924242424242424, 0.9924812030075187, 0.9850746268656716, 0.9851851851851852, 0.9852941176470589, 0.9854014598540146, 0.9855072463768116, 0.9784172661870504, 0.9785714285714285, 0.9787234042553191, 0.9788732394366197, 0.9790209790209791, 0.9791666666666666, 0.9793103448275862, 0.9794520547945206, 0.9795918367346939, 0.9797297297297297, 0.9798657718120806, 0.98, 0.9801324503311258, 0.9736842105263158, 0.9738562091503268, 0.974025974025974, 0.9741935483870968, 0.9743589743589743, 0.9745222929936306, 0.9746835443037974, 0.9748427672955975, 0.975, 0.9751552795031055, 0.9753086419753086, 0.9754601226993865, 0.975609756097561, 0.9757575757575757, 0.9759036144578314, 0.9760479041916168, 0.9761904761904762, 0.9763313609467456, 0.9764705882352941, 0.9766081871345029, 0.9767441860465116, 0.976878612716763, 0.9770114942528736, 0.9771428571428571, 0.9772727272727273, 0.9774011299435028, 0.9775280898876404, 0.9776536312849162, 0.9777777777777777, 0.9779005524861878, 0.978021978021978, 0.9781420765027322, 0.9782608695652174, 0.9783783783783784, 0.9731182795698925, 0.9732620320855615, 0.973404255319149, 0.9735449735449735, 0.9736842105263158, 0.9738219895287958, 0.9739583333333334, 0.9740932642487047, 0.9742268041237113, 0.9743589743589743, 0.9744897959183674, 0.9746192893401016, 0.9747474747474747, 0.9748743718592965, 0.975, 0.9751243781094527, 0.9752475247524752, 0.9753694581280788, 0.9754901960784313, 0.975609756097561, 0.9757281553398058, 0.9758454106280193, 0.9759615384615384, 0.9760765550239234, 0.9761904761904762, 0.976303317535545, 0.9764150943396226, 0.9765258215962441, 0.9766355140186916, 0.9720930232558139, 0.9675925925925926, 0.967741935483871, 0.9678899082568807, 0.9680365296803652, 0.9681818181818181, 0.9683257918552036, 0.9684684684684685, 0.968609865470852, 0.96875, 0.9688888888888889, 0.9690265486725663, 0.9691629955947136, 0.9692982456140351, 0.9694323144104804, 0.9695652173913043, 0.9696969696969697, 0.9698275862068966, 0.9699570815450643, 0.9700854700854701, 0.9702127659574468, 0.9703389830508474, 0.9704641350210971, 0.9705882352941176, 0.9707112970711297, 0.9708333333333333, 0.970954356846473, 0.9710743801652892, 0.9711934156378601, 0.9713114754098361, 0.9714285714285714, 0.9715447154471545, 0.97165991902834, 0.9717741935483871, 0.9718875502008032, 0.972, 0.9721115537848606, 0.9722222222222222, 0.9723320158102767, 0.9724409448818898, 0.9686274509803922, 0.96875, 0.9688715953307393, 0.9689922480620154, 0.9691119691119691, 0.9692307692307692, 0.9693486590038314, 0.9694656488549618, 0.9695817490494296, 0.9696969696969697, 0.969811320754717, 0.9661654135338346, 0.9662921348314607, 0.9664179104477612, 0.966542750929368, 0.9666666666666667, 0.966789667896679, 0.9669117647058824, 0.967032967032967, 0.9671532846715328, 0.9672727272727273, 0.967391304347826, 0.9675090252707581, 0.9640287769784173, 0.96415770609319, 0.9642857142857143, 0.9644128113879004, 0.9645390070921985, 0.9646643109540636, 0.9647887323943662, 0.9649122807017544, 0.965034965034965, 0.9651567944250871, 0.9652777777777778, 0.9653979238754326, 0.9655172413793104, 0.9656357388316151, 0.9657534246575342, 0.9658703071672355, 0.9659863945578231, 0.9661016949152542, 0.9662162162162162, 0.9663299663299664, 0.9664429530201343, 0.9665551839464883, 0.9666666666666667, 0.9667774086378738, 0.9668874172185431, 0.966996699669967, 0.9671052631578947, 0.9672131147540983, 0.9673202614379085, 0.9641693811074918, 0.9642857142857143, 0.9644012944983819, 0.9612903225806452, 0.9614147909967846, 0.9615384615384616, 0.9616613418530351, 0.9585987261146497, 0.9587301587301588, 0.9588607594936709, 0.9589905362776026, 0.9591194968553459, 0.9592476489028213, 0.959375, 0.9595015576323987, 0.9596273291925466, 0.9597523219814241, 0.9598765432098766, 0.96, 0.9601226993865031, 0.9602446483180428, 0.9603658536585366, 0.9604863221884499, 0.9606060606060606, 0.9607250755287009, 0.9608433734939759, 0.960960960960961, 0.9610778443113772, 0.9611940298507463, 0.9613095238095238, 0.9614243323442137, 0.9615384615384616, 0.9616519174041298, 0.961764705882353, 0.9618768328445748, 0.9619883040935673, 0.9620991253644315, 0.9622093023255814, 0.9623188405797102, 0.9624277456647399, 0.962536023054755, 0.9626436781609196, 0.9627507163323782, 0.9628571428571429, 0.9629629629629629, 0.9630681818181818, 0.9631728045325779, 0.963276836158192, 0.9633802816901409, 0.9634831460674157, 0.9635854341736695, 0.9636871508379888, 0.9637883008356546, 0.9638888888888889, 0.96398891966759, 0.9640883977900553, 0.9641873278236914, 0.9642857142857143, 0.9643835616438357, 0.9644808743169399, 0.9645776566757494, 0.9619565217391305, 0.962059620596206, 0.9621621621621622, 0.9622641509433962, 0.9623655913978495, 0.9624664879356568, 0.9625668449197861, 0.9626666666666667, 0.9627659574468085, 0.9628647214854111, 0.9629629629629629, 0.9630606860158312, 0.9631578947368421, 0.963254593175853, 0.9633507853403142, 0.9634464751958225, 0.9635416666666666, 0.9636363636363636, 0.9637305699481865, 0.9638242894056848, 0.9639175257731959, 0.9640102827763496, 0.9641025641025641, 0.9641943734015346, 0.9642857142857143, 0.9643765903307888, 0.9644670050761421, 0.9645569620253165, 0.9646464646464646, 0.964735516372796, 0.964824120603015, 0.9649122807017544, 0.965, 0.9650872817955112, 0.9651741293532339, 0.9652605459057072, 0.9653465346534653, 0.9654320987654321, 0.9655172413793104, 0.9656019656019657, 0.9656862745098039, 0.9657701711491442, 0.9658536585365853, 0.9659367396593674, 0.9660194174757282, 0.9661016949152542, 0.966183574879227, 0.9662650602409638, 0.9663461538461539, 0.9664268585131894, 0.9665071770334929, 0.9665871121718377, 0.9666666666666667, 0.9667458432304038, 0.966824644549763, 0.966903073286052, 0.9669811320754716, 0.9670588235294117, 0.9671361502347418, 0.9672131147540983, 0.9672897196261683, 0.9673659673659674, 0.9651162790697675, 0.9651972157772621, 0.9652777777777778, 0.9653579676674365, 0.9654377880184332, 0.9655172413793104, 0.9655963302752294, 0.965675057208238, 0.9657534246575342, 0.9658314350797267, 0.9659090909090909, 0.9659863945578231, 0.9660633484162896, 0.9661399548532731, 0.9662162162162162, 0.9662921348314607, 0.9663677130044843, 0.9664429530201343, 0.9665178571428571, 0.9665924276169265, 0.9666666666666667, 0.9667405764966741, 0.9668141592920354, 0.9668874172185431, 0.9669603524229075, 0.967032967032967, 0.9671052631578947, 0.9671772428884027, 0.9672489082969432, 0.9673202614379085, 0.967391304347826, 0.9674620390455532, 0.9675324675324676, 0.9676025917926566, 0.9676724137931034, 0.967741935483871, 0.9678111587982833, 0.9657387580299786, 0.9658119658119658, 0.9658848614072495, 0.9659574468085106, 0.9639065817409767, 0.9639830508474576, 0.9640591966173362, 0.9641350210970464, 0.9642105263157895, 0.9642857142857143, 0.9643605870020965, 0.9644351464435147, 0.964509394572025, 0.9645833333333333, 0.9646569646569647, 0.9647302904564315, 0.9648033126293996, 0.9648760330578512, 0.9628865979381444, 0.9629629629629629, 0.9609856262833676, 0.9590163934426229, 0.9591002044989775, 0.9591836734693877, 0.9592668024439919, 0.959349593495935, 0.9594320486815415, 0.9595141700404858, 0.9595959595959596, 0.9596774193548387, 0.9577464788732394, 0.9578313253012049, 0.9579158316633266, 0.958]\n",
    "accuracy_cumulative_nn_50_50_ml1600_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9090909090909091, 0.9166666666666666, 0.9230769230769231, 0.8571428571428571, 0.8666666666666667, 0.875, 0.8823529411764706, 0.8888888888888888, 0.8947368421052632, 0.9, 0.9047619047619048, 0.9090909090909091, 0.9130434782608695, 0.9166666666666666, 0.88, 0.8846153846153846, 0.8888888888888888, 0.8928571428571429, 0.896551724137931, 0.9, 0.9032258064516129, 0.90625, 0.9090909090909091, 0.9117647058823529, 0.9142857142857143, 0.9166666666666666, 0.918918918918919, 0.9210526315789473, 0.9230769230769231, 0.925, 0.926829268292683, 0.9285714285714286, 0.9302325581395349, 0.9318181818181818, 0.9333333333333333, 0.9347826086956522, 0.9361702127659575, 0.9375, 0.9387755102040817, 0.94, 0.9411764705882353, 0.9423076923076923, 0.9433962264150944, 0.9444444444444444, 0.9454545454545454, 0.9464285714285714, 0.9473684210526315, 0.9482758620689655, 0.9491525423728814, 0.95, 0.9508196721311475, 0.9516129032258065, 0.9523809523809523, 0.953125, 0.9384615384615385, 0.9393939393939394, 0.9402985074626866, 0.9264705882352942, 0.927536231884058, 0.9285714285714286, 0.9295774647887324, 0.9305555555555556, 0.9315068493150684, 0.9324324324324325, 0.9333333333333333, 0.9342105263157895, 0.935064935064935, 0.9358974358974359, 0.9367088607594937, 0.9375, 0.9259259259259259, 0.926829268292683, 0.927710843373494, 0.9285714285714286, 0.9294117647058824, 0.9302325581395349, 0.9310344827586207, 0.9318181818181818, 0.9325842696629213, 0.9333333333333333, 0.9340659340659341, 0.9347826086956522, 0.9354838709677419, 0.925531914893617, 0.9263157894736842, 0.9270833333333334, 0.9278350515463918, 0.9285714285714286, 0.9292929292929293, 0.93, 0.9306930693069307, 0.9313725490196079, 0.9320388349514563, 0.9326923076923077, 0.9333333333333333, 0.9339622641509434, 0.9345794392523364, 0.9351851851851852, 0.926605504587156, 0.9272727272727272, 0.9279279279279279, 0.9285714285714286, 0.9292035398230089, 0.9298245614035088, 0.9217391304347826, 0.9224137931034483, 0.9230769230769231, 0.923728813559322, 0.9159663865546218, 0.9083333333333333, 0.9090909090909091, 0.9098360655737705, 0.9105691056910569, 0.9112903225806451, 0.912, 0.9126984126984127, 0.905511811023622, 0.90625, 0.9069767441860465, 0.9076923076923077, 0.9083969465648855, 0.9090909090909091, 0.9097744360902256, 0.9104477611940298, 0.9111111111111111, 0.9117647058823529, 0.9124087591240876, 0.9130434782608695, 0.9136690647482014, 0.9142857142857143, 0.9148936170212766, 0.9084507042253521, 0.9090909090909091, 0.9027777777777778, 0.903448275862069, 0.9041095890410958, 0.9047619047619048, 0.9054054054054054, 0.9060402684563759, 0.9066666666666666, 0.9072847682119205, 0.9078947368421053, 0.9084967320261438, 0.9090909090909091, 0.9096774193548387, 0.9102564102564102, 0.910828025477707, 0.9113924050632911, 0.9119496855345912, 0.9125, 0.9130434782608695, 0.9074074074074074, 0.9079754601226994, 0.9085365853658537, 0.9090909090909091, 0.9096385542168675, 0.9101796407185628, 0.9107142857142857, 0.9112426035502958, 0.9058823529411765, 0.9064327485380117, 0.9069767441860465, 0.9017341040462428, 0.896551724137931, 0.8971428571428571, 0.8977272727272727, 0.8983050847457628, 0.898876404494382, 0.8994413407821229, 0.9, 0.9005524861878453, 0.9010989010989011, 0.9016393442622951, 0.9021739130434783, 0.9027027027027027, 0.9032258064516129, 0.9037433155080213, 0.9042553191489362, 0.9047619047619048, 0.9052631578947369, 0.9057591623036649, 0.90625, 0.9067357512953368, 0.9020618556701031, 0.9025641025641026, 0.8979591836734694, 0.8984771573604061, 0.898989898989899, 0.8994974874371859, 0.9, 0.900497512437811, 0.900990099009901, 0.9014778325123153, 0.9019607843137255, 0.9024390243902439, 0.9029126213592233, 0.9033816425120773, 0.9038461538461539, 0.9043062200956937, 0.9047619047619048, 0.9052132701421801, 0.9056603773584906, 0.9061032863849765, 0.9065420560747663, 0.9069767441860465, 0.9074074074074074, 0.9078341013824884, 0.908256880733945, 0.908675799086758, 0.9090909090909091, 0.9095022624434389, 0.9099099099099099, 0.9103139013452914, 0.9107142857142857, 0.9111111111111111, 0.911504424778761, 0.9118942731277533, 0.9122807017543859, 0.9126637554585153, 0.9130434782608695, 0.9134199134199135, 0.9137931034482759, 0.9141630901287554, 0.9145299145299145, 0.9148936170212766, 0.9152542372881356, 0.9156118143459916, 0.9159663865546218, 0.9163179916317992, 0.9166666666666666, 0.91701244813278, 0.9173553719008265, 0.9176954732510288, 0.9139344262295082, 0.9102040816326531, 0.9105691056910569, 0.9109311740890689, 0.9112903225806451, 0.9116465863453815, 0.912, 0.9123505976095617, 0.9126984126984127, 0.9130434782608695, 0.9133858267716536, 0.9137254901960784, 0.9140625, 0.914396887159533, 0.9147286821705426, 0.915057915057915, 0.9153846153846154, 0.9157088122605364, 0.916030534351145, 0.9163498098859315, 0.9166666666666666, 0.9169811320754717, 0.9172932330827067, 0.9138576779026217, 0.914179104477612, 0.9144981412639405, 0.9148148148148149, 0.915129151291513, 0.9154411764705882, 0.9120879120879121, 0.9087591240875912, 0.9090909090909091, 0.9094202898550725, 0.9097472924187726, 0.9100719424460432, 0.910394265232975, 0.9107142857142857, 0.9110320284697508, 0.9113475177304965, 0.911660777385159, 0.9119718309859155, 0.9122807017543859, 0.9125874125874126, 0.9128919860627178, 0.9131944444444444, 0.9134948096885813, 0.9137931034482759, 0.9140893470790378, 0.9143835616438356, 0.9146757679180887, 0.9149659863945578, 0.911864406779661, 0.9121621621621622, 0.9124579124579124, 0.912751677852349, 0.9130434782608695, 0.9133333333333333, 0.9136212624584718, 0.9139072847682119, 0.9141914191419142, 0.9144736842105263, 0.9147540983606557, 0.9150326797385621, 0.9153094462540716, 0.9155844155844156, 0.9158576051779935, 0.9161290322580645, 0.9163987138263665, 0.9166666666666666, 0.9169329073482428, 0.9171974522292994, 0.9174603174603174, 0.9145569620253164, 0.9148264984227129, 0.9150943396226415, 0.9153605015673981, 0.915625, 0.9158878504672897, 0.9161490683229814, 0.9164086687306502, 0.9166666666666666, 0.916923076923077, 0.9171779141104295, 0.9143730886850153, 0.9146341463414634, 0.9148936170212766, 0.9151515151515152, 0.9154078549848943, 0.9156626506024096, 0.9159159159159159, 0.9161676646706587, 0.9164179104477612, 0.9166666666666666, 0.9169139465875371, 0.9171597633136095, 0.9174041297935103, 0.9147058823529411, 0.9120234604105572, 0.9122807017543859, 0.9125364431486881, 0.9127906976744186, 0.9130434782608695, 0.9132947976878613, 0.9135446685878963, 0.9137931034482759, 0.9140401146131805, 0.9142857142857143, 0.9145299145299145, 0.9147727272727273, 0.9150141643059491, 0.9152542372881356, 0.9154929577464789, 0.9129213483146067, 0.9131652661064426, 0.9134078212290503, 0.9136490250696379, 0.9138888888888889, 0.9113573407202216, 0.9116022099447514, 0.9118457300275482, 0.9093406593406593, 0.9095890410958904, 0.9098360655737705, 0.9073569482288828, 0.907608695652174, 0.907859078590786, 0.9081081081081082, 0.9083557951482479, 0.9086021505376344, 0.9088471849865952, 0.9090909090909091, 0.9093333333333333, 0.9095744680851063, 0.9098143236074271, 0.91005291005291, 0.9102902374670184, 0.9105263157894737, 0.910761154855643, 0.9109947643979057, 0.9112271540469974, 0.9114583333333334, 0.9116883116883117, 0.9119170984455959, 0.9121447028423773, 0.9123711340206185, 0.9125964010282777, 0.9128205128205128, 0.9130434782608695, 0.9132653061224489, 0.9134860050890585, 0.9137055837563451, 0.9139240506329114, 0.9141414141414141, 0.9143576826196473, 0.914572864321608, 0.9147869674185464, 0.915, 0.9152119700748129, 0.9154228855721394, 0.9156327543424317, 0.9158415841584159, 0.9160493827160494, 0.916256157635468, 0.9164619164619164, 0.9166666666666666, 0.9168704156479217, 0.9170731707317074, 0.9172749391727494, 0.9174757281553398, 0.9176755447941889, 0.9178743961352657, 0.9180722891566265, 0.9182692307692307, 0.9184652278177458, 0.9186602870813397, 0.918854415274463, 0.919047619047619, 0.9192399049881235, 0.919431279620853, 0.91725768321513, 0.9174528301886793, 0.9176470588235294, 0.9178403755868545, 0.9180327868852459, 0.9182242990654206, 0.9184149184149184, 0.9186046511627907, 0.9187935034802784, 0.9189814814814815, 0.9191685912240185, 0.9170506912442397, 0.9172413793103448, 0.9151376146788991, 0.9153318077803204, 0.9155251141552512, 0.9157175398633257, 0.9159090909090909, 0.9160997732426304, 0.916289592760181, 0.9164785553047404, 0.9166666666666666, 0.9168539325842696, 0.9170403587443946, 0.9172259507829977, 0.9174107142857143, 0.9175946547884187, 0.9177777777777778, 0.917960088691796, 0.918141592920354, 0.9183222958057395, 0.9185022026431718, 0.9186813186813186, 0.918859649122807, 0.9190371991247265, 0.9192139737991266, 0.9193899782135077, 0.9195652173913044, 0.9197396963123644, 0.9199134199134199, 0.91792656587473, 0.915948275862069, 0.9139784946236559, 0.9120171673819742, 0.9122055674518201, 0.9123931623931624, 0.9125799573560768, 0.9127659574468086, 0.9129511677282378, 0.913135593220339, 0.9133192389006343, 0.9135021097046413, 0.9136842105263158, 0.9138655462184874, 0.9140461215932913, 0.9142259414225942, 0.9144050104384134, 0.9145833333333333, 0.9147609147609148, 0.9149377593360996, 0.9151138716356108, 0.9152892561983471, 0.9154639175257732, 0.9156378600823045, 0.9158110882956879, 0.9159836065573771, 0.9161554192229039, 0.9163265306122449, 0.9164969450101833, 0.9166666666666666, 0.9168356997971603, 0.917004048582996, 0.9171717171717172, 0.9173387096774194, 0.9175050301810865, 0.9176706827309237, 0.9178356713426854, 0.918, 0.9181636726546906, 0.9183266932270916, 0.9184890656063618, 0.9166666666666666, 0.9168316831683169, 0.9169960474308301, 0.9171597633136095, 0.9173228346456693, 0.9174852652259332, 0.9176470588235294, 0.9178082191780822, 0.91796875, 0.9181286549707602, 0.9182879377431906, 0.9184466019417475, 0.9186046511627907, 0.9187620889748549, 0.918918918918919, 0.9190751445086706, 0.9192307692307692, 0.9193857965451055, 0.9195402298850575, 0.9196940726577438, 0.9198473282442748, 0.92, 0.9201520912547528, 0.920303605313093, 0.9204545454545454, 0.9206049149338374, 0.9207547169811321, 0.9209039548022598, 0.9210526315789473, 0.9212007504690432, 0.9213483146067416, 0.9214953271028037, 0.9216417910447762, 0.9217877094972067, 0.9219330855018587, 0.922077922077922, 0.9222222222222223, 0.922365988909427, 0.922509225092251, 0.9226519337016574, 0.9227941176470589, 0.9229357798165138, 0.9230769230769231, 0.923217550274223, 0.9233576642335767, 0.9234972677595629, 0.9236363636363636, 0.9237749546279492, 0.9239130434782609, 0.9240506329113924, 0.924187725631769, 0.9243243243243243, 0.9244604316546763, 0.9245960502692998, 0.9247311827956989, 0.924865831842576, 0.925, 0.9251336898395722, 0.9252669039145908, 0.9236234458259325, 0.9237588652482269, 0.9238938053097345, 0.9240282685512368, 0.9241622574955908, 0.9242957746478874, 0.9244288224956063, 0.9245614035087719, 0.9246935201401051, 0.9248251748251748, 0.9232111692844677, 0.9233449477351916, 0.9234782608695652, 0.9236111111111112, 0.9237435008665511, 0.9238754325259516, 0.924006908462867, 0.9241379310344827, 0.9242685025817556, 0.9243986254295533, 0.9245283018867925, 0.9246575342465754, 0.9247863247863248, 0.9249146757679181, 0.9250425894378195, 0.9251700680272109, 0.9252971137521222, 0.9254237288135593, 0.9255499153976311, 0.9256756756756757, 0.9258010118043845, 0.9259259259259259, 0.9260504201680673, 0.9261744966442953, 0.9262981574539364, 0.9247491638795987, 0.9248747913188647, 0.925, 0.9251247920133111, 0.925249169435216, 0.9253731343283582, 0.9254966887417219, 0.9256198347107438, 0.9257425742574258, 0.9258649093904449, 0.9259868421052632, 0.9261083743842364, 0.9245901639344263, 0.9247135842880524, 0.9248366013071896, 0.9249592169657422, 0.9250814332247557, 0.9252032520325203, 0.9253246753246753, 0.9254457050243112, 0.9255663430420712, 0.925686591276252, 0.9258064516129032, 0.9259259259259259, 0.9260450160771704, 0.9261637239165329, 0.9262820512820513, 0.9264, 0.9265175718849841, 0.9266347687400319, 0.9267515923566879, 0.9268680445151033, 0.926984126984127, 0.9270998415213946, 0.9272151898734177, 0.9257503949447078, 0.9258675078864353, 0.925984251968504, 0.9261006289308176, 0.9262166405023547, 0.9263322884012539, 0.9264475743348983, 0.9265625, 0.9266770670826833, 0.926791277258567, 0.926905132192846, 0.9270186335403726, 0.9271317829457364, 0.9272445820433437, 0.9273570324574961, 0.9274691358024691, 0.9275808936825886, 0.9276923076923077, 0.9278033794162827, 0.9279141104294478, 0.9280245022970903, 0.9281345565749235, 0.9282442748091603, 0.9283536585365854, 0.928462709284627, 0.9285714285714286, 0.9271623672230652, 0.9272727272727272, 0.9273827534039334, 0.9274924471299094, 0.9276018099547512, 0.927710843373494, 0.9278195488721804, 0.9279279279279279, 0.9280359820089955, 0.9281437125748503, 0.9282511210762332, 0.9283582089552239, 0.9284649776453056, 0.9285714285714286, 0.9286775631500743, 0.9287833827893175, 0.9288888888888889, 0.9289940828402367, 0.9290989660265879, 0.9292035398230089, 0.9293078055964654, 0.9294117647058824, 0.9295154185022027, 0.9296187683284457, 0.9297218155197657, 0.9298245614035088, 0.92992700729927, 0.9300291545189504, 0.9301310043668122, 0.9302325581395349, 0.93033381712627, 0.9304347826086956, 0.9305354558610709, 0.930635838150289, 0.9307359307359307, 0.930835734870317, 0.9309352517985612, 0.9310344827586207, 0.9311334289813487, 0.9312320916905444, 0.9313304721030042, 0.9314285714285714, 0.9300998573466477, 0.9301994301994302, 0.930298719772404, 0.9303977272727273, 0.9304964539007092, 0.9305949008498584, 0.9306930693069307, 0.9307909604519774, 0.9308885754583921, 0.9309859154929577, 0.9310829817158931, 0.9311797752808989, 0.9312762973352033, 0.9313725490196079, 0.9314685314685315, 0.9315642458100558, 0.9316596931659693, 0.9317548746518106, 0.9318497913769124, 0.9319444444444445, 0.9320388349514563, 0.9321329639889196, 0.9322268326417704, 0.9323204419889503, 0.9324137931034483, 0.9325068870523416, 0.9325997248968363, 0.9326923076923077, 0.9314128943758574, 0.9315068493150684, 0.9316005471956225, 0.930327868852459, 0.9304229195088677, 0.9305177111716622, 0.9306122448979591, 0.9307065217391305, 0.9308005427408412, 0.9308943089430894, 0.9309878213802436, 0.9310810810810811, 0.9311740890688259, 0.931266846361186, 0.9313593539703903, 0.9314516129032258, 0.9315436241610738, 0.9316353887399463, 0.9317269076305221, 0.9318181818181818, 0.931909212283044, 0.932, 0.9320905459387483, 0.9321808510638298, 0.9322709163346613, 0.9323607427055703, 0.9324503311258279, 0.9325396825396826, 0.9326287978863936, 0.9327176781002638, 0.9314888010540184, 0.9315789473684211, 0.9316688567674113, 0.931758530183727, 0.9318479685452162, 0.930628272251309, 0.930718954248366, 0.9308093994778068, 0.9308996088657105, 0.9309895833333334, 0.9310793237971391, 0.9311688311688312, 0.9312581063553826, 0.9313471502590673, 0.9314359637774903, 0.9315245478036176, 0.9316129032258065, 0.9317010309278351, 0.9317889317889317, 0.9318766066838047, 0.9319640564826701, 0.9320512820512821, 0.9321382842509603, 0.9309462915601023, 0.9310344827586207, 0.9311224489795918, 0.9312101910828026, 0.9312977099236641, 0.9313850063532402, 0.9314720812182741, 0.9315589353612167, 0.9316455696202531, 0.93173198482933, 0.9318181818181818, 0.9319041614123581, 0.9307304785894207, 0.9308176100628931, 0.9309045226130653, 0.93099121706399, 0.9298245614035088, 0.9299123904881101, 0.93, 0.9300873907615481, 0.9301745635910225, 0.9302615193026152, 0.9303482587064676, 0.9304347826086956, 0.9305210918114144, 0.9306071871127634, 0.9306930693069307, 0.930778739184178, 0.9308641975308642, 0.9309494451294698, 0.9310344827586207, 0.9298892988929889, 0.9287469287469288, 0.9288343558282208, 0.928921568627451, 0.9290085679314566, 0.9290953545232273, 0.9291819291819292, 0.9292682926829269, 0.9293544457978076, 0.9294403892944039, 0.928311057108141, 0.9283980582524272, 0.9284848484848485, 0.9285714285714286, 0.9286577992744861, 0.928743961352657, 0.9288299155609168, 0.9289156626506024, 0.9290012033694344, 0.9278846153846154, 0.9279711884753902, 0.9280575539568345, 0.9281437125748503, 0.9282296650717703, 0.9283154121863799, 0.9284009546539379, 0.9284862932061978, 0.9285714285714286, 0.9286563614744352, 0.9287410926365796, 0.9288256227758007, 0.9277251184834123, 0.9266272189349113, 0.9267139479905437, 0.9268004722550177, 0.9268867924528302, 0.9269729093050648, 0.9270588235294117, 0.927144535840188, 0.9272300469483568, 0.9273153575615475, 0.927400468384075, 0.927485380116959, 0.927570093457944, 0.9276546091015169, 0.9277389277389277, 0.9278230500582072, 0.9279069767441861, 0.9279907084785134, 0.9280742459396751, 0.9281575898030128, 0.9282407407407407, 0.9283236994219654, 0.9284064665127021, 0.9284890426758939, 0.9285714285714286, 0.9286536248561565, 0.9287356321839081, 0.928817451205511, 0.9288990825688074, 0.9289805269186713, 0.9290617848970252, 0.9291428571428572, 0.9292237442922374, 0.9293044469783353, 0.929384965831435, 0.9294653014789533, 0.9295454545454546, 0.9296254256526674, 0.9297052154195011, 0.9297848244620611, 0.9298642533936652, 0.9299435028248587, 0.9300225733634312, 0.9301014656144306, 0.9301801801801802, 0.9302587176602924, 0.9303370786516854, 0.9304152637485971, 0.9304932735426009, 0.9305711086226204, 0.930648769574944, 0.9307262569832402, 0.9308035714285714, 0.9308807134894092, 0.9309576837416481, 0.9310344827586207, 0.9311111111111111, 0.9311875693673696, 0.9312638580931264, 0.9313399778516057, 0.9314159292035398, 0.9314917127071823, 0.9315673289183223, 0.9316427783902976, 0.9317180616740088, 0.9317931793179318, 0.9318681318681319, 0.9319429198682766, 0.9320175438596491, 0.932092004381161, 0.9321663019693655, 0.9322404371584699, 0.9323144104803494, 0.9323882224645583, 0.9324618736383442, 0.9325353645266594, 0.9326086956521739, 0.9326818675352877, 0.9327548806941431, 0.9328277356446371, 0.9329004329004329, 0.932972972972973, 0.9330453563714903, 0.9331175836030206, 0.9331896551724138, 0.9332615715823466, 0.9333333333333333, 0.9334049409237379, 0.9334763948497854, 0.9335476956055734, 0.9336188436830836, 0.9336898395721925, 0.9337606837606838, 0.9338313767342583, 0.9339019189765458, 0.933972310969116, 0.9340425531914893, 0.9341126461211477, 0.9341825902335457, 0.9342523860021209, 0.934322033898305, 0.9333333333333333, 0.9334038054968288, 0.9334741288278775, 0.9335443037974683, 0.9336143308746049, 0.9326315789473684, 0.9327024185068349, 0.9327731092436975, 0.9328436516264428, 0.9329140461215933, 0.9329842931937172, 0.9330543933054394, 0.9331243469174504, 0.9331941544885177, 0.9332638164754953, 0.9333333333333333, 0.9334027055150884, 0.9334719334719335, 0.9335410176531672, 0.9336099585062241, 0.933678756476684, 0.9337474120082816, 0.9338159255429163, 0.9338842975206612, 0.9339525283797729, 0.934020618556701, 0.9340885684860968, 0.934156378600823, 0.934224049331963, 0.9342915811088296, 0.9343589743589743, 0.9344262295081968, 0.9344933469805528, 0.934560327198364, 0.9346271705822268, 0.9346938775510204, 0.9347604485219164, 0.9348268839103869, 0.9348931841302136, 0.9349593495934959, 0.9350253807106599, 0.9350912778904665, 0.9341438703140831, 0.9342105263157895, 0.9342770475227502, 0.9343434343434344, 0.934409687184662, 0.9344758064516129, 0.9345417925478349, 0.9346076458752515, 0.9346733668341709, 0.9347389558232931, 0.9348044132397192, 0.9348697394789579, 0.934934934934935, 0.935]\n",
    "accuracy_cumulative_nn_50_d0_50_ml0_e400 = [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.7142857142857143, 0.625, 0.6666666666666666, 0.6, 0.6363636363636364, 0.5833333333333334, 0.6153846153846154, 0.6428571428571429, 0.6666666666666666, 0.625, 0.5882352941176471, 0.6111111111111112, 0.5789473684210527, 0.6, 0.6190476190476191, 0.5909090909090909, 0.6086956521739131, 0.625, 0.64, 0.6153846153846154, 0.5925925925925926, 0.5714285714285714, 0.5517241379310345, 0.5333333333333333, 0.5161290322580645, 0.5, 0.5151515151515151, 0.5294117647058824, 0.5428571428571428, 0.5555555555555556, 0.5405405405405406, 0.5263157894736842, 0.5384615384615384, 0.525, 0.5121951219512195, 0.5238095238095238, 0.5348837209302325, 0.5454545454545454, 0.5555555555555556, 0.5652173913043478, 0.574468085106383, 0.5625, 0.5510204081632653, 0.56, 0.5686274509803921, 0.5769230769230769, 0.5849056603773585, 0.5925925925925926, 0.6, 0.5892857142857143, 0.5789473684210527, 0.5689655172413793, 0.559322033898305, 0.55, 0.5573770491803278, 0.5645161290322581, 0.5714285714285714, 0.578125, 0.5846153846153846, 0.5909090909090909, 0.5970149253731343, 0.5882352941176471, 0.5942028985507246, 0.5857142857142857, 0.5774647887323944, 0.5833333333333334, 0.589041095890411, 0.5945945945945946, 0.6, 0.5921052631578947, 0.5844155844155844, 0.5769230769230769, 0.5822784810126582, 0.575, 0.5679012345679012, 0.5609756097560976, 0.5662650602409639, 0.5714285714285714, 0.5764705882352941, 0.5813953488372093, 0.5747126436781609, 0.5795454545454546, 0.5730337078651685, 0.5777777777777777, 0.5824175824175825, 0.5760869565217391, 0.5806451612903226, 0.5851063829787234, 0.5894736842105263, 0.59375, 0.5979381443298969, 0.6020408163265306, 0.5959595959595959, 0.6, 0.594059405940594, 0.5980392156862745, 0.5922330097087378, 0.5961538461538461, 0.6, 0.6037735849056604, 0.5981308411214953, 0.6018518518518519, 0.5963302752293578, 0.5909090909090909, 0.5945945945945946, 0.5982142857142857, 0.6017699115044248, 0.6052631578947368, 0.6086956521739131, 0.6120689655172413, 0.6068376068376068, 0.6016949152542372, 0.6050420168067226, 0.6083333333333333, 0.6115702479338843, 0.6147540983606558, 0.6178861788617886, 0.6209677419354839, 0.616, 0.6111111111111112, 0.6062992125984252, 0.6015625, 0.6046511627906976, 0.6, 0.6030534351145038, 0.6060606060606061, 0.6090225563909775, 0.6119402985074627, 0.6074074074074074, 0.6029411764705882, 0.5985401459854015, 0.5942028985507246, 0.5899280575539568, 0.5857142857142857, 0.5886524822695035, 0.5915492957746479, 0.5944055944055944, 0.5972222222222222, 0.593103448275862, 0.5958904109589042, 0.5986394557823129, 0.5945945945945946, 0.5906040268456376, 0.5933333333333334]\n",
    "plt.plot(accuracy_cumulative_nn_50_50_ml1600_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_50_ml1400_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_50_ml1200_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_50_ml1000_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_50_ml800_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_50_ml600_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_50_ml400_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_50_ml200_e200)\n",
    "plt.plot(accuracy_cumulative_nn_50_d0_50_ml0_e200)\n",
    "plt.xlabel('trials')\n",
    "plt.xlim(0,len(accuracy_cumulative_nn_50_50_ml200_e200))\n",
    "plt.ylabel('cumulative accuracy')\n",
    "plt.xlim(0,1000)\n",
    "plt.ylim(0.5,1.01)\n",
    "plt.legend(['ml 1600', 'ml 1400', 'ml 1200', 'ml 1000', 'ml 800', 'ml 600', 'ml 400', 'ml 200', 'ml 0: reference'])\n",
    "plt.title('Accuracy improves after re-training with added manual labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_cumulative_cnn_cce_32_64_d025_ml0_e200 = [1.0, 0.5, 0.6666666666666666, 0.75, 0.6, 0.5, 0.5714285714285714, 0.5, 0.5555555555555556, 0.6, 0.6363636363636364, 0.6666666666666666, 0.6923076923076923, 0.7142857142857143, 0.7333333333333333, 0.75, 0.7647058823529411, 0.7222222222222222, 0.7368421052631579, 0.7, 0.7142857142857143, 0.7272727272727273, 0.7391304347826086, 0.75, 0.72, 0.7307692307692307, 0.7407407407407407, 0.7142857142857143, 0.6896551724137931, 0.6666666666666666, 0.6774193548387096, 0.6875, 0.696969696969697, 0.7058823529411765, 0.6857142857142857, 0.6944444444444444, 0.6756756756756757, 0.6578947368421053, 0.6410256410256411, 0.625, 0.6097560975609756, 0.6190476190476191, 0.627906976744186, 0.6363636363636364, 0.6222222222222222, 0.6304347826086957, 0.6170212765957447, 0.625, 0.6326530612244898, 0.62, 0.6078431372549019, 0.5961538461538461, 0.6037735849056604, 0.5925925925925926, 0.6, 0.6071428571428571, 0.5964912280701754, 0.5862068965517241, 0.576271186440678, 0.5666666666666667, 0.5737704918032787, 0.5645161290322581, 0.5714285714285714, 0.578125, 0.5846153846153846, 0.5909090909090909, 0.582089552238806, 0.5735294117647058, 0.5652173913043478, 0.5571428571428572, 0.5633802816901409, 0.5694444444444444, 0.5753424657534246, 0.581081081081081, 0.5866666666666667, 0.5921052631578947, 0.5844155844155844, 0.5769230769230769, 0.569620253164557, 0.575, 0.5679012345679012, 0.5609756097560976, 0.5662650602409639, 0.5714285714285714, 0.5764705882352941, 0.5813953488372093, 0.5862068965517241, 0.5795454545454546, 0.5730337078651685, 0.5666666666666667, 0.5714285714285714, 0.5652173913043478, 0.5698924731182796, 0.574468085106383, 0.5684210526315789, 0.5729166666666666, 0.5670103092783505, 0.5612244897959183, 0.5656565656565656, 0.56, 0.5544554455445545, 0.5490196078431373, 0.5533980582524272, 0.5576923076923077, 0.5619047619047619, 0.5660377358490566, 0.5607476635514018, 0.5555555555555556, 0.5504587155963303, 0.5454545454545454, 0.5495495495495496, 0.5446428571428571, 0.5486725663716814, 0.5526315789473685, 0.5478260869565217, 0.5517241379310345, 0.5470085470085471, 0.5423728813559322, 0.5378151260504201, 0.5416666666666666, 0.5454545454545454, 0.5491803278688525, 0.5528455284552846, 0.5564516129032258, 0.552, 0.5555555555555556, 0.5590551181102362, 0.5546875, 0.5503875968992248, 0.5538461538461539, 0.5572519083969466, 0.5606060606060606, 0.5639097744360902, 0.5671641791044776, 0.5703703703703704, 0.5661764705882353, 0.5620437956204379, 0.5579710144927537, 0.5611510791366906, 0.5642857142857143, 0.5673758865248227, 0.5704225352112676, 0.5734265734265734, 0.5763888888888888, 0.5724137931034483, 0.5753424657534246, 0.5782312925170068, 0.581081081081081, 0.5771812080536913, 0.58, 0.5761589403973509, 0.5723684210526315, 0.5686274509803921, 0.5714285714285714, 0.567741935483871, 0.5705128205128205, 0.5732484076433121, 0.569620253164557, 0.5723270440251572, 0.56875, 0.5714285714285714, 0.5679012345679012, 0.5705521472392638, 0.573170731707317, 0.5757575757575758, 0.572289156626506, 0.5688622754491018, 0.5714285714285714, 0.5680473372781065, 0.5647058823529412, 0.5672514619883041, 0.563953488372093, 0.5664739884393064, 0.5689655172413793, 0.5714285714285714, 0.5681818181818182, 0.5706214689265536, 0.5674157303370787, 0.5642458100558659, 0.5611111111111111, 0.56353591160221, 0.5604395604395604, 0.5628415300546448, 0.5652173913043478, 0.5621621621621622, 0.5645161290322581, 0.5614973262032086, 0.5585106382978723, 0.5608465608465608, 0.5578947368421052, 0.5549738219895288, 0.5520833333333334, 0.5544041450777202, 0.5567010309278351, 0.5538461538461539, 0.5510204081632653, 0.5532994923857868, 0.5505050505050505, 0.5527638190954773, 0.55]\n",
    "accuracy_cumulative_cnn_cce_32_64_d025_ml0_e400 = [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.8571428571428571, 0.75, 0.6666666666666666, 0.7, 0.7272727272727273, 0.6666666666666666, 0.6923076923076923, 0.7142857142857143, 0.6666666666666666, 0.6875, 0.6470588235294118, 0.6111111111111112, 0.631578947368421, 0.6, 0.5714285714285714, 0.5454545454545454, 0.5652173913043478, 0.5833333333333334, 0.56, 0.5769230769230769, 0.5555555555555556, 0.5357142857142857, 0.5517241379310345, 0.5666666666666667, 0.5806451612903226, 0.5625, 0.5757575757575758, 0.5882352941176471, 0.6, 0.6111111111111112, 0.6216216216216216, 0.6052631578947368, 0.6153846153846154, 0.6, 0.6097560975609756, 0.6190476190476191, 0.627906976744186, 0.6363636363636364, 0.6444444444444445, 0.6521739130434783, 0.6382978723404256, 0.6458333333333334, 0.6326530612244898, 0.64, 0.6470588235294118, 0.6346153846153846, 0.6415094339622641, 0.6481481481481481, 0.6545454545454545, 0.6428571428571429, 0.631578947368421, 0.6206896551724138, 0.6101694915254238, 0.6166666666666667, 0.6065573770491803, 0.6129032258064516, 0.6190476190476191, 0.625, 0.6307692307692307, 0.6363636363636364, 0.6268656716417911, 0.6176470588235294, 0.6086956521739131, 0.6, 0.6056338028169014, 0.6111111111111112, 0.6164383561643836, 0.6216216216216216, 0.6266666666666667, 0.618421052631579, 0.6103896103896104, 0.6025641025641025, 0.5949367088607594, 0.5875, 0.5925925925925926, 0.5975609756097561, 0.6024096385542169, 0.6071428571428571, 0.6, 0.5930232558139535, 0.5862068965517241, 0.5795454545454546, 0.5842696629213483, 0.5777777777777777, 0.5824175824175825, 0.5869565217391305, 0.5913978494623656, 0.5957446808510638, 0.6, 0.59375, 0.5979381443298969, 0.5918367346938775, 0.5858585858585859, 0.58, 0.5841584158415841, 0.5784313725490197, 0.5728155339805825, 0.5769230769230769, 0.580952380952381, 0.5849056603773585, 0.5794392523364486, 0.5833333333333334, 0.5779816513761468, 0.5727272727272728, 0.5765765765765766, 0.5714285714285714, 0.5752212389380531, 0.5789473684210527, 0.5826086956521739, 0.5775862068965517, 0.5726495726495726, 0.576271186440678, 0.5798319327731093, 0.5833333333333334, 0.5867768595041323, 0.5901639344262295, 0.5934959349593496, 0.5967741935483871, 0.6, 0.6031746031746031, 0.5984251968503937, 0.59375, 0.5891472868217055, 0.5846153846153846, 0.5877862595419847, 0.5833333333333334, 0.5864661654135338, 0.5895522388059702, 0.5851851851851851, 0.5808823529411765, 0.5766423357664233, 0.5797101449275363, 0.5755395683453237, 0.5714285714285714, 0.5673758865248227, 0.5633802816901409, 0.5594405594405595, 0.5625, 0.5655172413793104, 0.5616438356164384, 0.564625850340136, 0.5608108108108109, 0.5570469798657718, 0.5533333333333333, 0.5496688741721855, 0.5526315789473685, 0.5555555555555556, 0.5584415584415584, 0.5548387096774193, 0.5576923076923077, 0.554140127388535, 0.5506329113924051, 0.5534591194968553, 0.55625, 0.5590062111801242, 0.5555555555555556, 0.5521472392638037, 0.5548780487804879, 0.5515151515151515, 0.5542168674698795, 0.5508982035928144, 0.5476190476190477, 0.5443786982248521, 0.5411764705882353, 0.5380116959064327, 0.5348837209302325, 0.5375722543352601, 0.5402298850574713, 0.5428571428571428, 0.5454545454545454, 0.5423728813559322, 0.5449438202247191, 0.5418994413407822, 0.5388888888888889, 0.5414364640883977, 0.5439560439560439, 0.546448087431694, 0.5489130434782609, 0.5513513513513514, 0.553763440860215, 0.5561497326203209, 0.5585106382978723, 0.5555555555555556, 0.5526315789473685, 0.5497382198952879, 0.5520833333333334, 0.5544041450777202, 0.5567010309278351, 0.558974358974359, 0.5561224489795918, 0.5532994923857868, 0.5505050505050505, 0.5527638190954773, 0.55]\n",
    "plt.plot(accuracy_cumulative_cnn_cce_32_64_d025_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_cnn_cce_32_64_d025_ml0_e400)\n",
    "plt.xlabel('trials')\n",
    "plt.xlim(0,len(accuracy_cumulative_cnn_cce_32_64_d025_ml0_e200))\n",
    "plt.ylim(0.5,1.01)\n",
    "plt.ylabel('cumulative accuracy')\n",
    "plt.legend(['epochs = 200: reference', 'epochs = 400']) #cnn_cce_32_64_d0.25_ml0_e200\n",
    "plt.title('CNN accuracy for differrent duration of training');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_cumulative_cnn_cce_32_64_d025_ml0_e200 = [1.0, 0.5, 0.6666666666666666, 0.75, 0.6, 0.5, 0.5714285714285714, 0.5, 0.5555555555555556, 0.6, 0.6363636363636364, 0.6666666666666666, 0.6923076923076923, 0.7142857142857143, 0.7333333333333333, 0.75, 0.7647058823529411, 0.7222222222222222, 0.7368421052631579, 0.7, 0.7142857142857143, 0.7272727272727273, 0.7391304347826086, 0.75, 0.72, 0.7307692307692307, 0.7407407407407407, 0.7142857142857143, 0.6896551724137931, 0.6666666666666666, 0.6774193548387096, 0.6875, 0.696969696969697, 0.7058823529411765, 0.6857142857142857, 0.6944444444444444, 0.6756756756756757, 0.6578947368421053, 0.6410256410256411, 0.625, 0.6097560975609756, 0.6190476190476191, 0.627906976744186, 0.6363636363636364, 0.6222222222222222, 0.6304347826086957, 0.6170212765957447, 0.625, 0.6326530612244898, 0.62, 0.6078431372549019, 0.5961538461538461, 0.6037735849056604, 0.5925925925925926, 0.6, 0.6071428571428571, 0.5964912280701754, 0.5862068965517241, 0.576271186440678, 0.5666666666666667, 0.5737704918032787, 0.5645161290322581, 0.5714285714285714, 0.578125, 0.5846153846153846, 0.5909090909090909, 0.582089552238806, 0.5735294117647058, 0.5652173913043478, 0.5571428571428572, 0.5633802816901409, 0.5694444444444444, 0.5753424657534246, 0.581081081081081, 0.5866666666666667, 0.5921052631578947, 0.5844155844155844, 0.5769230769230769, 0.569620253164557, 0.575, 0.5679012345679012, 0.5609756097560976, 0.5662650602409639, 0.5714285714285714, 0.5764705882352941, 0.5813953488372093, 0.5862068965517241, 0.5795454545454546, 0.5730337078651685, 0.5666666666666667, 0.5714285714285714, 0.5652173913043478, 0.5698924731182796, 0.574468085106383, 0.5684210526315789, 0.5729166666666666, 0.5670103092783505, 0.5612244897959183, 0.5656565656565656, 0.56, 0.5544554455445545, 0.5490196078431373, 0.5533980582524272, 0.5576923076923077, 0.5619047619047619, 0.5660377358490566, 0.5607476635514018, 0.5555555555555556, 0.5504587155963303, 0.5454545454545454, 0.5495495495495496, 0.5446428571428571, 0.5486725663716814, 0.5526315789473685, 0.5478260869565217, 0.5517241379310345, 0.5470085470085471, 0.5423728813559322, 0.5378151260504201, 0.5416666666666666, 0.5454545454545454, 0.5491803278688525, 0.5528455284552846, 0.5564516129032258, 0.552, 0.5555555555555556, 0.5590551181102362, 0.5546875, 0.5503875968992248, 0.5538461538461539, 0.5572519083969466, 0.5606060606060606, 0.5639097744360902, 0.5671641791044776, 0.5703703703703704, 0.5661764705882353, 0.5620437956204379, 0.5579710144927537, 0.5611510791366906, 0.5642857142857143, 0.5673758865248227, 0.5704225352112676, 0.5734265734265734, 0.5763888888888888, 0.5724137931034483, 0.5753424657534246, 0.5782312925170068, 0.581081081081081, 0.5771812080536913, 0.58, 0.5761589403973509, 0.5723684210526315, 0.5686274509803921, 0.5714285714285714, 0.567741935483871, 0.5705128205128205, 0.5732484076433121, 0.569620253164557, 0.5723270440251572, 0.56875, 0.5714285714285714, 0.5679012345679012, 0.5705521472392638, 0.573170731707317, 0.5757575757575758, 0.572289156626506, 0.5688622754491018, 0.5714285714285714, 0.5680473372781065, 0.5647058823529412, 0.5672514619883041, 0.563953488372093, 0.5664739884393064, 0.5689655172413793, 0.5714285714285714, 0.5681818181818182, 0.5706214689265536, 0.5674157303370787, 0.5642458100558659, 0.5611111111111111, 0.56353591160221, 0.5604395604395604, 0.5628415300546448, 0.5652173913043478, 0.5621621621621622, 0.5645161290322581, 0.5614973262032086, 0.5585106382978723, 0.5608465608465608, 0.5578947368421052, 0.5549738219895288, 0.5520833333333334, 0.5544041450777202, 0.5567010309278351, 0.5538461538461539, 0.5510204081632653, 0.5532994923857868, 0.5505050505050505, 0.5527638190954773, 0.55]\n",
    "accuracy_cumulative_cnn_mse_32_64_d025_ml0_e200 = [1.0, 0.5, 0.6666666666666666, 0.75, 0.8, 0.8333333333333334, 0.7142857142857143, 0.75, 0.7777777777777778, 0.8, 0.7272727272727273, 0.6666666666666666, 0.6153846153846154, 0.6428571428571429, 0.6666666666666666, 0.6875, 0.6470588235294118, 0.6666666666666666, 0.631578947368421, 0.65, 0.6190476190476191, 0.6363636363636364, 0.6521739130434783, 0.625, 0.64, 0.6538461538461539, 0.6666666666666666, 0.6428571428571429, 0.6551724137931034, 0.6333333333333333, 0.6129032258064516, 0.625, 0.6363636363636364, 0.6470588235294118, 0.6571428571428571, 0.6666666666666666, 0.6486486486486487, 0.6578947368421053, 0.6666666666666666, 0.65, 0.6341463414634146, 0.6428571428571429, 0.6511627906976745, 0.6590909090909091, 0.6666666666666666, 0.6739130434782609, 0.6595744680851063, 0.6666666666666666, 0.6530612244897959, 0.66, 0.6470588235294118, 0.6346153846153846, 0.6415094339622641, 0.6481481481481481, 0.6363636363636364, 0.6428571428571429, 0.6491228070175439, 0.6379310344827587, 0.6440677966101694, 0.65, 0.6557377049180327, 0.6612903225806451, 0.6666666666666666, 0.671875, 0.676923076923077, 0.6818181818181818, 0.6865671641791045, 0.6764705882352942, 0.6811594202898551, 0.6714285714285714, 0.676056338028169, 0.6666666666666666, 0.6712328767123288, 0.6621621621621622, 0.6533333333333333, 0.6447368421052632, 0.6493506493506493, 0.6410256410256411, 0.6455696202531646, 0.6375, 0.6419753086419753, 0.6341463414634146, 0.6385542168674698, 0.6428571428571429, 0.6470588235294118, 0.6511627906976745, 0.6436781609195402, 0.6363636363636364, 0.6292134831460674, 0.6222222222222222, 0.6153846153846154, 0.6086956521739131, 0.6129032258064516, 0.6170212765957447, 0.6210526315789474, 0.625, 0.6185567010309279, 0.6224489795918368, 0.6262626262626263, 0.63, 0.6237623762376238, 0.6274509803921569, 0.6310679611650486, 0.6346153846153846, 0.638095238095238, 0.6415094339622641, 0.6448598130841121, 0.6388888888888888, 0.6422018348623854, 0.6454545454545455, 0.6486486486486487, 0.6517857142857143, 0.6548672566371682, 0.6578947368421053, 0.6608695652173913, 0.6551724137931034, 0.6581196581196581, 0.652542372881356, 0.6554621848739496, 0.65, 0.6446280991735537, 0.639344262295082, 0.6422764227642277, 0.6451612903225806, 0.648, 0.6507936507936508, 0.6456692913385826, 0.640625, 0.6434108527131783, 0.6461538461538462, 0.648854961832061, 0.6439393939393939, 0.6390977443609023, 0.6417910447761194, 0.6370370370370371, 0.6397058823529411, 0.6423357664233577, 0.6376811594202898, 0.6330935251798561, 0.6357142857142857, 0.6312056737588653, 0.6338028169014085, 0.6293706293706294, 0.6319444444444444, 0.6275862068965518, 0.6301369863013698, 0.6326530612244898, 0.6351351351351351, 0.6308724832214765, 0.6333333333333333, 0.6291390728476821, 0.631578947368421, 0.6339869281045751, 0.6363636363636364, 0.6387096774193548, 0.6410256410256411, 0.6369426751592356, 0.6329113924050633, 0.6289308176100629, 0.625, 0.6273291925465838, 0.6234567901234568, 0.6257668711656442, 0.6280487804878049, 0.6303030303030303, 0.6325301204819277, 0.6347305389221557, 0.6309523809523809, 0.6331360946745562, 0.6352941176470588, 0.631578947368421, 0.627906976744186, 0.630057803468208, 0.632183908045977, 0.6342857142857142, 0.6363636363636364, 0.632768361581921, 0.6292134831460674, 0.6256983240223464, 0.6222222222222222, 0.6243093922651933, 0.6263736263736264, 0.6284153005464481, 0.6304347826086957, 0.6324324324324324, 0.6290322580645161, 0.6256684491978609, 0.6223404255319149, 0.6190476190476191, 0.6210526315789474, 0.6230366492146597, 0.6197916666666666, 0.6217616580310881, 0.6237113402061856, 0.6256410256410256, 0.6224489795918368, 0.6192893401015228, 0.6161616161616161, 0.6180904522613065, 0.62]\n",
    "accuracy_cumulative_cnn_mae_32_64_d025_ml0_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.8571428571428571, 0.75, 0.6666666666666666, 0.6, 0.5454545454545454, 0.5, 0.5384615384615384, 0.5714285714285714, 0.6, 0.5625, 0.5294117647058824, 0.5, 0.5263157894736842, 0.5, 0.47619047619047616, 0.5, 0.5217391304347826, 0.5416666666666666, 0.52, 0.5384615384615384, 0.5185185185185185, 0.5, 0.5172413793103449, 0.5, 0.4838709677419355, 0.46875, 0.48484848484848486, 0.5, 0.5142857142857142, 0.5277777777777778, 0.5405405405405406, 0.5263157894736842, 0.5128205128205128, 0.525, 0.5365853658536586, 0.5238095238095238, 0.5348837209302325, 0.5454545454545454, 0.5555555555555556, 0.5652173913043478, 0.574468085106383, 0.5625, 0.5714285714285714, 0.56, 0.5686274509803921, 0.5576923076923077, 0.5660377358490566, 0.5740740740740741, 0.5636363636363636, 0.5714285714285714, 0.5614035087719298, 0.5689655172413793, 0.559322033898305, 0.55, 0.5409836065573771, 0.532258064516129, 0.5396825396825397, 0.546875, 0.5384615384615384, 0.5454545454545454, 0.5373134328358209, 0.5294117647058824, 0.5217391304347826, 0.5142857142857142, 0.5070422535211268, 0.5, 0.5068493150684932, 0.5135135135135135, 0.52, 0.5263157894736842, 0.5194805194805194, 0.5128205128205128, 0.5189873417721519, 0.525, 0.5308641975308642, 0.5365853658536586, 0.5421686746987951, 0.5476190476190477, 0.5529411764705883, 0.5581395348837209, 0.5632183908045977, 0.5681818181818182, 0.5617977528089888, 0.5555555555555556, 0.5604395604395604, 0.5543478260869565, 0.5591397849462365, 0.5638297872340425, 0.5578947368421052, 0.5520833333333334, 0.5567010309278351, 0.5510204081632653, 0.5555555555555556, 0.55, 0.5544554455445545, 0.5588235294117647, 0.5631067961165048, 0.5673076923076923, 0.5619047619047619, 0.5660377358490566, 0.5700934579439252, 0.5740740740740741, 0.5779816513761468, 0.5818181818181818, 0.5765765765765766, 0.5714285714285714, 0.5663716814159292, 0.5701754385964912, 0.5652173913043478, 0.5689655172413793, 0.5641025641025641, 0.5677966101694916, 0.5630252100840336, 0.5666666666666667, 0.5619834710743802, 0.5655737704918032, 0.5691056910569106, 0.5725806451612904, 0.576, 0.5793650793650794, 0.5748031496062992, 0.578125, 0.5813953488372093, 0.5769230769230769, 0.5725190839694656, 0.5681818181818182, 0.5639097744360902, 0.5671641791044776, 0.5703703703703704, 0.5661764705882353, 0.5620437956204379, 0.5652173913043478, 0.5611510791366906, 0.5571428571428572, 0.5602836879432624, 0.5563380281690141, 0.5594405594405595, 0.5625, 0.5586206896551724, 0.5616438356164384, 0.564625850340136, 0.5675675675675675, 0.5704697986577181, 0.5666666666666667, 0.5695364238410596, 0.5657894736842105, 0.5686274509803921, 0.5714285714285714, 0.567741935483871, 0.5705128205128205, 0.5668789808917197, 0.569620253164557, 0.5723270440251572, 0.56875, 0.5714285714285714, 0.5679012345679012, 0.5705521472392638, 0.573170731707317, 0.5757575757575758, 0.572289156626506, 0.5688622754491018, 0.5654761904761905, 0.5680473372781065, 0.5705882352941176, 0.5672514619883041, 0.563953488372093, 0.5664739884393064, 0.5689655172413793, 0.5714285714285714, 0.5738636363636364, 0.5706214689265536, 0.5730337078651685, 0.5754189944134078, 0.5722222222222222, 0.574585635359116, 0.5769230769230769, 0.5792349726775956, 0.5815217391304348, 0.5837837837837838, 0.5860215053763441, 0.5882352941176471, 0.5851063829787234, 0.582010582010582, 0.5789473684210527, 0.5759162303664922, 0.578125, 0.5803108808290155, 0.5824742268041238, 0.5846153846153846, 0.5867346938775511, 0.583756345177665, 0.5858585858585859, 0.5879396984924623, 0.585]\n",
    "plt.plot(accuracy_cumulative_cnn_cce_32_64_d025_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_cnn_mse_32_64_d025_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_cnn_mae_32_64_d025_ml0_e200)\n",
    "plt.xlabel('trials')\n",
    "plt.xlim(0,len(accuracy_cumulative_cnn_cce_32_64_d025_ml0_e200))\n",
    "plt.ylim(0.4,1.01)\n",
    "plt.ylabel('cumulative accuracy')\n",
    "plt.legend(['loss = categorical_crossentropy: reference', 'loss = mean_squared_error', 'loss = mean_absolute_error']) #cnn_cce_32_64_d0.25_ml0_e200\n",
    "plt.title('CNN accuracy for differrent loss functions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_cumulative_cnn_cce_Adam_32_64_d025_ml0_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.875, 0.8888888888888888, 0.8, 0.8181818181818182, 0.8333333333333334, 0.8461538461538461, 0.8571428571428571, 0.8666666666666667, 0.875, 0.8823529411764706, 0.8888888888888888, 0.8947368421052632, 0.85, 0.8571428571428571, 0.8636363636363636, 0.8695652173913043, 0.875, 0.88, 0.8846153846153846, 0.8518518518518519, 0.8214285714285714, 0.7931034482758621, 0.7666666666666667, 0.7741935483870968, 0.78125, 0.7878787878787878, 0.7941176470588235, 0.8, 0.8055555555555556, 0.7837837837837838, 0.7631578947368421, 0.7692307692307693, 0.75, 0.7560975609756098, 0.7619047619047619, 0.7674418604651163, 0.7727272727272727, 0.7777777777777778, 0.782608695652174, 0.7872340425531915, 0.7708333333333334, 0.7755102040816326, 0.78, 0.7647058823529411, 0.75, 0.7547169811320755, 0.7592592592592593, 0.7636363636363637, 0.7678571428571429, 0.7543859649122807, 0.7413793103448276, 0.7457627118644068, 0.75, 0.7540983606557377, 0.7580645161290323, 0.7619047619047619, 0.765625, 0.7692307692307693, 0.7727272727272727, 0.7761194029850746, 0.7794117647058824, 0.782608695652174, 0.7857142857142857, 0.7887323943661971, 0.7916666666666666, 0.7945205479452054, 0.7972972972972973, 0.8, 0.8026315789473685, 0.8051948051948052, 0.8076923076923077, 0.810126582278481, 0.8125, 0.8148148148148148, 0.8170731707317073, 0.8192771084337349, 0.8214285714285714, 0.8235294117647058, 0.8255813953488372, 0.8275862068965517, 0.8181818181818182, 0.8202247191011236, 0.8222222222222222, 0.8241758241758241, 0.8260869565217391, 0.8279569892473119, 0.8297872340425532, 0.8315789473684211, 0.8333333333333334, 0.8247422680412371, 0.826530612244898, 0.8282828282828283, 0.82, 0.8217821782178217, 0.8235294117647058, 0.8252427184466019, 0.8269230769230769, 0.8285714285714286, 0.8301886792452831, 0.8317757009345794, 0.8333333333333334, 0.8348623853211009, 0.8363636363636363, 0.8378378378378378, 0.8392857142857143, 0.8407079646017699, 0.8421052631578947, 0.8434782608695652, 0.8448275862068966, 0.8376068376068376, 0.8305084745762712, 0.8235294117647058, 0.825, 0.8264462809917356, 0.8278688524590164, 0.8292682926829268, 0.8306451612903226, 0.832, 0.8333333333333334, 0.8267716535433071, 0.828125, 0.8294573643410853, 0.8307692307692308, 0.8320610687022901, 0.8333333333333334, 0.8345864661654135, 0.835820895522388, 0.837037037037037, 0.8382352941176471, 0.8394160583941606, 0.8405797101449275, 0.841726618705036, 0.8428571428571429, 0.8439716312056738, 0.8450704225352113, 0.8461538461538461, 0.8472222222222222, 0.8482758620689655, 0.8493150684931506, 0.8503401360544217, 0.8513513513513513, 0.8523489932885906, 0.8533333333333334, 0.8543046357615894, 0.8552631578947368, 0.8562091503267973, 0.8571428571428571, 0.8580645161290322, 0.8589743589743589, 0.8598726114649682, 0.8544303797468354, 0.8490566037735849, 0.84375, 0.8385093167701864, 0.8333333333333334, 0.8343558282208589, 0.8353658536585366, 0.8363636363636363, 0.8373493975903614, 0.8323353293413174, 0.8273809523809523, 0.8284023668639053, 0.8235294117647058, 0.8245614035087719, 0.8255813953488372, 0.8265895953757225, 0.8275862068965517, 0.8285714285714286, 0.8295454545454546, 0.8248587570621468, 0.8258426966292135, 0.8268156424581006, 0.8222222222222222, 0.8232044198895028, 0.8241758241758241, 0.825136612021858, 0.8260869565217391, 0.827027027027027, 0.8279569892473119, 0.8235294117647058, 0.824468085106383, 0.8253968253968254, 0.8210526315789474, 0.8219895287958116, 0.8229166666666666, 0.8238341968911918, 0.8247422680412371, 0.8256410256410256, 0.826530612244898, 0.8274111675126904, 0.8282828282828283, 0.8291457286432161, 0.83]\n",
    "accuracy_cumulative_cnn_cce_Nadam_32_64_d025_ml0_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.75, 0.7777777777777778, 0.8, 0.8181818181818182, 0.8333333333333334, 0.8461538461538461, 0.8571428571428571, 0.8666666666666667, 0.875, 0.8823529411764706, 0.8888888888888888, 0.8947368421052632, 0.9, 0.9047619047619048, 0.9090909090909091, 0.9130434782608695, 0.9166666666666666, 0.92, 0.9230769230769231, 0.8888888888888888, 0.8928571428571429, 0.896551724137931, 0.8666666666666667, 0.8709677419354839, 0.875, 0.8787878787878788, 0.8823529411764706, 0.8857142857142857, 0.8888888888888888, 0.8918918918918919, 0.868421052631579, 0.8461538461538461, 0.825, 0.8292682926829268, 0.8333333333333334, 0.8372093023255814, 0.8409090909090909, 0.8444444444444444, 0.8478260869565217, 0.851063829787234, 0.8541666666666666, 0.8571428571428571, 0.84, 0.8235294117647058, 0.8269230769230769, 0.8301886792452831, 0.8333333333333334, 0.8363636363636363, 0.8392857142857143, 0.8245614035087719, 0.8275862068965517, 0.8305084745762712, 0.8166666666666667, 0.819672131147541, 0.8064516129032258, 0.8095238095238095, 0.8125, 0.8, 0.803030303030303, 0.7910447761194029, 0.7941176470588235, 0.782608695652174, 0.7857142857142857, 0.7887323943661971, 0.7916666666666666, 0.7945205479452054, 0.7972972972972973, 0.8, 0.8026315789473685, 0.8051948051948052, 0.8076923076923077, 0.7974683544303798, 0.7875, 0.7901234567901234, 0.7926829268292683, 0.7951807228915663, 0.7976190476190477, 0.8, 0.8023255813953488, 0.7931034482758621, 0.7954545454545454, 0.797752808988764, 0.7888888888888889, 0.7912087912087912, 0.7934782608695652, 0.7956989247311828, 0.7978723404255319, 0.8, 0.8020833333333334, 0.7938144329896907, 0.7959183673469388, 0.797979797979798, 0.79, 0.7821782178217822, 0.7843137254901961, 0.7864077669902912, 0.7884615384615384, 0.7904761904761904, 0.7924528301886793, 0.794392523364486, 0.7962962962962963, 0.7981651376146789, 0.8, 0.8018018018018018, 0.8035714285714286, 0.8053097345132744, 0.8070175438596491, 0.808695652173913, 0.8103448275862069, 0.8034188034188035, 0.7966101694915254, 0.7983193277310925, 0.8, 0.8016528925619835, 0.8032786885245902, 0.8048780487804879, 0.8064516129032258, 0.808, 0.8095238095238095, 0.8031496062992126, 0.8046875, 0.7984496124031008, 0.7923076923076923, 0.7862595419847328, 0.7878787878787878, 0.7894736842105263, 0.7910447761194029, 0.7925925925925926, 0.7941176470588235, 0.7883211678832117, 0.782608695652174, 0.7841726618705036, 0.7857142857142857, 0.7872340425531915, 0.7887323943661971, 0.7902097902097902, 0.7916666666666666, 0.7931034482758621, 0.7945205479452054, 0.7959183673469388, 0.7972972972972973, 0.7986577181208053, 0.8, 0.8013245033112583, 0.8026315789473685, 0.803921568627451, 0.8051948051948052, 0.8064516129032258, 0.8076923076923077, 0.802547770700637, 0.8037974683544303, 0.8050314465408805, 0.80625, 0.8074534161490683, 0.808641975308642, 0.8098159509202454, 0.8109756097560976, 0.806060606060606, 0.8072289156626506, 0.8083832335329342, 0.8095238095238095, 0.8106508875739645, 0.8117647058823529, 0.8128654970760234, 0.813953488372093, 0.815028901734104, 0.8160919540229885, 0.8171428571428572, 0.8181818181818182, 0.8135593220338984, 0.8146067415730337, 0.8156424581005587, 0.8111111111111111, 0.8121546961325967, 0.8076923076923077, 0.8087431693989071, 0.8097826086956522, 0.8108108108108109, 0.8118279569892473, 0.8074866310160428, 0.8085106382978723, 0.8095238095238095, 0.8052631578947368, 0.806282722513089, 0.8072916666666666, 0.8082901554404145, 0.8092783505154639, 0.8102564102564103, 0.8061224489795918, 0.8020304568527918, 0.803030303030303, 0.7989949748743719, 0.795]\n",
    "accuracy_cumulative_cnn_cce_SGD_32_64_d025_ml0_e200 = [1.0, 0.5, 0.6666666666666666, 0.75, 0.8, 0.8333333333333334, 0.7142857142857143, 0.75, 0.7777777777777778, 0.7, 0.7272727272727273, 0.6666666666666666, 0.6923076923076923, 0.7142857142857143, 0.7333333333333333, 0.75, 0.7647058823529411, 0.7222222222222222, 0.7368421052631579, 0.75, 0.7619047619047619, 0.7727272727272727, 0.782608695652174, 0.7916666666666666, 0.8, 0.8076923076923077, 0.7777777777777778, 0.7857142857142857, 0.7931034482758621, 0.7666666666666667, 0.7741935483870968, 0.75, 0.7575757575757576, 0.7647058823529411, 0.7714285714285715, 0.7777777777777778, 0.7567567567567568, 0.7631578947368421, 0.7692307692307693, 0.75, 0.7560975609756098, 0.7619047619047619, 0.7674418604651163, 0.7727272727272727, 0.7777777777777778, 0.782608695652174, 0.7659574468085106, 0.75, 0.7346938775510204, 0.74, 0.7450980392156863, 0.75, 0.7547169811320755, 0.7592592592592593, 0.7636363636363637, 0.7678571428571429, 0.7543859649122807, 0.7413793103448276, 0.7457627118644068, 0.75, 0.7377049180327869, 0.7419354838709677, 0.746031746031746, 0.75, 0.7538461538461538, 0.7575757575757576, 0.746268656716418, 0.75, 0.7391304347826086, 0.7428571428571429, 0.7323943661971831, 0.7361111111111112, 0.7397260273972602, 0.7432432432432432, 0.7466666666666667, 0.75, 0.7402597402597403, 0.7435897435897436, 0.7468354430379747, 0.7375, 0.7407407407407407, 0.7439024390243902, 0.7469879518072289, 0.75, 0.7529411764705882, 0.7558139534883721, 0.7586206896551724, 0.7613636363636364, 0.7528089887640449, 0.7555555555555555, 0.7582417582417582, 0.7608695652173914, 0.7634408602150538, 0.7659574468085106, 0.7684210526315789, 0.7708333333333334, 0.7731958762886598, 0.7755102040816326, 0.7676767676767676, 0.77, 0.7623762376237624, 0.7647058823529411, 0.7669902912621359, 0.7692307692307693, 0.7714285714285715, 0.7735849056603774, 0.7757009345794392, 0.7777777777777778, 0.7706422018348624, 0.7727272727272727, 0.7747747747747747, 0.7767857142857143, 0.7787610619469026, 0.7807017543859649, 0.782608695652174, 0.7844827586206896, 0.7777777777777778, 0.7796610169491526, 0.773109243697479, 0.7666666666666667, 0.768595041322314, 0.7704918032786885, 0.7723577235772358, 0.7741935483870968, 0.776, 0.7777777777777778, 0.7716535433070866, 0.7734375, 0.7674418604651163, 0.7615384615384615, 0.7633587786259542, 0.7651515151515151, 0.7669172932330827, 0.7686567164179104, 0.7703703703703704, 0.7720588235294118, 0.7664233576642335, 0.7608695652173914, 0.762589928057554, 0.7571428571428571, 0.7588652482269503, 0.7605633802816901, 0.7622377622377622, 0.7638888888888888, 0.7655172413793103, 0.7671232876712328, 0.7687074829931972, 0.7702702702702703, 0.7718120805369127, 0.7733333333333333, 0.7748344370860927, 0.7697368421052632, 0.7712418300653595, 0.7727272727272727, 0.7741935483870968, 0.7756410256410257, 0.7770700636942676, 0.7784810126582279, 0.779874213836478, 0.78125, 0.782608695652174, 0.7777777777777778, 0.7791411042944786, 0.7804878048780488, 0.7818181818181819, 0.7831325301204819, 0.7784431137724551, 0.7797619047619048, 0.7751479289940828, 0.7764705882352941, 0.7719298245614035, 0.7732558139534884, 0.7745664739884393, 0.7758620689655172, 0.7771428571428571, 0.7784090909090909, 0.7740112994350282, 0.7752808988764045, 0.776536312849162, 0.7722222222222223, 0.7734806629834254, 0.7747252747252747, 0.7759562841530054, 0.7771739130434783, 0.7783783783783784, 0.7795698924731183, 0.7754010695187166, 0.776595744680851, 0.7777777777777778, 0.7736842105263158, 0.774869109947644, 0.7760416666666666, 0.7772020725388601, 0.7783505154639175, 0.7794871794871795, 0.7806122448979592, 0.7766497461928934, 0.7777777777777778, 0.7788944723618091, 0.775]\n",
    "accuracy_cumulative_cnn_cce_AdaMax_32_64_d025_ml0_e200 = [1.0, 1.0, 1.0, 0.75, 0.8, 0.8333333333333334, 0.7142857142857143, 0.75, 0.6666666666666666, 0.6, 0.5454545454545454, 0.5833333333333334, 0.6153846153846154, 0.6428571428571429, 0.6666666666666666, 0.6875, 0.6470588235294118, 0.6111111111111112, 0.5789473684210527, 0.6, 0.6190476190476191, 0.5909090909090909, 0.6086956521739131, 0.625, 0.64, 0.6538461538461539, 0.6666666666666666, 0.6785714285714286, 0.6551724137931034, 0.6333333333333333, 0.6451612903225806, 0.65625, 0.6666666666666666, 0.6764705882352942, 0.6857142857142857, 0.6666666666666666, 0.6486486486486487, 0.631578947368421, 0.6410256410256411, 0.625, 0.6341463414634146, 0.6428571428571429, 0.6511627906976745, 0.6590909090909091, 0.6666666666666666, 0.6739130434782609, 0.6808510638297872, 0.6666666666666666, 0.6530612244897959, 0.66, 0.6666666666666666, 0.6730769230769231, 0.6792452830188679, 0.6851851851851852, 0.6909090909090909, 0.6964285714285714, 0.6842105263157895, 0.6896551724137931, 0.6779661016949152, 0.6666666666666666, 0.6721311475409836, 0.6774193548387096, 0.6825396825396826, 0.6875, 0.6923076923076923, 0.696969696969697, 0.6865671641791045, 0.6764705882352942, 0.6811594202898551, 0.6857142857142857, 0.676056338028169, 0.6805555555555556, 0.684931506849315, 0.6891891891891891, 0.6933333333333334, 0.6973684210526315, 0.6883116883116883, 0.6923076923076923, 0.6962025316455697, 0.7, 0.7037037037037037, 0.7073170731707317, 0.7108433734939759, 0.7142857142857143, 0.7176470588235294, 0.7209302325581395, 0.7126436781609196, 0.7045454545454546, 0.7078651685393258, 0.7, 0.7032967032967034, 0.7065217391304348, 0.7096774193548387, 0.7127659574468085, 0.7157894736842105, 0.71875, 0.711340206185567, 0.7142857142857143, 0.7171717171717171, 0.71, 0.7128712871287128, 0.7156862745098039, 0.7184466019417476, 0.7211538461538461, 0.7238095238095238, 0.7264150943396226, 0.7289719626168224, 0.7314814814814815, 0.7339449541284404, 0.7272727272727273, 0.7297297297297297, 0.7232142857142857, 0.7256637168141593, 0.7280701754385965, 0.7304347826086957, 0.7241379310344828, 0.717948717948718, 0.7203389830508474, 0.7226890756302521, 0.725, 0.7272727272727273, 0.7295081967213115, 0.7317073170731707, 0.7338709677419355, 0.736, 0.7380952380952381, 0.7401574803149606, 0.734375, 0.7364341085271318, 0.7384615384615385, 0.7404580152671756, 0.7424242424242424, 0.7443609022556391, 0.746268656716418, 0.7481481481481481, 0.75, 0.7445255474452555, 0.7463768115942029, 0.7482014388489209, 0.7428571428571429, 0.7446808510638298, 0.7464788732394366, 0.7482517482517482, 0.75, 0.7517241379310344, 0.7534246575342466, 0.7482993197278912, 0.7432432432432432, 0.7449664429530202, 0.74, 0.7417218543046358, 0.7368421052631579, 0.738562091503268, 0.7402597402597403, 0.7419354838709677, 0.7435897435897436, 0.7388535031847133, 0.740506329113924, 0.7421383647798742, 0.7375, 0.7391304347826086, 0.7407407407407407, 0.7423312883435583, 0.7439024390243902, 0.7454545454545455, 0.7469879518072289, 0.7425149700598802, 0.7440476190476191, 0.7455621301775148, 0.7411764705882353, 0.7426900584795322, 0.7441860465116279, 0.7456647398843931, 0.7471264367816092, 0.7485714285714286, 0.75, 0.7457627118644068, 0.7415730337078652, 0.7430167597765364, 0.7444444444444445, 0.7458563535911602, 0.7472527472527473, 0.7486338797814208, 0.75, 0.7513513513513513, 0.7526881720430108, 0.7540106951871658, 0.7553191489361702, 0.7566137566137566, 0.7578947368421053, 0.7591623036649214, 0.7604166666666666, 0.7616580310880829, 0.7628865979381443, 0.764102564102564, 0.7653061224489796, 0.766497461928934, 0.7676767676767676, 0.7688442211055276, 0.765]\n",
    "accuracy_cumulative_cnn_cce_RMSProp_32_64_d025_ml0_e200 = [0.0, 0.5, 0.6666666666666666, 0.75, 0.8, 0.8333333333333334, 0.8571428571428571, 0.75, 0.7777777777777778, 0.8, 0.7272727272727273, 0.75, 0.7692307692307693, 0.7857142857142857, 0.8, 0.8125, 0.7647058823529411, 0.7222222222222222, 0.7368421052631579, 0.75, 0.7619047619047619, 0.7727272727272727, 0.782608695652174, 0.7916666666666666, 0.8, 0.8076923076923077, 0.7777777777777778, 0.7857142857142857, 0.7931034482758621, 0.8, 0.7741935483870968, 0.78125, 0.7878787878787878, 0.7941176470588235, 0.8, 0.8055555555555556, 0.7837837837837838, 0.7631578947368421, 0.7692307692307693, 0.775, 0.7804878048780488, 0.7857142857142857, 0.7906976744186046, 0.7954545454545454, 0.8, 0.8043478260869565, 0.7872340425531915, 0.7708333333333334, 0.7755102040816326, 0.76, 0.7450980392156863, 0.75, 0.7547169811320755, 0.7592592592592593, 0.7636363636363637, 0.7678571428571429, 0.7543859649122807, 0.7413793103448276, 0.7457627118644068, 0.7333333333333333, 0.7377049180327869, 0.7419354838709677, 0.746031746031746, 0.75, 0.7538461538461538, 0.7575757575757576, 0.746268656716418, 0.7352941176470589, 0.7391304347826086, 0.7428571428571429, 0.7464788732394366, 0.7361111111111112, 0.7397260273972602, 0.7432432432432432, 0.7466666666666667, 0.75, 0.7402597402597403, 0.7307692307692307, 0.7215189873417721, 0.7125, 0.7160493827160493, 0.7195121951219512, 0.7228915662650602, 0.7261904761904762, 0.7294117647058823, 0.7325581395348837, 0.7241379310344828, 0.7159090909090909, 0.7191011235955056, 0.7222222222222222, 0.7252747252747253, 0.7282608695652174, 0.7311827956989247, 0.7340425531914894, 0.7368421052631579, 0.7395833333333334, 0.7422680412371134, 0.7448979591836735, 0.7474747474747475, 0.75, 0.7524752475247525, 0.7549019607843137, 0.7572815533980582, 0.7596153846153846, 0.7619047619047619, 0.7641509433962265, 0.7663551401869159, 0.7592592592592593, 0.7522935779816514, 0.7545454545454545, 0.7567567567567568, 0.7589285714285714, 0.7610619469026548, 0.7631578947368421, 0.7652173913043478, 0.7672413793103449, 0.7606837606837606, 0.7627118644067796, 0.7563025210084033, 0.75, 0.7520661157024794, 0.7540983606557377, 0.7560975609756098, 0.7580645161290323, 0.76, 0.753968253968254, 0.7480314960629921, 0.7421875, 0.7441860465116279, 0.7384615384615385, 0.7404580152671756, 0.7424242424242424, 0.7443609022556391, 0.746268656716418, 0.7481481481481481, 0.75, 0.7518248175182481, 0.7536231884057971, 0.7553956834532374, 0.75, 0.75177304964539, 0.7535211267605634, 0.7552447552447552, 0.7569444444444444, 0.7586206896551724, 0.7602739726027398, 0.7551020408163265, 0.7567567567567568, 0.7583892617449665, 0.76, 0.7615894039735099, 0.7631578947368421, 0.7647058823529411, 0.7662337662337663, 0.7677419354838709, 0.7692307692307693, 0.7707006369426752, 0.7658227848101266, 0.7610062893081762, 0.7625, 0.7577639751552795, 0.7592592592592593, 0.7607361963190185, 0.7621951219512195, 0.7636363636363637, 0.7650602409638554, 0.7604790419161677, 0.7559523809523809, 0.7514792899408284, 0.7470588235294118, 0.7485380116959064, 0.75, 0.7514450867052023, 0.7528735632183908, 0.7542857142857143, 0.7556818181818182, 0.7570621468926554, 0.7528089887640449, 0.7541899441340782, 0.75, 0.7513812154696132, 0.7472527472527473, 0.7486338797814208, 0.75, 0.7513513513513513, 0.7526881720430108, 0.7540106951871658, 0.7553191489361702, 0.7566137566137566, 0.7578947368421053, 0.7591623036649214, 0.7604166666666666, 0.7616580310880829, 0.7628865979381443, 0.764102564102564, 0.7653061224489796, 0.7614213197969543, 0.7575757575757576, 0.7537688442211056, 0.75]\n",
    "accuracy_cumulative_cnn_cce_AdaGrad_32_64_d025_ml0_e200 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.75, 0.6666666666666666, 0.6, 0.6363636363636364, 0.5833333333333334, 0.6153846153846154, 0.6428571428571429, 0.6666666666666666, 0.6875, 0.6470588235294118, 0.6666666666666666, 0.631578947368421, 0.6, 0.6190476190476191, 0.5909090909090909, 0.6086956521739131, 0.625, 0.64, 0.6538461538461539, 0.6296296296296297, 0.6071428571428571, 0.5862068965517241, 0.5666666666666667, 0.5806451612903226, 0.5625, 0.5757575757575758, 0.5882352941176471, 0.6, 0.6111111111111112, 0.5945945945945946, 0.6052631578947368, 0.6153846153846154, 0.6, 0.5853658536585366, 0.5952380952380952, 0.6046511627906976, 0.6136363636363636, 0.6222222222222222, 0.6304347826086957, 0.6170212765957447, 0.6041666666666666, 0.6122448979591837, 0.6, 0.6078431372549019, 0.6153846153846154, 0.6226415094339622, 0.6296296296296297, 0.6363636363636364, 0.6428571428571429, 0.6491228070175439, 0.6551724137931034, 0.6440677966101694, 0.6333333333333333, 0.639344262295082, 0.6290322580645161, 0.6349206349206349, 0.640625, 0.6461538461538462, 0.6515151515151515, 0.6417910447761194, 0.6470588235294118, 0.6376811594202898, 0.6285714285714286, 0.6197183098591549, 0.625, 0.6301369863013698, 0.6351351351351351, 0.64, 0.6447368421052632, 0.6363636363636364, 0.6282051282051282, 0.6329113924050633, 0.625, 0.6172839506172839, 0.6219512195121951, 0.6265060240963856, 0.6309523809523809, 0.6352941176470588, 0.6395348837209303, 0.632183908045977, 0.625, 0.6179775280898876, 0.6222222222222222, 0.6263736263736264, 0.6195652173913043, 0.6236559139784946, 0.6276595744680851, 0.631578947368421, 0.6354166666666666, 0.6288659793814433, 0.6224489795918368, 0.6262626262626263, 0.62, 0.6138613861386139, 0.6176470588235294, 0.6213592233009708, 0.625, 0.6285714285714286, 0.6320754716981132, 0.6261682242990654, 0.6203703703703703, 0.6238532110091743, 0.6181818181818182, 0.6126126126126126, 0.6160714285714286, 0.6194690265486725, 0.6228070175438597, 0.6260869565217392, 0.6293103448275862, 0.6239316239316239, 0.6186440677966102, 0.6134453781512605, 0.6083333333333333, 0.6033057851239669, 0.6065573770491803, 0.6097560975609756, 0.6129032258064516, 0.616, 0.6190476190476191, 0.6141732283464567, 0.609375, 0.6046511627906976, 0.6, 0.6030534351145038, 0.6060606060606061, 0.6090225563909775, 0.6119402985074627, 0.6148148148148148, 0.6176470588235294, 0.6131386861313869, 0.6086956521739131, 0.6115107913669064, 0.6071428571428571, 0.6099290780141844, 0.6126760563380281, 0.6153846153846154, 0.6180555555555556, 0.6206896551724138, 0.6232876712328768, 0.6190476190476191, 0.6148648648648649, 0.6174496644295302, 0.6133333333333333, 0.6158940397350994, 0.6118421052631579, 0.6143790849673203, 0.6168831168831169, 0.6193548387096774, 0.6217948717948718, 0.6178343949044586, 0.6139240506329114, 0.610062893081761, 0.6125, 0.6149068322981367, 0.6172839506172839, 0.6196319018404908, 0.6219512195121951, 0.6242424242424243, 0.6265060240963856, 0.6227544910179641, 0.625, 0.621301775147929, 0.6235294117647059, 0.6257309941520468, 0.627906976744186, 0.630057803468208, 0.632183908045977, 0.6342857142857142, 0.6306818181818182, 0.6271186440677966, 0.6235955056179775, 0.6201117318435754, 0.6166666666666667, 0.6132596685082873, 0.6098901098901099, 0.6120218579234973, 0.6141304347826086, 0.6162162162162163, 0.6182795698924731, 0.6149732620320856, 0.6170212765957447, 0.6190476190476191, 0.6210526315789474, 0.6178010471204188, 0.6197916666666666, 0.6217616580310881, 0.6237113402061856, 0.6256410256410256, 0.6275510204081632, 0.6294416243654822, 0.6313131313131313, 0.628140703517588, 0.625]\n",
    "accuracy_cumulative_cnn_cce_Ftrl_32_64_d025_ml0_e200 = [0.0, 0.5, 0.3333333333333333, 0.25, 0.2, 0.16666666666666666, 0.14285714285714285, 0.125, 0.1111111111111111, 0.1, 0.09090909090909091, 0.16666666666666666, 0.15384615384615385, 0.14285714285714285, 0.13333333333333333, 0.125, 0.11764705882352941, 0.1111111111111111, 0.10526315789473684, 0.1, 0.09523809523809523, 0.13636363636363635, 0.13043478260869565, 0.125, 0.12, 0.11538461538461539, 0.1111111111111111, 0.10714285714285714, 0.10344827586206896, 0.1, 0.0967741935483871, 0.125, 0.12121212121212122, 0.11764705882352941, 0.11428571428571428, 0.1111111111111111, 0.10810810810810811, 0.10526315789473684, 0.10256410256410256, 0.1, 0.0975609756097561, 0.11904761904761904, 0.11627906976744186, 0.11363636363636363, 0.1111111111111111, 0.10869565217391304, 0.10638297872340426, 0.10416666666666667, 0.10204081632653061, 0.1, 0.09803921568627451, 0.11538461538461539, 0.11320754716981132, 0.1111111111111111, 0.10909090909090909, 0.10714285714285714, 0.10526315789473684, 0.10344827586206896, 0.1016949152542373, 0.1, 0.09836065573770492, 0.11290322580645161, 0.1111111111111111, 0.109375, 0.1076923076923077, 0.10606060606060606, 0.1044776119402985, 0.10294117647058823, 0.10144927536231885, 0.1, 0.09859154929577464, 0.1111111111111111, 0.1095890410958904, 0.10810810810810811, 0.10666666666666667, 0.10526315789473684, 0.1038961038961039, 0.10256410256410256, 0.10126582278481013, 0.1, 0.09876543209876543, 0.10975609756097561, 0.10843373493975904, 0.10714285714285714, 0.10588235294117647, 0.10465116279069768, 0.10344827586206896, 0.10227272727272728, 0.10112359550561797, 0.1, 0.0989010989010989, 0.10869565217391304, 0.10752688172043011, 0.10638297872340426, 0.10526315789473684, 0.10416666666666667, 0.10309278350515463, 0.10204081632653061, 0.10101010101010101, 0.1, 0.09900990099009901, 0.10784313725490197, 0.10679611650485436, 0.10576923076923077, 0.10476190476190476, 0.10377358490566038, 0.102803738317757, 0.10185185185185185, 0.10091743119266056, 0.1, 0.0990990990990991, 0.10714285714285714, 0.10619469026548672, 0.10526315789473684, 0.10434782608695652, 0.10344827586206896, 0.10256410256410256, 0.1016949152542373, 0.10084033613445378, 0.1, 0.09917355371900827, 0.10655737704918032, 0.10569105691056911, 0.10483870967741936, 0.104, 0.10317460317460317, 0.10236220472440945, 0.1015625, 0.10077519379844961, 0.1, 0.09923664122137404, 0.10606060606060606, 0.10526315789473684, 0.1044776119402985, 0.1037037037037037, 0.10294117647058823, 0.10218978102189781, 0.10144927536231885, 0.10071942446043165, 0.1, 0.09929078014184398, 0.1056338028169014, 0.1048951048951049, 0.10416666666666667, 0.10344827586206896, 0.10273972602739725, 0.10204081632653061, 0.10135135135135136, 0.10067114093959731, 0.1, 0.09933774834437085, 0.10526315789473684, 0.10457516339869281, 0.1038961038961039, 0.1032258064516129, 0.10256410256410256, 0.10191082802547771, 0.10126582278481013, 0.10062893081761007, 0.1, 0.09937888198757763, 0.10493827160493827, 0.10429447852760736, 0.10365853658536585, 0.10303030303030303, 0.10240963855421686, 0.10179640718562874, 0.10119047619047619, 0.10059171597633136, 0.1, 0.09941520467836257, 0.10465116279069768, 0.10404624277456648, 0.10344827586206896, 0.10285714285714286, 0.10227272727272728, 0.1016949152542373, 0.10112359550561797, 0.1005586592178771, 0.1, 0.09944751381215469, 0.1043956043956044, 0.10382513661202186, 0.10326086956521739, 0.10270270270270271, 0.10215053763440861, 0.10160427807486631, 0.10106382978723404, 0.10052910052910052, 0.1, 0.09947643979057591, 0.10416666666666667, 0.10362694300518134, 0.10309278350515463, 0.10256410256410256, 0.10204081632653061, 0.10152284263959391, 0.10101010101010101, 0.10050251256281408, 0.1]\n",
    "plt.plot(accuracy_cumulative_cnn_cce_Adam_32_64_d025_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_cnn_cce_Nadam_32_64_d025_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_cnn_cce_SGD_32_64_d025_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_cnn_cce_AdaMax_32_64_d025_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_cnn_cce_RMSProp_32_64_d025_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_cnn_cce_AdaGrad_32_64_d025_ml0_e200)\n",
    "plt.plot(accuracy_cumulative_cnn_cce_32_64_d025_ml0_e200)\n",
    "#plt.plot(accuracy_cumulative_cnn_cce_Ftrl_32_64_d025_ml0_e200)\n",
    "plt.xlabel('trials')\n",
    "plt.xlim(0,len(accuracy_cumulative_cnn_cce_Adam_32_64_d025_ml0_e200))\n",
    "plt.ylim(0.5,1.01)\n",
    "plt.ylabel('cumulative accuracy')\n",
    "plt.legend(['Adam', 'Nadam', 'SGD', 'AdaMax', 'RMSProp', 'AdaGrad', 'AdaDelta'])\n",
    "plt.title('CNN accuracy for differrent optimizers');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "1) It is well worth saving misclassified digit images and their correct labels for re-training of the model. This can be seen from the following table of accuracy of handwritten digit recognition after 500 trials.\n",
    "\n",
    "|Manual labels|Accuracy|\n",
    "|:--:|:------:|\n",
    "|0  |0.694|\n",
    "|200|0.836|\n",
    "|400|0.924|\n",
    "|600|0.934|\n",
    "\n",
    "2) After adding 200 manually labeled images, test accuracy has slightly decreased (from 0.975200 to 0.974100), while the actual performance of digit recognition of the images drawn in the GUI window has strongly increased. Thus, training accuracy is not a reliable metric of actual performance.\n",
    "\n",
    "3) NN model is prone to overfitting: the accuracy for 400 training epochs is significantly lower than for 200 epochs when not using manual labels. Keep the number of epochs below 400.\n",
    "\n",
    "4) Accuracy is sensitive to the number of neurons in the 2nd layer: 50 neurons has a higher accuracy of 0.694 (after 500 trials) compared to 25 neurons with accuracy of 0.573333 and 75 neurons with accuracy of 0.566667 (after 150 trials). If there is one maximum of accuracy versus the number of neurons, then we have localized the lower and upper boundary for this hyperparameter.\n",
    "\n",
    "5) Dropout between the two layers of NN model lowers the accuracy.\n",
    "\n",
    "## 7. Future Directions\n",
    "* Compare the accuracy of recognition of original handwriter whose manual labels were used to re-train the NN model to the accuracy values for another handwriter using the same models.\n",
    "* Narrow down the optimal ranges of all hyperparameters within current NN and CNN configurations, including number of neurons in each hidden layer and dropout rates after each layer, loss function, optimizer, and activation functions.\n",
    "* Test different numbers of layers.\n",
    "* Minimize the number of control buttons in the GUI to simplify and speedup the process of dealing with misclassified digit. For instance, try to implement input of correct label via hitting Enter key, which would allow to eliminate the button 'Get label'.\n",
    "* Enable creation of new classes for letter symbols. This would require to switch from integer to string variables for the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Kaggle Competition\n",
    "https://www.kaggle.com/competitions/digit-recognizer/\n",
    "### Data Description\n",
    "The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "\n",
    "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n",
    "\n",
    "Visually, if we omit the \"pixel\" prefix, the pixels make up the image like this:\n",
    "\n",
    "000 001 002 003 ... 026 027\n",
    "\n",
    "028 029 030 031 ... 054 055\n",
    "\n",
    "056 057 058 059 ... 082 083\n",
    "\n",
    " |   |   |   |  ...  |   |\n",
    " \n",
    "728 729 730 731 ... 754 755\n",
    "\n",
    "756 757 758 759 ... 782 783 \n",
    "\n",
    "The test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.\n",
    "\n",
    "Your submission file should be in the following format: For each of the 28000 images in the test set, output a single line containing the ImageId and the digit you predict. For example, if you predict that the first image is of a 3, the second image is of a 7, and the third image is of a 8, then your submission file would look like:\n",
    "\n",
    "ImageId,Label\n",
    "\n",
    "1,3<br>\n",
    "2,7<br>\n",
    "3,8<br>\n",
    "\n",
    "(27997 more lines)\n",
    "The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing against Kaggle dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (103600, 28, 28)\n",
      "X_test.shape = (10000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"X_train.shape =\",X_train.shape)\n",
    "print(\"X_test.shape =\",X_test.shape)\n",
    "X_test = pd.read_csv('data/test.csv')\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def di(n):\n",
    "    '''Display a test image #n.'''\n",
    "    img = np.array(X_test.iloc[n-1]).reshape(sy,sx).astype('float32')/255\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f'Test image {n}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      0\n",
       "4        5      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000 Kaggle test labels\n"
     ]
    }
   ],
   "source": [
    "y_test_kaggle = pd.read_csv('data/kaggle_test.csv')\n",
    "display(y_test_kaggle.head())\n",
    "print(f\"{y_test_kaggle.shape[0]} Kaggle test labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Predict a digit\n",
    "n = 0 # image number\n",
    "img = np.array(X_test.iloc[n]).reshape(1,sy*sx).astype('float32')/255\n",
    "res = model.predict([img]) # probabilities of digits\n",
    "np.argmax(res) # most probable digit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for dd in range(10):\n",
    "    N = len(X_test) # no. of test digits\n",
    "    test_digits = [0]*N\n",
    "    for i in range(N): test_digits[i] = dd\n",
    "    submission = pd.DataFrame({'ImageId':np.arange(1,N+1), 'Label':test_digits})\n",
    "    submission.to_csv('submission_'+str(dd)+'.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle accuracy = 0.9987857142857143\n",
      "Convolutional sequential NN, 1600 manual labels, 19 training epochs\n"
     ]
    }
   ],
   "source": [
    "# Predict test digits\n",
    "N = len(X_test) # no. of test digits\n",
    "test_digits = [0]*N\n",
    "if mx:\n",
    "    for i in range(N): # all test images\n",
    "        img = np.array(X_test.iloc[i]).reshape(1,sy,sx).astype('float32')/255\n",
    "        res = model.predict([img]) # probabilities of digits\n",
    "        test_digits[i] = np.argmax(res) # most probable digit\n",
    "else:\n",
    "    for i in range(N): # all test images\n",
    "        img = np.array(X_test.iloc[i]).reshape(1,sy*sx).astype('float32')/255\n",
    "        res = model.predict([img]) # probabilities of digits\n",
    "        test_digits[i] = np.argmax(res) # most probable digit\n",
    "test_digits[:10]\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({'ImageId':np.arange(1,N+1), 'Label':test_digits})\n",
    "submission.to_csv('submission054.csv', index=None)\n",
    "\n",
    "kaggle_acc = sum(test_digits == y_test_kaggle.Label.values)/28000\n",
    "print(\"Kaggle accuracy =\", kaggle_acc)\n",
    "\n",
    "# Description\n",
    "if mx:\n",
    "    print(f\"Convolutional sequential NN, {ml} manual labels, {epochs} training epochs\")\n",
    "else:\n",
    "    print(f\"Non-convolutional sequential NN, {ml} manual labels, {epochs} training epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('performance/cnn_relu48_relu96_relu48_d0.25.ml1600_k.e1-13.csv', 'a')\n",
    "f.write(f\",{kaggle_acc}\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = pd.read_csv('performance/cnn_relu48_relu96_relu48_d0.25.ml1600_k.e1-13.csv')\n",
    "# Best result\n",
    "idx = kaggle.accuracy_kaggle.argmax()\n",
    "print(f\"Kaggle accuracy = {kaggle.accuracy_kaggle[idx]}\")\n",
    "print(f\"Epochs = {kaggle.epochs[idx]}\")\n",
    "# Plot\n",
    "plt.scatter(kaggle.epochs, kaggle.accuracy_kaggle)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('CNN nl1=48, nl2=96, nl3=48');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = pd.DataFrame({\n",
    "    'ml':[1600],\n",
    "    'kl':[42000],\n",
    "    'epochs':[50],\n",
    "    'nl1':[48],\n",
    "    'nl2':[96],\n",
    "    'nl3':[48],\n",
    "    'accuracy':[0.99925]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = pd.DataFrame({'ml':[800,1600,800,800,800,800,800], 'epochs':[200,200,400,100,300,150,250],\\\n",
    "                       'accuracy':[0.99664,0.99421,0.99664, 0.99671, 0.99653, 0.99642, 0.99596]})\n",
    "ep = kaggle.loc[kaggle.ml == 800, 'epochs']\n",
    "ac = kaggle.loc[kaggle.ml == 800, 'accuracy']\n",
    "plt.scatter(ep,ac);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_cnn = pd.DataFrame({'ml':[800,42800], 'epochs':[200,200],\\\n",
    "                       'accuracy':[0.99907,0.99921]})\n",
    "ep = kaggle_cnn.loc[kaggle_cnn.ml >= 800, 'epochs']\n",
    "ac = kaggle_cnn.loc[kaggle_cnn.ml >= 800, 'accuracy']\n",
    "plt.scatter(ep,ac);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Non-convolutional sequential NN, 1600 manual labels, 200 training epochs: score 0.99421, place 286.\n",
    "2. Non-convolutional sequential NN, 800 manual labels, 200 training epochs: score 0.99664, place 95.\n",
    "4. Non-convolutional sequential NN, 800 manual labels, 100 training epochs: score 0.99671, place 93."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
